{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-infrastructure-helper","title":"Welcome to Infrastructure helper","text":""},{"location":"#for-practicing-linux","title":"For practicing linux","text":"<pre><code># see this first https://www.youtube.com/watch?v=CLh2ACdXNbc\ndocker run -it techworldwithnana/youtube:linux-tutorial\n</code></pre>"},{"location":"#youtube","title":"Youtube","text":""},{"location":"#channels","title":"Channels","text":"<ul> <li>Abhishek.Veeramalla</li> <li>TrainWithShubham</li> <li>Cloud Champ</li> </ul>"},{"location":"#playlists","title":"Playlists","text":"<ul> <li>DEVOPS ZERO TO HERO COURSE<ul> <li>iam-veeramalla/Jenkins-Zero-To-Hero</li> </ul> </li> </ul>"},{"location":"#medium-blogs","title":"Medium Blogs","text":"<ul> <li>DevSecOps Pipeline: Automating CI/CD Pipeline For Secure Multi-Language Applications Using Jenkins</li> <li>End-to-End DevSecOps and GitOps Implementation with Jenkins, Docker, SonarQube, Trivy, Terraform, ArgoCD, and Amazon EKS</li> </ul>"},{"location":"aws/eks/","title":"EKS (Elastic Kubernetes Service)","text":""},{"location":"aws/eks/#eks-elastic-kubernetes-service","title":"EKS (Elastic Kubernetes Service)","text":""},{"location":"aws/eks/#youtube","title":"Youtube","text":"<ul> <li>AWS EKS Tutorial | What is EKS? | EKS Explained | KodeKloud</li> <li> <p>ECS and EKS: What Works Best for Your Project? | AWS ECS vs EKS | KodeKloud</p> </li> <li> <p>Mastering AWS EKS: Top 10 AWS EKS Interview Questions Unveiled with Answers!\"</p> </li> <li> <p>How to Deploy AWS EKS with Terraform - The Simplest Guide to Get Up and Running</p> </li> <li> <p>Amazon EKS Tutorial (Terraform - Ingress - Nginx - TLS - IAM Users - Autoscaling)</p> </li> <li>AWS EKS Kubernetes Tutorial [Full Course]</li> </ul>"},{"location":"aws/introduction/","title":"introduction","text":""},{"location":"aws/introduction/#fundamentals-of-aws-cloud-computing","title":"Fundamentals of AWS Cloud Computing","text":"<p>This video explains the basics of AWS cloud computing and how different AWS services are used together to build real-world applications. The video is divided into 6 key parts:</p>"},{"location":"aws/introduction/#1-static-content-hosting-delivery","title":"1. Static Content Hosting &amp; Delivery","text":"<ul> <li>S3 (Simple Storage Service):<ul> <li>S3 is the storage location for all website files, including images, HTML, JavaScript, and CSS.</li> <li>It is highly reliable and can handle files of any size.</li> <li>S3 organizes files into buckets, which act as root folders for websites.</li> </ul> </li> <li>CloudFront:<ul> <li>CloudFront is AWS's Content Delivery Network (CDN).</li> <li>It copies website files to data centers globally (Edge locations) for fast user access, regardless of location.</li> <li>CloudFront enhances security with features like signed URLs and cookies for content access control.</li> <li>It integrates with AWS WAF (Web Application Firewall) to protect websites from cyber attacks.</li> </ul> </li> <li>Route 53:<ul> <li>Route 53 is AWS's DNS service.</li> <li>It translates website domain names into IP addresses.</li> <li>It directs users to the nearest or fastest server location.</li> <li>It can split traffic for testing new website versions.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#2-backend-services-compute-layer","title":"2. Backend Services / Compute Layer","text":"<ul> <li>Serverless with API Gateway &amp; Lambda:<ul> <li>API Gateway receives requests and routes them to appropriate Lambda functions.</li> <li>Lambda functions execute code in response to triggers.</li> <li>Lambda is serverless, meaning AWS manages the underlying servers.</li> <li>Serverless is ideal for unpredictable workloads and specific tasks.</li> <li>It scales automatically and charges only for compute time used.</li> </ul> </li> <li>EC2 (Elastic Cloud Compute):<ul> <li>EC2 provides virtual servers in AWS data centers.</li> <li>It offers complete control over the server environment, including OS, software, and security.</li> <li>EC2 is scalable, allowing users to adjust server capacity as needed.</li> <li>It suits applications needing specific configurations or legacy software.</li> </ul> </li> <li>Containers with ECS (Elastic Container Service):<ul> <li>ECS manages containers, which package applications and their dependencies.</li> <li>Containers ensure consistent performance across different environments.</li> <li>ECS is ideal for microservices architectures.</li> <li>It allows independent scaling and updates of application components.</li> <li>It balances serverless and EC2, offering more control than Lambda but less management than EC2.</li> </ul> </li> <li>Elastic Kubernetes Service (EKS):<ul> <li>Managed service for running Kubernetes on AWS without needing to install and operate your own Kubernetes control plane.</li> </ul> </li> <li>AWS Fargate:<ul> <li>Serverless compute engine for containers, allowing users to run containers without managing servers.</li> </ul> </li> <li>Lightsail:<ul> <li>Easiest way to get started with AWS for small-scale applications, offering virtual servers, storage, and networking.</li> </ul> </li> <li>Batch:<ul> <li>Fully managed service for running batch computing workloads of any scale.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#3-data-storage-management","title":"3. Data Storage &amp; Management","text":"<ul> <li>S3 (Object Storage):<ul> <li>S3 is object storage, suitable for files like images, videos, and documents.</li> <li>Files are stored as complete objects and accessed via URLs.</li> <li>It is best for infrequently modified, complete files.</li> </ul> </li> <li>RDS (Relational Database Service):<ul> <li>RDS is for traditional SQL databases.</li> <li>It automatically manages tasks like backups, security, and scaling.</li> <li>It is suitable for structured data with clear relationships.</li> <li>Example: e-commerce applications managing orders, customers, and products.</li> </ul> </li> <li>DynamoDB (NoSQL Database):<ul> <li>DynamoDB is a NoSQL database designed for speed and scalability.</li> <li>It handles large data volumes with millisecond responses.</li> <li>It is flexible and best for data that doesn't fit into tables or needs fast access.</li> <li>Example: tracking delivery driver locations.</li> </ul> </li> <li>EBS (Elastic Block Store):<ul> <li>Provides persistent block storage volumes for use with Amazon EC2 instances.</li> </ul> </li> <li>EFS (Elastic File System):<ul> <li>Provides scalable file storage for use with Amazon EC2.</li> </ul> </li> <li>Glacier:<ul> <li>Secure, durable, and extremely low-cost cloud storage service for data archiving and long-term backup.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#4-ai-and-machine-learning","title":"4. AI and Machine Learning","text":"<ul> <li>Amazon Bedrock:<ul> <li>Bedrock offers pre-built AI models.</li> <li>It allows easy integration of advanced AI capabilities without building models from scratch.</li> <li>Useful for quickly adding chatbots or other AI features.</li> <li>Offers customization and security.</li> </ul> </li> <li>Amazon SageMaker:<ul> <li>SageMaker is a comprehensive platform for building, training, and deploying custom machine learning models.</li> <li>It is ideal for complex tasks like predicting user behavior or fraud detection.</li> <li>Offers full control over AI development.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#5-security","title":"5. Security","text":"<ul> <li>VPC (Virtual Private Cloud):<ul> <li>VPC is a private network within AWS.</li> <li>It allows control over network configurations, subnets, and internet access.</li> <li>It uses public subnets for internet-facing resources.</li> <li>It uses private subnets for internal resources.</li> <li>NAT Gateways provide secure internet access from private subnets.</li> </ul> </li> <li>IAM (Identity and Access Management):<ul> <li>IAM controls access to AWS resources.</li> <li>It ensures users and services have only necessary permissions.</li> <li>It enables granular permission settings for services like Lambda and EC2.</li> <li>Crucial for securing AI and machine learning workloads.</li> </ul> </li> <li>Security Tools:<ul> <li>AWS provides additional security services.</li> <li>GuardDuty for threat detection.</li> <li>KMS for encryption key management.</li> <li>AWS Shield and WAF for cyber attack protection.</li> </ul> </li> <li>Secrets Manager:<ul> <li>Protects access to applications, services, and IT resources without the upfront investment and on-going maintenance costs of operating your own infrastructure.</li> </ul> </li> <li>AWS CDK (Cloud Development Kit):<ul> <li>Open-source software development framework to define your cloud application resources using familiar programming languages.</li> </ul> </li> <li>Config:<ul> <li>Provides AWS resource inventory, configuration history, and configuration change notifications to enable security and governance.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#6-monitoring-and-auditing","title":"6. Monitoring and Auditing","text":"<ul> <li>CloudWatch:<ul> <li>CloudWatch monitors AWS services and applications.</li> <li>It collects performance metrics, logs, and events.</li> <li>It provides dashboards and alerts for real-time insights into application performance.</li> <li>It can automate responses to issues.</li> </ul> </li> <li>CloudTrail:<ul> <li>CloudTrail records API calls within the AWS account.</li> <li>It logs all changes made to the AWS environment.</li> <li>Essential for auditing and security.</li> <li>Tracks who made changes, when, and what was changed.</li> <li>Especially important in AI and machine learning for tracking model performance and access.</li> </ul> </li> <li>X-Ray:<ul> <li>Helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#networking-services","title":"Networking Services","text":"<p>Networking services facilitate communication between resources and manage traffic.</p> <ul> <li>VPC (Virtual Private Cloud):<ul> <li>A VPC is a virtual private cloud that isolates your resources from other resources in the AWS cloud. It allows you to launch AWS resources into a virtual network that you've defined.</li> </ul> </li> <li>Subnets:<ul> <li>A subnet is a range of IP addresses within a VPC. You can create public and private subnets. Public subnets have access to the internet, while private subnets do not.</li> </ul> </li> <li>Security Group:<ul> <li>A security group acts as a virtual firewall that controls the traffic to and from your EC2 instances. You can specify rules to allow or deny traffic based on IP addresses, protocols, and ports.</li> </ul> </li> <li>Internet Gateway:<ul> <li>An internet gateway connects your VPC to the internet. It allows communication between instances in your VPC and the internet.</li> </ul> </li> <li>Route Table:<ul> <li>A route table determines how traffic flows within a VPC. You can create custom route tables for your subnets to control the routing of traffic.</li> </ul> </li> <li>NAT Gateway:<ul> <li>A NAT Gateway in AWS is a managed Network Address Translation service that allows instances in private subnets to access the internet while preventing external services from initiating connections to those instances.</li> </ul> </li> <li>NACL (Network ACL):<ul> <li>A Network ACL is a virtual firewall that controls traffic to and from your subnets. It provides an additional layer of security at the subnet level.</li> </ul> </li> <li>Load Balancer:<ul> <li>A load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, to ensure high availability and reliability.</li> </ul> </li> <li>Direct Connect:<ul> <li>Direct Connect provides a dedicated network connection from your premises to AWS. It allows you to establish a private connection between AWS and your data center, office, or colocation environment.</li> </ul> </li> <li>Transit Gateway:<ul> <li>Transit Gateway enables customers to connect their VPCs and their on-premises networks to a single gateway. It simplifies network architecture and reduces the complexity of managing multiple connections.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#messaging-services","title":"Messaging Services","text":"<p>These services enable communication between distributed systems or components.</p> <ul> <li>SQS (Simple Queue Service):<ul> <li>SQS is a fully managed message queuing service that enables decoupling of microservices, distributed systems, and serverless applications. It allows you to send, store, and receive messages between software components.</li> </ul> </li> <li>SNS (Simple Notification Service):<ul> <li>SNS is a managed messaging service for sending notifications from the cloud to subscribers or other applications. It supports multiple messaging protocols, including HTTP/HTTPS, email, SMS, and AWS Lambda.</li> </ul> </li> <li>MQ:<ul> <li>MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes it easy to set up and operate message brokers in the cloud. It enables communication between distributed applications and microservices.</li> </ul> </li> </ul>"},{"location":"aws/introduction/#aws-projects-for-learning","title":"AWS projects for learning","text":""},{"location":"aws/introduction/#project-1-deploy-a-static-website","title":"Project 1: Deploy a Static Website","text":"<ul> <li>Description: Learn how to deploy a static website using Amazon S3 and Amazon CloudFront.</li> <li>Resources: AWS S3, AWS CloudFront, AWS Certificate Manager, AWS WAF, AWS CloudWatch</li> </ul>"},{"location":"aws/introduction/#project-2-silent-scalper","title":"Project 2: Silent Scalper","text":"<ul> <li>Description: Build a serverless application that monitors stock prices and sends notifications when certain conditions are met.</li> <li>Resources: AWS S3, AWS Lambda, DynamoDB, API Gateway, SNS, AWS CloudWatch</li> </ul>"},{"location":"aws/introduction/#project-3-the-smart-vault","title":"Project 3: The Smart Vault","text":"<ul> <li>Description: Create a secure storage solution for sensitive data with automated backup and monitoring.</li> <li>Resources: EC2, EBS, EventBridge, AWS Lambda, SNS, AWS CloudWatch</li> </ul>"},{"location":"aws/introduction/#project-4-ai-customer-service-bot","title":"Project 4: AI Customer Service Bot","text":"<ul> <li>Description: Develop an AI-powered chatbot for customer service that can handle common queries and escalate complex issues.</li> <li>Resources: AWS S3, AWS Lambda, SNS, Amazon Lex, Amazon Polly</li> </ul>"},{"location":"aws/introduction/#project-5-intelligent-document-engine","title":"Project 5: Intelligent Document Engine","text":"<ul> <li>Description: Build a document processing system that extracts information from documents and stores it in a searchable database.</li> <li>Resources: AWS S3, Textract, SageMaker, OpenSearch, DynamoDB, MLOps</li> </ul>"},{"location":"aws/links/","title":"links","text":""},{"location":"aws/links/#official-documentation-and-prerequisites","title":"Official documentation and prerequisites","text":"<ul> <li>Cloud computing basics<ul> <li>AWS Cloud Essentials</li> <li>Hands-on Tutorials</li> <li>Welcome to AWS Documentation</li> </ul> </li> <li>linux<ul> <li>Learn the ways of Linux-fu, for free</li> <li>Freecodecamp vides from YouTube</li> </ul> </li> <li>Networking<ul> <li>Freecodecamp videos from YouTube</li> </ul> </li> <li>Operating System<ul> <li>Freecodecamp videos from YouTube</li> </ul> </li> <li>Terraform<ul> <li>Get Started - AWS</li> </ul> </li> <li>Github Actions<ul> <li>GitHub Actions Certification - Full Course to PASS the Exam</li> </ul> </li> </ul>"},{"location":"aws/links/#udemy","title":"Udemy","text":""},{"location":"aws/links/#cloud-practitioner","title":"Cloud Practitioner","text":"<ul> <li>Ultimate AWS Certified Cloud Practitioner CLF-C02<ul> <li>6 Practice Exams | AWS Certified Cloud Practitioner CLF-C02</li> </ul> </li> </ul>"},{"location":"aws/links/#developer-associate","title":"Developer Associate","text":"<ul> <li>Ultimate AWS Certified Developer Associate 2025 DVA-C02<ul> <li>Practice Exams | AWS Certified Developer Associate 2024</li> <li>AWS Certified Developer Associate Practice Exams DVA-C02</li> </ul> </li> </ul>"},{"location":"aws/links/#solutions-architect-associate","title":"Solutions Architect Associate","text":"<ul> <li>Ultimate AWS Certified Solutions Architect Associate 2025<ul> <li>Practice Exams | AWS Certified Solutions Architect Associate</li> </ul> </li> </ul>"},{"location":"aws/links/#youtube","title":"Youtube","text":"<ul> <li>The only Cloud services you actually need to know</li> <li>AWS Cloud Engineer Full Course for Beginners</li> <li>AWS Networking Basics For Programmers | Hands On</li> <li>Terraform AWS VPC Tutorial - Public, Private, and Isolated Subnets</li> <li> <p>AWS EKS Kubernetes Tutorial [Full Course]</p> </li> <li> <p>Top AWS Services Explained - System Design</p> </li> <li>Top Advance AWS Services Explained - System Design</li> </ul>"},{"location":"aws/links/#youtube-playlists","title":"Youtube playlists","text":"<ul> <li> <p>AWS Zero to Hero - AWS Simplified</p> <ul> <li>iam-veeramalla/aws-devops-zero-to-hero</li> </ul> </li> <li> <p>Piyush Garg</p> </li> <li> <p>AWS - Amazon Web Services</p> </li> <li> <p>KodeKloud</p> </li> <li>AWS</li> <li> <p>AWS Cloud Practitioner (CLF-C02)</p> </li> <li> <p>Tiny Technical Tutorials</p> </li> <li>AWS: Learn Amazon Web Services (AWS) with bite-size tutorials that are easy to understand</li> <li> <p>AWS Projects: Get hands-on practice with multiple AWS services</p> </li> <li> <p>DGR Uploads</p> </li> <li> <p>AWS_Interview_Questions</p> </li> <li> <p>Digital Cloud Training</p> <ul> <li>AWS Serverless with AWS Lambda, API Gateway &amp; EventBridge | Full Course for Beginners</li> </ul> </li> <li> <p>Gaurav Sharma</p> </li> <li>learning-ocean.com</li> <li>AWS API Gateway</li> <li>AWS Lambda tutorials - Serverless</li> <li>DynamoDB in Hindi Step by step Guide</li> <li>AWS VPC Tutorials Step by Step - A Complete Guide to Virtual Private Cloud - For Beginners</li> <li> <p>AWS Tutorials - AWS tutorials For Beginners - AWS Certification - AWS Training - In Hindi</p> </li> <li> <p>Be A Better Dev</p> </li> <li>AWS S3 File Upload + Lambda Trigger - Step by Step Tutorial</li> <li>Top 5 Use Cases for AWS Lambda</li> <li>AWS RDS Aurora Tutorials</li> <li> <p>AWS Lambda Guides - Everything you need to know about Lambdas!</p> </li> <li> <p>AWS Cloud Computing Course</p> </li> <li> <p>AWS Tutorials by pixegami</p> </li> <li> <p>Tech with Hitch</p> </li> <li> <p>Build a REST API (CRUD) with AWS Lambda, API Gateway &amp; DynamoDB Using Python | Step-by-Step Guide</p> <ul> <li>lambda_function.py</li> </ul> </li> <li> <p>Code With Vini</p> </li> </ul>"},{"location":"aws/links/#amazon-firecracker","title":"Amazon firecracker","text":"<ul> <li>Big Misconceptions about Bare Metal, Virtual Machines, and Containers</li> <li>What Are MicroVMs? And Why Should I Care?</li> <li>How AWS's Firecracker virtual machines work | Amazon Science</li> <li>Face off: VMs vs. Containers vs Firecracker</li> <li>Lightning Talk: What Are MicroVMs? And Why Should I Care? - Richard Case, SUSE</li> <li>FireCracker for Linux</li> </ul>"},{"location":"aws/links/#medium","title":"Medium","text":""},{"location":"aws/links/#blogs","title":"Blogs","text":"<ul> <li>Serverless Order Management Using AWS Step Functions and DynamoDB</li> </ul>"},{"location":"aws/links/#channel","title":"Channel","text":"<ul> <li>Towards AWS</li> <li>AWS in Plain English</li> </ul>"},{"location":"aws/networking/","title":"networking","text":""},{"location":"aws/networking/#networking","title":"Networking","text":""},{"location":"aws/networking/#watch-these-videos","title":"Watch these videos","text":"<ul> <li>AWS Networking Basics For Programmers | Hands On</li> </ul>"},{"location":"aws/networking/#setup-2-ec2-instances-private-and-public","title":"Setup 2 EC2 Instances (Private and Public)","text":"<ol> <li>Create a VPC with CIDR <code>10.0.0.0/16</code>.</li> <li>Create a public subnet with CIDR <code>10.0.0.0/24</code> and a private subnet with CIDR <code>10.0.1.0/24</code>.</li> <li>Launch an EC2 instance in the public subnet and VPC with auto-assign public IP. Use a security group with a rule to SSH from anywhere and add the keypair to SSH into the instance with <code>ec2-user</code>.</li> <li>At this time, if we try to SSH into the instance, we will not be able to do it. We need an Internet Gateway. A subnet named as public does not mean it is exposed to the public. The gateway allows VPC to access the Internet.</li> <li>Attach an Internet Gateway to the VPC. Still, we will not be able to access the instance.</li> <li>We need to give the public subnet a route table and a route to the Internet Gateway. Also, add a route table to the private subnet. Route tables belong to a subnet, so we will associate subnets to their proper route table.</li> <li>Edit the route inside the public route table and add <code>0.0.0.0/0</code> so everyone will talk to each other, and the target will be the Internet Gateway.</li> <li>Now, if we check, we will have an SSH connection to the public instance.</li> <li>Create a private instance with the same VPC and private subnet with no auto-assign IP. Use a new security group and add the keypair to SSH.</li> <li>Let's SSH into the private instance. We can't do that directly. We can SSH into the private instance from the public instance as they are in the same VPC, so they are all accessible from one to another. Copy the private key into the public instance, and now we can SSH into the private instance with <code>ec2-user</code> from the public instance.</li> <li>But we will not be able to talk to the internet from the private instance, like doing <code>ping google.com</code> or <code>yum update -y</code>.</li> <li>If there is a way so that none will talk to the private EC2 instance, but the private EC2 instance can talk to the internet, we could use a NAT Gateway here.</li> <li>Create a NAT Gateway in the public subnet as this subnet has access to the internet.</li> <li>Use the private route table to use the NAT Gateway to access the internet. We need an Elastic IP in the NAT Gateway.</li> <li>Add a route in the private route table <code>0.0.0.0/0</code> and point to the NAT Gateway.</li> <li>Now we will be able to access the internet from the private instance and also update the packages.</li> </ol>"},{"location":"aws/networking/#nacl-network-access-control-list","title":"NACL: Network Access Control List","text":"<p>A virtual firewall on the subnet. It is stateless, so if you allow something to the subnet, that does not mean it will allow traffic out from the subnet. We also need an outbound rule. The security group, which is a firewall to the EC2 instance, is stateful. So if you allow something in, it will also allow automatically back.</p>"},{"location":"aws/serverless-application/","title":"Serverless application","text":"<p>To Build a proper serverless architechture we can use the following things</p> <ol> <li>"},{"location":"aws/slides/","title":"slides","text":"<p>Select a PDF: --Select a PDF-- Cloud Practitioner Developer Associate Solutions Architect Full Screen</p>"},{"location":"aws/codes/developer-associate/api-gateway/mapping-template/","title":"set($inputRoot = \\(input.path('\\)'))","text":""},{"location":"aws/codes/developer-associate/api-gateway/mapping-template/#setinputroot-inputpath","title":"set($inputRoot = \\(input.path('\\)'))","text":"<p>{     \"renamedexample\" : $inputRoot.example,    \"anotherkey\" : \"anothervalue\" }</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/SAM/","title":"Step 1 - Download a sample application","text":""},{"location":"aws/codes/developer-associate/sam-codedeploy/SAM/#step-1-download-a-sample-application","title":"Step 1 - Download a sample application","text":"<p>sam init --runtime python3.7</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/SAM/#step-2-build-your-application","title":"Step 2 - Build your application","text":"<p>cd sam-app sam build</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/SAM/#step-3-package-your-application","title":"Step 3 - Package your application","text":"<p>sam deploy --guided</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/","title":"sam-app","text":""},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#sam-app","title":"sam-app","text":"<p>This project contains source code and supporting files for a serverless application that you can deploy with the SAM CLI. It includes the following files and folders.</p> <ul> <li>hello_world - Code for the application's Lambda function.</li> <li>events - Invocation events that you can use to invoke the function.</li> <li>tests - Unit tests for the application code. </li> <li>template.yaml - A template that defines the application's AWS resources.</li> </ul> <p>The application uses several AWS resources, including Lambda functions and an API Gateway API. These resources are defined in the <code>template.yaml</code> file in this project. You can update the template to add AWS resources through the same deployment process that updates your application code.</p> <p>If you prefer to use an integrated development environment (IDE) to build and test your application, you can use the AWS Toolkit. The AWS Toolkit is an open source plug-in for popular IDEs that uses the SAM CLI to build and deploy serverless applications on AWS. The AWS Toolkit also adds a simplified step-through debugging experience for Lambda function code. See the following links to get started.</p> <ul> <li>PyCharm</li> <li>IntelliJ</li> <li>VS Code</li> <li>Visual Studio</li> </ul>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#deploy-the-sample-application","title":"Deploy the sample application","text":"<p>The Serverless Application Model Command Line Interface (SAM CLI) is an extension of the AWS CLI that adds functionality for building and testing Lambda applications. It uses Docker to run your functions in an Amazon Linux environment that matches Lambda. It can also emulate your application's build environment and API.</p> <p>To use the SAM CLI, you need the following tools.</p> <ul> <li>SAM CLI - Install the SAM CLI</li> <li>Python 3 installed</li> <li>Docker - Install Docker community edition</li> </ul> <p>To build and deploy your application for the first time, run the following in your shell:</p> <pre><code>sam build --use-container\nsam deploy --guided\n</code></pre> <p>The first command will build the source of your application. The second command will package and deploy your application to AWS, with a series of prompts:</p> <ul> <li>Stack Name: The name of the stack to deploy to CloudFormation. This should be unique to your account and region, and a good starting point would be something matching your project name.</li> <li>AWS Region: The AWS region you want to deploy your app to.</li> <li>Confirm changes before deploy: If set to yes, any change sets will be shown to you before execution for manual review. If set to no, the AWS SAM CLI will automatically deploy application changes.</li> <li>Allow SAM CLI IAM role creation: Many AWS SAM templates, including this example, create AWS IAM roles required for the AWS Lambda function(s) included to access AWS services. By default, these are scoped down to minimum required permissions. To deploy an AWS CloudFormation stack which creates or modified IAM roles, the <code>CAPABILITY_IAM</code> value for <code>capabilities</code> must be provided. If permission isn't provided through this prompt, to deploy this example you must explicitly pass <code>--capabilities CAPABILITY_IAM</code> to the <code>sam deploy</code> command.</li> <li>Save arguments to samconfig.toml: If set to yes, your choices will be saved to a configuration file inside the project, so that in the future you can just re-run <code>sam deploy</code> without parameters to deploy changes to your application.</li> </ul> <p>You can find your API Gateway Endpoint URL in the output values displayed after deployment.</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#use-the-sam-cli-to-build-and-test-locally","title":"Use the SAM CLI to build and test locally","text":"<p>Build your application with the <code>sam build --use-container</code> command.</p> <pre><code>sam-app$ sam build --use-container\n</code></pre> <p>The SAM CLI installs dependencies defined in <code>hello_world/requirements.txt</code>, creates a deployment package, and saves it in the <code>.aws-sam/build</code> folder.</p> <p>Test a single function by invoking it directly with a test event. An event is a JSON document that represents the input that the function receives from the event source. Test events are included in the <code>events</code> folder in this project.</p> <p>Run functions locally and invoke them with the <code>sam local invoke</code> command.</p> <pre><code>sam-app$ sam local invoke HelloWorldFunction --event events/event.json\n</code></pre> <p>The SAM CLI can also emulate your application's API. Use the <code>sam local start-api</code> to run the API locally on port 3000.</p> <pre><code>sam-app$ sam local start-api\nsam-app$ curl http://localhost:3000/\n</code></pre> <p>The SAM CLI reads the application template to determine the API's routes and the functions that they invoke. The <code>Events</code> property on each function's definition includes the route and method for each path.</p> <pre><code>      Events:\n        HelloWorld:\n          Type: Api\n          Properties:\n            Path: /hello\n            Method: get\n</code></pre>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#add-a-resource-to-your-application","title":"Add a resource to your application","text":"<p>The application template uses AWS Serverless Application Model (AWS SAM) to define application resources. AWS SAM is an extension of AWS CloudFormation with a simpler syntax for configuring common serverless application resources such as functions, triggers, and APIs. For resources not included in the SAM specification, you can use standard AWS CloudFormation resource types.</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#fetch-tail-and-filter-lambda-function-logs","title":"Fetch, tail, and filter Lambda function logs","text":"<p>To simplify troubleshooting, SAM CLI has a command called <code>sam logs</code>. <code>sam logs</code> lets you fetch logs generated by your deployed Lambda function from the command line. In addition to printing the logs on the terminal, this command has several nifty features to help you quickly find the bug.</p> <p><code>NOTE</code>: This command works for all AWS Lambda functions; not just the ones you deploy using SAM.</p> <pre><code>sam-app$ sam logs -n HelloWorldFunction --stack-name sam-app --tail\n</code></pre> <p>You can find more information and examples about filtering Lambda function logs in the SAM CLI Documentation.</p>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#unit-tests","title":"Unit tests","text":"<p>Tests are defined in the <code>tests</code> folder in this project. Use PIP to install the pytest and run unit tests.</p> <pre><code>sam-app$ pip install pytest pytest-mock --user\nsam-app$ python -m pytest tests/ -v\n</code></pre>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#cleanup","title":"Cleanup","text":"<p>To delete the sample application that you created, use the AWS CLI. Assuming you used your project name for the stack name, you can run the following:</p> <pre><code>aws cloudformation delete-stack --stack-name sam-app\n</code></pre>"},{"location":"aws/codes/developer-associate/sam-codedeploy/sam-app/#resources","title":"Resources","text":"<p>See the AWS SAM developer guide for an introduction to SAM specification, the SAM CLI, and serverless application concepts.</p> <p>Next, you can use AWS Serverless Application Repository to deploy ready to use Apps that go beyond hello world samples and learn how authors developed their applications: AWS Serverless Application Repository main page</p>"},{"location":"cloudflare/links/","title":"Cloudflare","text":""},{"location":"cloudflare/links/#cloudflare","title":"Cloudflare","text":""},{"location":"cloudflare/links/#youtube","title":"Youtube","text":"<ul> <li>The Cloudflare Series</li> </ul>"},{"location":"computer-architechture/links/","title":"links","text":""},{"location":"computer-architechture/links/#low-level-propgramming","title":"Low level propgramming","text":""},{"location":"computer-architechture/links/#website-and-blogs","title":"Website and blogs","text":""},{"location":"computer-architechture/links/#blogs","title":"Blogs","text":"<ul> <li>cpu.land</li> </ul>"},{"location":"computer-architechture/links/#books","title":"Books","text":"<ul> <li>Crafting Interpreters book for building interpreter in C/Java</li> <li>Writing An Interpreter In Go check your drive</li> <li>Writing A Compiler In Go check your drive</li> <li>handmade.network</li> </ul>"},{"location":"computer-architechture/links/#project-using-low-level-programming","title":"Project using low level programming","text":"<ul> <li>c-codebase</li> <li>Rift</li> <li>Indie-Toolbox</li> <li>graphit</li> <li>multieditor</li> </ul>"},{"location":"computer-architechture/links/#youtube","title":"Youtube","text":""},{"location":"computer-architechture/links/#interpretor","title":"Interpretor","text":"<ul> <li>Creating a Compiler</li> </ul>"},{"location":"computer-architechture/links/#single-videos","title":"Single videos","text":"<ul> <li>unlock the lowest levels of coding</li> <li>My 2 Year Journey of Learning C, in 9 minutes<ul> <li>2 Years Of Learning C | Prime Reacts</li> </ul> </li> <li>10 years of embedded coding in 10 minutes</li> <li>X86 Needs To Die</li> </ul>"},{"location":"computer-architechture/links/#series","title":"series","text":"<ul> <li>Handmade Hero Complete</li> <li>Mr. 4th Programming<ul> <li>Base</li> <li>OS</li> <li>x64 Practice</li> <li>Graphics</li> <li>Graphics 2</li> </ul> </li> </ul>"},{"location":"computer-architechture/links/#little-complex-topics","title":"Little complex topics","text":"<ul> <li>I made the same game in Assembly, C and C++</li> <li>LLVM in 100 Seconds</li> <li>i wrote my own memory allocator in C to prove a point</li> </ul>"},{"location":"computer-architechture/links/#youtube-channel","title":"Youtube channel","text":"<ul> <li>Low Level Learning</li> <li>Tech With Nikola</li> <li>Jacob Sorber<ul> <li>Data Structures</li> </ul> </li> <li>3Blue1Brown</li> <li>mycodeschool</li> </ul>"},{"location":"container/docker/","title":"Docker","text":""},{"location":"container/docker/#cheatsheet-for-docker-commands","title":"Cheatsheet for Docker Commands","text":""},{"location":"container/docker/#images","title":"Images:","text":"<ul> <li>Images are read-only templates used to create containers.</li> <li>Images are created with the docker build command, either by us or by other docker users.</li> <li>Images are composed of layers of other images.</li> <li>Images are stored in a Docker registry.</li> </ul>"},{"location":"container/docker/#containers","title":"Containers:","text":"<ul> <li>If an image is a class, then a container is an instance of a class - a runtime object.</li> <li>Containers are lightweight and portable encapsulations of an environment in which to run applications.</li> <li>Containers are created from images. Inside a container, it has all the binaries and dependencies needed to run the application.</li> </ul>"},{"location":"container/docker/#registries-and-repositories","title":"Registries and Repositories:","text":"<ul> <li>A registry is where we store our images.</li> <li>You can host your own registry, or you can use Docker's public registry which is called DockerHub.</li> <li>Inside a registry, images are stored in repositories.</li> <li>Docker repository is a collection of different docker images with the same name, that have different tags, each tag usually represents a different version of the image.</li> </ul>"},{"location":"container/docker/#basic-commands","title":"Basic Commands","text":"<p>Login into remote repository: </p><pre><code>docker login\n</code></pre> <p>create a directory with all docker related things </p><pre><code>docker init\n</code></pre> <p>Pull either from local repository or remote image repository and run it: </p><pre><code>docker run ngnix\n</code></pre> <p>By default, tag considered as latest: </p><pre><code>docker run image_name/tag\n</code></pre> <p>Just to download an image, it will not run: </p><pre><code>docker pull nginx\n</code></pre> <p>Docker run starts the process in the container and attaches the console to the process's standard input, output, and standard error. Containers started in detached mode and exit when the root process used to run the container exits.</p> <p>Pull either from local or remote and run it in a detached mode and will return the container_id: </p><pre><code>docker run -d ngnix\n</code></pre> <p>It will again attach cmd to the docker container: </p><pre><code>docker attach container_id\n</code></pre> <p>It will run ubuntu image and then run bash in interactive mode in the same command prompt. Here i means interactive and t means same terminal: </p><pre><code>docker run -it ubuntu bash\n</code></pre> <p>Nginx on port 80 on docker internal network will be connected to externally port 5000. http://localhost:8080 or http://host-ip:8080 in your browser: </p><pre><code>docker run -p our_port/docker_internal_port -&gt; docker run -p 5000:80 nginx\n</code></pre> <p>Show the current docker running process: </p><pre><code>docker ps\n</code></pre> <p>Example output: </p><pre><code>CONTAINER ID   IMAGE        COMMAND             CREATED         STATUS         PORTS                                       NAMES\n2f11c8f2dc81   tomcat:9.0   \"catalina.sh run\"   2 minutes ago   Up 2 minutes   0.0.0.0:8888-&gt;8080/tcp, :::8888-&gt;8080/tcp   cool_heisenberg\n</code></pre> <p>Execute a command in a running container: </p><pre><code>docker exec 2f11c8f2dc81 ps -eaf\n</code></pre> <p>Example output: </p><pre><code>UID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  1 04:16 pts/0    00:00:03 /usr/local/openjdk-11/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start\nroot        40     0  0 04:19 ?        00:00:00 ps -eaf\n</code></pre> <p>Show all docker processes: </p><pre><code>docker ps -a\n</code></pre> <p>To pause a running container: </p><pre><code>docker pause conainer_id \n</code></pre> <p>To stop a running a container/process: </p><pre><code>docker stop container_name/container_id\n</code></pre> <p>Stops all the containers: </p><pre><code>docker stop $(docker ps -a -q)\n</code></pre> <p>To remove a running container: </p><pre><code>docker rm container_name/container_id\n</code></pre> <p>To show all available images and its size: </p><pre><code>docker images\n</code></pre> <p>To delete an available image: </p><pre><code>docker rmi image_name\n</code></pre>"},{"location":"container/docker/#about-container-specifications","title":"About container specifications","text":"<p>Container is meant to run a specific task like a server or a process. The container will be running as long as process inside in it will be in a living stage. So we can append a command into the process like this:</p> <p>It will run ubuntu image and sleep for 10 sec. when 10 sec is completed then ubuntu will be stopped as it has no running process: </p><pre><code>docker run ubuntu sleep 10\n</code></pre> <p>Check ubuntu version: </p><pre><code>docker run ubuntu cat /etc/\"os-release\"\n</code></pre> <p>Run a command in a running container: </p><pre><code>docker exec\n</code></pre> <p>This will create a new Bash session in the container ubuntu_bash: </p><pre><code>docker exec -it ubuntu_bash bash\n</code></pre> <p>We can specify environment variable using --env: </p><pre><code>docker run --env MYSQL_ROOT_PASSWORD=100997 mysql\n</code></pre> <p>We can volume mount using the -v flag. -v means it map docker internal directory to external file storage: </p><pre><code>docker run -v /opt/tempMysql:/var/lib/mysql --env MYSQL_ROOT_PASSWORD=100997 mysql\n</code></pre> <p>To get more information about the container. We can see the environment variables using docker inspect container_name/id. Networks and all other things are there: </p><pre><code>docker inspect container_id/container_name\n</code></pre> <p>To get logs for the container: </p><pre><code>docker logs container_id/container_name\n</code></pre> <p>To give container a name and port: </p><pre><code>docker run -p 8080:8080 --name=jenkins-master -d jenkins/jenkins\n</code></pre> <p>To take a snapshot of a running container: </p><pre><code>docker commit &lt;container_name&gt; &lt;image_name&gt;\n</code></pre>"},{"location":"container/docker/#about-images-and-volumes","title":"About images and volumes","text":"<p>For deleting a single image: </p><pre><code>docker rmi image-name/tag-name\n</code></pre> <p>For deleting an image forcefully (required when the a running container is still using the image): </p><pre><code>docker rmi -f image-name/tag-name\n</code></pre> <p>To show all the layer details of the image: </p><pre><code>docker history &lt;image_name&gt;\n</code></pre> <p>For deleting all images: </p><pre><code>docker image prune -a\n</code></pre> <p>Docker uses the storage driver to do all volume and storage related things. Docker uses the different type of storage drivers.</p> <p>To create docker volume. It will create a data_ volume folder under the volume directory of \"var/lib/docker\" directory: </p><pre><code>docker volume create data_volume\n</code></pre> <p>This will mount the data_volume directory inside the docker: </p><pre><code>docker run -v data_volume:var/lib/mysql mysql:latest\n</code></pre> <p>If we want to use external directory: </p><pre><code>docker run -v external_path:var/lib/mysql mysql:latest\ndocker run --mount type=bind,source=C:/Users/ghosh/OneDrive/Desktop/devops/data/mysql,target=/var/lib/mysql mysql\n</code></pre> <p>For deleting all volumes: </p><pre><code>docker volume prune -a\n</code></pre>"},{"location":"container/docker/#image-layers","title":"Image layers","text":"<p>What is the image layers: 1. To build a good image we must divide the docker file into different stages: 1. Base Image, 2. Working directory, 3. Build, 4. Run and Expose 2. In the first line of docker build docker sends the context to the docker daemon. We can reduce the context by adding dockerignore file.</p> <p>For reference: go to https://github.com/boot-services/metadata-service/tree/with-mongodb. Check docker files in this order: 1.  Dockerfile 2.  Dockerfie.optimised.d 3.  Dockerfile.multistage.d 4.  Dockerfile.multistage.optimized.d</p> <p>CMD and ENTRYPOINT in docker: - If we see any dockerfile there we can find CMD, it defines the program that will be run when the container starts. - For nginx image -&gt; CMD[\"nginx\"]</p> <ul> <li> <p>Then there is another keyword ENTRYPOINT. Here can specify whatever program we want to start at the start of the container and if we add anything in the command line then it will be appended with the ENTRYPOINT. If we may have any default value, then we can pass that in CMD and if we specify more than one value then we must provide list of string on CMD or ENTRYPOINT.</p> </li> <li> <p>If we want to modify the ENTRYPOINT during runtime like sleep to sleep-v2 then we can do like this:</p> </li> <li>Previous version: docker run ubuntu-sleeper</li> <li>New version: docker run --entrypoint sleep-v2 ubuntu-sleeper 20</li> </ul> <p>Ubuntu-sleeper image: it will start a ubuntu os and then start sleep process for 10 seconds: </p><pre><code>FROM UBUNTU                 # Base image\nENTRYPOINT [\"sleep\"]        # Command on start\nCMD [\"10\"]                  # default value\n</code></pre>"},{"location":"container/docker/#docker-is-one-of-the-implementations-of-the-virtualization-technology","title":"Docker is one of the implementations of the virtualization technology.","text":"<p>Before docker we have hypervisor. </p>"},{"location":"container/docker/#following-is-the-hypervisor-based-architecture-virtualization-happens-on-physical-layer","title":"Following is the Hypervisor based architecture. Virtualization happens on physical layer.","text":"<ul> <li>Physical layer</li> <li>Host operating system</li> <li>Hyepervisor (VMware, Virtual Box)<ul> <li>Guest OS</li> <li>Binaries/libraries</li> <li>Applications</li> </ul> </li> </ul>"},{"location":"container/docker/#benefits","title":"Benefits","text":"<ul> <li>Cost efficient</li> <li>Easy to scale</li> </ul>"},{"location":"container/docker/#limitations","title":"Limitations:","text":"<ul> <li>Kernal Resource Duplication</li> <li>Application portability issue</li> <li>Runtime isolation. (like running JRE 7 and JRE8 in same system)</li> </ul>"},{"location":"container/docker/#following-is-the-container-based-architecture-is-like-virtualization-happens-on-operating-system-layer-docker-client-server-architecture","title":"Following is the Container based architecture is like. Virtualization happens on operating system layer. Docker client server architecture","text":"<ul> <li>Physical layer</li> <li>Host operating system</li> <li>Container Engine<ul> <li>Binaries/libraries</li> <li>Applications</li> </ul> </li> </ul>"},{"location":"container/docker/#benefits_1","title":"Benefits","text":"<ul> <li>Cost efficient</li> <li>Fast time for boot up and deployment.</li> <li>Portability </li> </ul>"},{"location":"container/docker/#sample-docker-files","title":"Sample docker files","text":"<p>Single stage not optimized dockerfile:</p> <p>This Dockerfile is a simple example of building a Docker image in a single stage. It is not optimized for size or build efficiency.</p> <pre><code># Use the official OpenJDK 11 runtime as the base image\nFROM openjdk:11-jre-slim\n\n# Define a build argument JAR_FILE with a default value of target/*.jar\nARG JAR_FILE=target/*.jar\n\n# Copy the JAR file from the host to the container\nCOPY ${JAR_FILE} app.jar\n\n# Expose port 8080 to the outside world\nEXPOSE 8080\n\n# Define the command to run the application\nENTRYPOINT [\"java\",\"-jar\",\"-Xms256m\", \"-Xmx512m\",\"/app.jar\"]\n</code></pre> <p>Explanation: - <code>FROM openjdk:11-jre-slim</code>: This line specifies the base image to use, which is the official OpenJDK 11 runtime. - <code>ARG JAR_FILE=target/*.jar</code>: This line defines a build argument named <code>JAR_FILE</code> with a default value of <code>target/*.jar</code>. - <code>COPY ${JAR_FILE} app.jar</code>: This line copies the JAR file from the host machine to the container, renaming it to <code>app.jar</code>. - <code>EXPOSE 8080</code>: This line exposes port 8080, allowing the application to be accessible on this port. - <code>ENTRYPOINT [\"java\",\"-jar\",\"-Xms256m\", \"-Xmx512m\",\"/app.jar\"]</code>: This line defines the command to run the application, specifying the Java runtime options and the path to the JAR file.</p> <p>Multi stage optimized dockerfile:</p> <p>This Dockerfile is an example of building a Docker image using multiple stages. It is optimized for size and build efficiency by separating the build and runtime stages.</p> <pre><code># ===========   BUILD STAGE =====================\n# Use the official Maven image to build the application\nFROM maven AS build\n\n# Set the working directory inside the container\nWORKDIR /workspace/app\n\n# Copy the pom.xml file and download the project dependencies\nCOPY pom.xml pom.xml\nRUN mvn dependency:go-offline -B\n\n# Copy the source code and build the application\nCOPY src src\nRUN mvn package -DskipTests\n\n# Extract the built JAR file for the runtime stage\nRUN mkdir -p target/dependency &amp;&amp; (cd target/dependency; jar -xf ../*.jar)\n\n# =========== RUN STAGE =====================\n# Use the official OpenJDK 11 runtime as the base image\nFROM openjdk:11-jre-slim\n\n# Create a volume for temporary files\nVOLUME /tmp\n\n# Define a build argument for the dependency directory\nARG DEPENDENCY=/workspace/app/target/dependency\n\n# Copy the extracted JAR files from the build stage\nCOPY --from=build ${DEPENDENCY}/BOOT-INF/lib /app/lib\nCOPY --from=build ${DEPENDENCY}/META-INF /app/META-INF\nCOPY --from=build ${DEPENDENCY}/BOOT-INF/classes /app\n\n# Expose port 8080 to the outside world\nEXPOSE 8080\n\n# Define the command to run the application\nENTRYPOINT [\"java\",\"-cp\",\"app:app/lib/*\",\"com.github.typicalitguy.DigitalLayerApplication\"]\n</code></pre> <p>Explanation: - <code>FROM maven AS build</code>: This line specifies the base image for the build stage, which is the official Maven image. - <code>WORKDIR /workspace/app</code>: This line sets the working directory inside the container. - <code>COPY pom.xml pom.xml</code>: This line copies the <code>pom.xml</code> file to the container. - <code>RUN mvn dependency:go-offline -B</code>: This line downloads the project dependencies. - <code>COPY src src</code>: This line copies the source code to the container. - <code>RUN mvn package -DskipTests</code>: This line builds the application, skipping the tests. - <code>RUN mkdir -p target/dependency &amp;&amp; (cd target/dependency; jar -xf ../*.jar)</code>: This line extracts the built JAR file for the runtime stage. - <code>FROM openjdk:11-jre-slim</code>: This line specifies the base image for the runtime stage, which is the official OpenJDK 11 runtime. - <code>VOLUME /tmp</code>: This line creates a volume for temporary files. - <code>ARG DEPENDENCY=/workspace/app/target/dependency</code>: This line defines a build argument for the dependency directory. - <code>COPY --from=build ${DEPENDENCY}/BOOT-INF/lib /app/lib</code>: This line copies the extracted JAR files from the build stage. - <code>COPY --from=build ${DEPENDENCY}/META-INF /app/META-INF</code>: This line copies the extracted JAR files from the build stage. - <code>COPY --from=build ${DEPENDENCY}/BOOT-INF/classes /app</code>: This line copies the extracted JAR files from the build stage. - <code>EXPOSE 8080</code>: This line exposes port 8080, allowing the application to be accessible on this port. - <code>ENTRYPOINT [\"java\",\"-cp\",\"app:app/lib/*\",\"com.github.typicalitguy.DigitalLayerApplication\"]</code>: This line defines the command to run the application, specifying the Java runtime options and the path to the application classes.</p> <p>Maven has a shortcut to build spring boot image: </p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\n    &lt;parent&gt;\n        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;\n        &lt;version&gt;2.6.7&lt;/version&gt;\n        &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;\n    &lt;/parent&gt;\n    &lt;groupId&gt;com.github.typicalitguy&lt;/groupId&gt;\n    &lt;artifactId&gt;ARC_REACTOR_DIGITAL&lt;/artifactId&gt;\n    &lt;version&gt;1.0.0&lt;/version&gt;\n    &lt;name&gt;ARC_REACTOR_DIGITAL&lt;/name&gt;\n    &lt;description&gt;Digital layer ARC_REACTOR Microservice&lt;/description&gt;\n    &lt;properties&gt;\n        &lt;java.version&gt;11&lt;/java.version&gt;\n        &lt;spring-cloud.version&gt;2021.0.1&lt;/spring-cloud.version&gt;\n    &lt;/properties&gt;\n    ...\n&lt;build&gt;\n     &lt;plugins&gt;\n      &lt;plugin&gt;\n       &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\n       &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;\n       &lt;configuration&gt;\n        &lt;image&gt;\n         &lt;name&gt;abhishek1009/${project.artifactId}:${project.version}&lt;/name&gt;\n        &lt;/image&gt;\n        &lt;pullPolicy&gt;IF_NOT_PRESENT&lt;/pullPolicy&gt;\n       &lt;/configuration&gt;\n      &lt;/plugin&gt;\n        &lt;/plugins&gt;\n    &lt;/build&gt;\n&lt;/project&gt;\n</code></pre> <p>Build image by maven: </p><pre><code>mvn spring-boot:built-image\n</code></pre> <p>Build image with skip test: </p><pre><code>mvn spring-boot:built-image -DskipTests\n</code></pre>"},{"location":"container/docker/#docker-build-and-docker-compose","title":"Docker build and docker compose","text":"<p>Build a docker image from Dockerfile: </p><pre><code>docker build -t repo_name/image_name:version dockerfile_location\ndocker build -t abhishek1009/arc-reactor-digital-test:1.0.0 .\n</code></pre> <p>Build a docker image from Dockerfile with different name: </p><pre><code>docker build -t repo_name/image_name:version -f &lt;Dockerfile_name&gt; dockerfile_location\ndocker build -t abhishek1009/arc-reactor-digital-test:1.0.1 -f Dockerfile.optimized .\n</code></pre> <p>If we do not use the docker compose then we have to run this following steps for running a set of microservices in local: </p><pre><code>docker build . -t voting-app\ndocker build . -t worker-app\ndocker build . -t result-app\n\ndocker run -d --name=redis redis\ndocker run -d -e POSTGRES_PASSWORD=10091997 --name=db postgres:9.4\n\ndocker run -p 5000:80 --link redis:redis voting-app\ndocker run -p 5001:80 --link db:db result-app\ndocker run --link redis:redis --link db:db worker-app\n</code></pre> <p>Instead we can do this but we need docker-compose.yaml: </p><pre><code>docker-compose up --build with docker-compose.yml\n</code></pre> <p>Sample docker-compose file</p> <pre><code>version: '3'\n\nservices:\n  app:\n    container_name: app\n    image: chat-app:latest\n    build:\n      context: .  # Set the build context to the current directory\n    ports:\n      - \"8000:80\"\n    volumes:\n      - ./:/home/workspace/chat-app  # Mounts the local directory to /home/workspace/chat-app in the container\n    depends_on:\n      - mongodb\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:80\"]\n      interval: 1m30s\n      timeout: 10s\n      retries: 3\n\n  mongodb:\n    image: mongo:latest\n    container_name: mongodb\n    ports:\n      - \"27017:27017\"\n    volumes:\n      - mongo-data:/data/db\n    networks:\n      - backend\n\n  nginx:\n    image: nginx:latest\n    container_name: nginx\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n    networks:\n      - frontend\n    depends_on:\n      - app\n\nnetworks:\n  frontend:\n  backend:\n\nvolumes:\n  mongo-data:\n</code></pre> <p>Explanation: - <code>version: '3'</code>: Specifies the version of the Docker Compose file format. - <code>services</code>: Defines the services that make up the application.   - <code>app</code>: The main application service.     - <code>container_name</code>: Sets the name of the container.     - <code>image</code>: Specifies the image to use.     - <code>build</code>: Defines the build context.     - <code>ports</code>: Maps the container port to the host port.     - <code>volumes</code>: Mounts the local directory to the container directory.     - <code>depends_on</code>: Specifies the dependencies for this service.     - <code>healthcheck</code>: Defines the health check configuration.   - <code>mongodb</code>: The MongoDB service.     - <code>image</code>: Specifies the image to use.     - <code>container_name</code>: Sets the name of the container.     - <code>ports</code>: Maps the container port to the host port.     - <code>volumes</code>: Mounts the volume to the container directory.     - <code>networks</code>: Specifies the network for this service.   - <code>nginx</code>: The Nginx service.     - <code>image</code>: Specifies the image to use.     - <code>container_name</code>: Sets the name of the container.     - <code>ports</code>: Maps the container port to the host port.     - <code>volumes</code>: Mounts the local configuration file to the container directory.     - <code>networks</code>: Specifies the network for this service. - <code>networks</code>: Defines the networks for the services.   - <code>frontend</code>: The frontend network.   - <code>backend</code>: The backend network. - <code>volumes</code>: Defines the volumes for the services.   - <code>mongo-data</code>: The volume for MongoDB data.</p> <p>Quick overview of an image </p><pre><code>docker scout quickview\n</code></pre> <p>For more go to the reference</p>"},{"location":"container/links/","title":"Links","text":""},{"location":"container/links/#youtube","title":"Youtube","text":""},{"location":"container/links/#youtube-videos-docker-container-runtime-containerd-kubernetes","title":"Youtube videos (docker, container runtime, containerd, kubernetes)","text":"<ul> <li>Okay, but do you REALLY need containers?</li> <li>Using docker in unusual ways</li> <li>100+ Docker Concepts you Need to Know</li> <li>The Only Docker Tutorial You Need To Get Started</li> <li>you need to learn Docker RIGHT NOW!!</li> <li>Docker networking is CRAZY!!</li> <li>Understanding Docker Architecture</li> <li>Docker Architecture | Docker components : daemon, containerd, containerd-shim, runc</li> <li>Introduction to Docker for CTFs</li> <li>How Docker Works - Intro to Namespaces</li> <li>Deepdive Containers - Kernel Sources and nsenter</li> <li>An introduction to cgroups, runc &amp; containerD</li> <li>Containers: cgroups, Linux kernel namespaces, ufs, Docker, and intro to Kubernetes pods</li> <li>What is a Container Runtime?</li> <li>Containerd - An open and reliable container runtime (CNCFMinutes 18)</li> <li> <p>Docker vs Containerd: Understanding the Differences and Choosing the Right Containerization Tool</p> </li> <li> <p>Podman vs. Docker</p> </li> <li>you should be using PODMAN</li> <li> <p>Docker vs Podman Explained</p> </li> <li> <p>Effortless Docker Management with LazyDocker: A Terminal UI for Containers, Images and Networks!</p> </li> </ul>"},{"location":"container/links/#docker","title":"Docker","text":"<ul> <li>Piyush Garg</li> <li>Docker | Beginners</li> <li>Master Docker Containerisation &amp; Deployments</li> <li> <p>Docker Management API - Build Your Own Docker Orchestration</p> </li> <li> <p>docker personal</p> </li> </ul>"},{"location":"container/links/#blogs","title":"Blogs","text":""},{"location":"container/links/#bootable-containers","title":"Bootable Containers","text":"<ul> <li>Fedora/CentOS bootc Documentation</li> </ul>"},{"location":"container/links/#medium","title":"Medium","text":"<ul> <li>Dockerfile cheat sheet</li> <li>6 Tips to Optimize Your Dockerfile</li> <li>4 Docker Options You May Not Know</li> <li>Docker Basic Interview Questions \u2014 How Many Can You Answer?</li> <li>DevOps in K8s \u2014 Write Dockerfile Efficiently</li> </ul>"},{"location":"elk-stack/links/","title":"links","text":""},{"location":"elk-stack/links/#elk-stack","title":"ELK Stack","text":""},{"location":"elk-stack/links/#udemy","title":"Udemy","text":"<ul> <li>Ultimate Elasticsearch 8, Kibana, Logstash, Beats: ELK Stack</li> <li> <p>ELASTIC 8: Monitoring with Elasticsearch, Kibana, Beats, APM</p> </li> <li> <p>Complete Guide to Elasticsearch</p> </li> <li>Data Visualization with Kibana</li> <li>Data Processing with Logstash (and Filebeat)</li> </ul>"},{"location":"git/advanced/","title":"advanced","text":""},{"location":"git/advanced/#generate-a-new-ssh-key","title":"Generate a New SSH Key","text":"<p>Generate a new SSH key: </p><pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre>"},{"location":"git/advanced/#ssh-related-files","title":"SSH Related Files","text":"<ul> <li><code>~/.ssh/config</code></li> <li><code>~/.ssh/id_ed25519</code></li> <li><code>~/.ssh/id_ed25519.pub</code></li> </ul> <p>When you generate new SSH keys using <code>ssh-keygen</code> and change the default key name, the keys are saved in the directory you specify. If you don't specify a directory, the keys are saved in the current working directory.</p>"},{"location":"git/advanced/#contents-of-sshconfig","title":"Contents of <code>~/.ssh/config</code>","text":"<pre><code>Host *\n   AddKeysToAgent yes\n   UseKeychain yes\n   IdentityFile ~/.ssh/id_ed25519\n</code></pre>"},{"location":"git/advanced/#multiple-github-accounts-on-a-single-machine","title":"Multiple GitHub Accounts on a Single Machine","text":"<p>Configure SSH for multiple GitHub accounts: </p><pre><code># Default GitHub account (Old account, if any)\nHost github.com\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/id_ed25519\n\n# New GitHub account\nHost github.com-personal\n  HostName github.com\n  User git\n  IdentityFile ~/.ssh/id_ed25519_personal\n</code></pre> <p>Set the remote URL for the new GitHub account: </p><pre><code>git remote set-url origin git@github.com-personal:abhishekghoshh/github-helper.git\n</code></pre> <p>Clone the repository using the new GitHub account: </p><pre><code>git clone git@github.com-personal:abhishekghoshh/github-helper.git\n</code></pre> <p>Also, set the git config <code>user.email</code> and <code>user.name</code> otherwise it will take the default.</p>"},{"location":"git/advanced/#check-ssh-configuration","title":"Check SSH Configuration","text":"<p>Test the SSH connection for the default GitHub account: </p><pre><code>ssh -T git@github.com\n</code></pre> <p>Test the SSH connection for the new GitHub account: </p><pre><code>ssh -T git@github.com-personal\n</code></pre>"},{"location":"git/advanced/#check-ssh-key-list","title":"Check SSH Key List","text":"<p>List all the SSH keys added to the SSH agent: </p><pre><code>ssh-add -l\n</code></pre>"},{"location":"git/advanced/#add-a-key-to-ssh-key-set","title":"Add a Key to SSH Key Set","text":"<p>Add a specific SSH key to the SSH agent: </p><pre><code>ssh-add ~/.ssh/id_ed25519_personal\n</code></pre> <p>Note: While pushing using git SSH, if you see it is showing another user and it has no access (which is normal), change the git remote URL.</p>"},{"location":"git/advanced/#working-with-git-sha-and-blob-files","title":"Working with Git SHA and Blob Files","text":"<p>Get SHA1 for a file: </p><pre><code>git ls-files -s\n</code></pre> <p>Get type of object (blob or tree): </p><pre><code>git cat-file -t SHA1_for_Object\n</code></pre> <p>Get size of object (blob or tree): </p><pre><code>git cat-file -s SHA1_for_Object\n</code></pre> <p>Get content of the object (blob or tree): </p><pre><code>git cat-file -p SHA1_for_Object\n</code></pre> <p>Shows the details of a specific commit, including its changes: </p><pre><code>git show\n</code></pre> <p>Display content of a specific commit: </p><pre><code>git show SHA1_of_commit\n</code></pre> <p>Git is a key-value store. There are 2 types of commands: <code>Porcelain</code> commands and <code>plumbing</code> commands. Git hashes the object and stores the history of the object in the directory. Git object content includes tree and blob.</p>"},{"location":"git/branching/","title":"branching","text":""},{"location":"git/branching/#git-branching-model","title":"Git branching model","text":"<p>List all branches </p><pre><code>git branch\ngit branch --list\n</code></pre> <p>List all branches with remote branches </p><pre><code>git branch -a\n</code></pre> <p>How to create a new branch in Git: By default, you have one branch, the main branch. With this command, you can create a new branch. Git won't switch to it automatically - you will need to do it manually with the next command. </p><pre><code>git branch branch_name\n</code></pre> <p>How to list branches in Git: You can view all created branches using the <code>git branch</code> command. It will show a list of all branches and mark the current branch with an asterisk and highlight it in green. </p><pre><code>git branch\n</code></pre> <p>How to delete a branch in Git: When you are done working with a branch and have merged it, you can delete it using the command below. Remove selected branch, if it is already merged into any other. (-D instead of -d forces deletion.) </p><pre><code>git branch -d branch_name\n</code></pre> <p>Force delete a local branch (whether merged or unmerged) </p><pre><code>git branch -D &lt;branch&gt;\n</code></pre> <p>Rename the current branch to <code>&lt;new_name&gt;</code> </p><pre><code>git branch -m &lt;new_name&gt;\n</code></pre> <p>How to check remote branches that Git is tracking: This command shows the name of all remote branches that Git is tracking for the current repository: </p><pre><code>git branch -r\n</code></pre> <p>Display list commit in different branches </p><pre><code>git branch -v\n</code></pre> <p>Display tracked branches </p><pre><code>git branch -vv\n</code></pre>"},{"location":"git/branching/#switch-branches","title":"Switch branches","text":"<p>How to switch to a newly created branch in Git: When you want to use a different or a newly created branch you can use this command: </p><pre><code>git checkout branch_name\ngit switch branch_name\n</code></pre> <p>How to create a branch in Git and switch to it immediately. In a single command, you can create and switch to a new branch right away. </p><pre><code>git checkout -b branch_name\n</code></pre> <p>Checking out (switching to) older commits. Checks out the third-to-last commit </p><pre><code>git checkout HEAD~3\n</code></pre> <p>Checking out (switching to) older commits </p><pre><code>git checkout &lt;commit_id&gt;\n</code></pre> <p>How to revert unstaged changes in Git: </p><pre><code>git checkout filename\n</code></pre>"},{"location":"git/branching/#how-to-merge-two-branches-in-git","title":"How to merge two branches in Git","text":"<p>To merge the history of the branch you are currently in with the <code>branch_name</code>, you will need to use the command below: Merging a branch into the main branch. Join specified <code>[branch_name]</code> branch into your current branch (the one you are on currently). </p><pre><code>git checkout main\ngit merge branch_name\n</code></pre> <p>Merging a branch and creating a commit message </p><pre><code>git merge --no-ff &lt;other_branch&gt;\n</code></pre> <p>How to abort a conflicting merge in Git: If you want to throw a merge away and start over, you can run the following command: </p><pre><code>git merge --abort\n</code></pre>"},{"location":"git/cloning/","title":"cloning","text":""},{"location":"git/cloning/#git-clone","title":"Git clone","text":"<p>Clone a repository from remote hosts (GitHub, GitLab, DagsHub, etc.) </p><pre><code>git clone &lt;remote_repo_url&gt;\n</code></pre> <p>Using HTTPS link </p><pre><code>git clone https://github.com/your_username/repo_name.git\n</code></pre> <p>Using SSH link </p><pre><code>git clone git@github.com:user_name/repo_name.git\n</code></pre> <p>Clone only a specific branch </p><pre><code>git clone -branch &lt;branch_name&gt; &lt;repo_url&gt;\n</code></pre> <p>Cloning into a specified directory </p><pre><code>git clone &lt;repo_url&gt; &lt;dir_name&gt;\n</code></pre>"},{"location":"git/commits/","title":"commits","text":""},{"location":"git/commits/#commit-changes","title":"Commit changes","text":"<p>How to commit changes in the editor in Git: This command will open a text editor in the terminal where you can write a full commit message. A commit message is made up of a short summary of changes, an empty line, and a full description of the changes after it. </p><pre><code>git commit\n</code></pre> <p>How to commit changes with a message in Git: You can add a commit message without opening the editor. This command lets you only specify a short summary for your commit message. </p><pre><code>git commit -m \"your commit message here\"\n</code></pre> <p>How to commit changes (and skip the staging area) in Git: You can add and commit tracked files with a single command by using the <code>-a</code> and <code>-m</code> options or <code>-am</code> option. </p><pre><code>git commit -a -m \"your commit message here\"\ngit commit -am \"your commit message here\"\n</code></pre> <p>How to amend the most recent commit in Git: <code>--amend</code> allows you to modify and add changes to the most recent commit. Edit commit message. Replace the last commit with the staged changes and last commit combined. Use with nothing staged to edit the last commit's message. </p><pre><code>git commit --amend\ngit commit --amend -m \"[New commit message]\"\n</code></pre> <p>In a merge conflict resolve after fixing the conflict: </p><pre><code>git commit -am \"Resolved merge conflicts and merged story index\"\n</code></pre> <p>!!Note!!: fixing up a local commit with amend is great and you can push it to a shared repository after you've fixed it. But you should avoid amending commits that have already been made public.</p> <p>Creates a new note and associates it with an object (commit, tag, etc.): </p><pre><code>git notes add\n</code></pre>"},{"location":"git/commits/#show-git-commit","title":"Show git commit","text":"<p>How to see a specific commit in Git: This command shows a specific commit. Replace <code>commit-id</code> with the id of the commit that you find in the commit log after the word commit. </p><pre><code>git show commit-id\n</code></pre> <p>Show any object in Git in human-readable format: </p><pre><code>git show [SHA]\n</code></pre>"},{"location":"git/commits/#uncommitted-changes","title":"Uncommitted Changes","text":"<p>Temporarily store modified, tracked files in order to change branches:</p> <p>Saving staged and unstaged changes to stash for later use: Put current changes in your working directory into stash for later use. </p><pre><code>git stash\n</code></pre> <p>Stashing staged, unstaged and untracked files as well: </p><pre><code>git stash -u\n</code></pre> <p>Stashing everything (including ignored files): </p><pre><code>git stash --all\n</code></pre> <p>Reapply previously stashed changes and empty the stash: Apply stored stash content into working directory, and clear stash. </p><pre><code>git stash pop\n</code></pre> <p>Show the 2nd stash: </p><pre><code>git stash show stash@{1}\n</code></pre> <p>Pop the 2nd stash: </p><pre><code>git stash pop stash@{1}\n</code></pre> <p>Reapply previously stashed changes and keep the stash: </p><pre><code>git stash apply\n</code></pre> <p>Dropping changes in the stash, discard the changes from top of stash stack: Delete a specific stash from all your previous stashes. </p><pre><code>git stash drop\n</code></pre> <p>Lists all stashes in the repository: </p><pre><code>git stash list\n</code></pre>"},{"location":"git/commits/#rollback","title":"Rollback","text":"<p>How to rollback to previous commit:</p> <p><code>git revert</code> will create a new commit that is the opposite of everything in the given commit: We can revert the latest commit by using the head alias like this: </p><pre><code>git revert HEAD\n</code></pre> <p>How to rollback an old commit in Git: You can revert an old commit using its commit id. This opens the editor so you can add a commit message. Undo a single given commit, without modifying commits that come after it (a safe reset). May result in revert conflicts. Create a new commit, reverting changes from the specified commit. It generates an inversion of changes. </p><pre><code>git revert commit_id_here\n</code></pre> <p>Undoes the changes introduced by the specified commit, but does not create a new commit: </p><pre><code>git revert --no-commit &lt;commit&gt;\n</code></pre> <p>Shows which files would be removed from working directory: Use the <code>-f</code> flag in place of the <code>-n</code> flag to execute the clean. </p><pre><code>git clean -n\n</code></pre>"},{"location":"git/configurations/","title":"configurations","text":""},{"location":"git/configurations/#git-configurations","title":"Git configurations","text":""},{"location":"git/configurations/#check-if-installation-is-successful-on-any-platform","title":"Check if installation is successful on any platform","text":"<pre><code>git --version\n</code></pre>"},{"location":"git/configurations/#ignoring-files","title":"Ignoring files","text":"<p>Create a .gitignore file and commit it. Show all commit logs with indication of any paths that moved. To ignore files, create a <code>.gitignore</code> file in your repository with a line for each pattern. File ignoring will work for the current and subdirectories where <code>.gitignore</code> file is placed. In this example, all files are ignored in the <code>logs</code> directory (excluding the <code>.gitkeep</code> file), whole <code>tmp</code> directory, and all files <code>*.swp</code>.  </p> <pre><code>logs/\n*.notes\npattern*/\n/logs *\n!logs/.gitkeep\n/tmp\n*.swp\nEOF\n</code></pre>"},{"location":"git/configurations/#commands-for-viewing-and-modifying-git-configurations","title":"Commands for viewing and modifying git configurations","text":"<p>System-wide ignore pattern for all local repositories </p><pre><code>git config --global core.excludesfile [file]\n</code></pre> <p>How to check your Git configuration: The command below returns a list of information about your git configuration, including username and email: </p><pre><code>git config -l / git config --list\n</code></pre> <p>Get the value of a single key </p><pre><code>git config --get &lt;key&gt;\n</code></pre> <p>Configure your email </p><pre><code>git config user.email [your.email@domain.com]\n</code></pre> <p>Configure your name </p><pre><code>git config user.name [your-name]\n</code></pre> <p>How to set up your Git username: With the command below, you can configure your username: </p><pre><code>git config --global user.name \"Abhishek Ghosh\"\n</code></pre> <p>How to set up your Git user email: This command lets you set up the user email address you'll use in your commits. All git projects under the current user </p><pre><code>git config --global user.email \"abhishek.ghosh@gmail.com\"\n</code></pre> <p>Local directory, single project (this is the default tag) </p><pre><code>git config --local user.email \"my_email@example.com\"\n</code></pre> <p>For all users on the current machine </p><pre><code>git config --system user.email \"my_email@example.com\"\n</code></pre> <p>Set automatic command-line coloring for Git for easy reviewing </p><pre><code>git config --global color.ui auto\n</code></pre> <p>How to cache your login credentials in Git: You can store login credentials in the cache so you don't have to type them in each time. Just use this command: </p><pre><code>git config --global credential.helper cache\n</code></pre> <p>Create shortcut for a Git command. E.g. <code>alias.glog \"log --graph --oneline\"</code> will set <code>git glog</code> equivalent to <code>git log --graph --oneline</code>. </p><pre><code>git config --global alias.&lt;alias-name&gt; &lt;git-command&gt;\n</code></pre> <p>Create an alias named gc for the <code>git commit</code> command </p><pre><code>git config --global alias.gc commit\ngc -m \"New commit\"\n</code></pre> <p>Create an alias named ga for the <code>git add</code> command </p><pre><code>git config --global alias.ga add\n</code></pre> <p>Set text editor used by commands for all users on the machine. <code>&lt;editor&gt;</code> arg should be the command that launches the desired editor (e.g., <code>vi</code>). </p><pre><code>git config --system core.editor &lt;editor&gt;\n</code></pre> <p>Open the global configuration file in a text editor for manual editing. </p><pre><code>git config --global --edit\n</code></pre>"},{"location":"git/intialization/","title":"initialization","text":""},{"location":"git/intialization/#git-project-initialization","title":"Git project initialization","text":"<p>How to initialize a Git repo: Everything starts from here. The first step is to initialize a new Git repo locally in your project root. You can do so with the command below: </p><pre><code>git init\n</code></pre> <p>Create a git-tracked repository inside a new directory </p><pre><code>git init [dir_name]\n</code></pre>"},{"location":"git/intialization/#add-a-remote-repository-in-git","title":"Add a remote repository in Git","text":"<p>Display if the repo is from the remote </p><pre><code>git remote\n</code></pre> <p>How to see remote URLs in Git: You can see all remote repositories for your local repository with this command: </p><pre><code>git remote -v\n</code></pre> <p>Add a remote repository to your local repository Just replace <code>&lt;url_to_remote&gt;</code> with your remote repo URL. </p><pre><code>git remote add &lt;remote&gt; &lt;url_to_remote&gt;\ngit remote add origin &lt;http or ssh url&gt;\n</code></pre> <p>How to get more info about a remote repo in Git: Just replace <code>origin</code> with the name of the remote obtained by running the <code>git remote -v</code> command. </p><pre><code>git remote show origin\n</code></pre> <p>Remove a connection to a remote repo called <code>&lt;remote&gt;</code> </p><pre><code>git remote rm &lt;remote&gt;\n</code></pre> <p>Rename a remote connection </p><pre><code>git remote rename &lt;old_name&gt; &lt;new_name&gt;\n</code></pre> <p>To get the contents of remote branches in Git without automatically merging: This lets you update the remote without merging any content into the local branches. You can call <code>git merge</code> or <code>git checkout</code> to do the merge. </p><pre><code>git remote update\n</code></pre>"},{"location":"git/introduction/","title":"introduction","text":""},{"location":"git/introduction/#introduction","title":"Introduction","text":"<p>Git is a distributed version control system that helps developers collaborate on projects of any scale. Linus Torvalds, the developer of the Linux kernel, created Git in 2005 to help control the Linux kernel's development.</p>"},{"location":"git/introduction/#what-is-a-distributed-version-control-system","title":"What is a Distributed Version Control System?","text":"<p>A distributed version control system is a system that helps you keep track of changes you've made to files in your project. This change history lives on your local machine and lets you revert to a previous version of your project with ease in case something goes wrong.</p> <p>Git makes collaboration easy. Everyone on the team can keep a full backup of the repositories they're working on on their local machine. Then, thanks to an external server like BitBucket, GitHub or GitLab, they can safely store the repository in a single place. This way, different members of the team can copy it locally and everyone has a clear overview of all changes made by the whole team.</p> <p>Git has many different commands you can use. And I've found that these fifty are the ones I use the most often (and are therefore the most helpful to remember).</p>"},{"location":"git/introduction/#basic-definitions","title":"Basic Definitions","text":"<ul> <li>git: an open source, distributed version-control system</li> <li>GitHub: a platform for hosting and collaborating on Git repositories</li> <li>Commit: a state of the code base. A snapshot of the project you can come back to. A Git object, a snapshot of your entire repository compressed into a SHA</li> <li>Branch: A copy of the project used for working in an isolated environment without affecting the main project. a reference to a commit; can have a tracked upstream. A lightweight movable pointer to a commit</li> <li>Local repo or repository: A local directory containing code and files for the project</li> <li>Remote: An online version of the local repository hosted on services like GitHub, GitLab, and BitBucket. A common repository on GitHub that all team member use to exchange their changes</li> <li>Clone: The act of making a clone or copy of a repository in a new directory. A local version of a repository, including all commits and branches</li> <li>fork: A copy of a repository on GitHub owned by a different user</li> <li>Git merge: The process of combining two branches together</li> <li>pull request: a place to compare and discuss the differences introduced on a branch with reviews, comments, integrated tests, and more</li> <li>HEAD: representing your current working directory, the HEAD pointer can be moved to different branches, tags, or commits</li> <li>Tag: a reference (standard) or an object (annotated)</li> </ul>"},{"location":"git/introduction/#more-advanced-definitions","title":"More Advanced Definitions","text":"<ul> <li>.gitignore file: A file that lists other files you want git not to track (e.g. large data folders, private info, and any local files that shouldn't be seen by the public.)</li> <li>Staging area: a cache that holds changes you want to commit next.</li> <li>Git stash: another type of cache that holds unwanted changes you may want to come back later</li> <li>Commit ID or hash: a unique identifier for each commit, used for switching to different save points.</li> <li>HEAD (always capitalized letters): a reference name for the latest commit, to save you having to type Commit IDs. HEAD\\~n syntax is used to refer to older commits (e.g. HEAD\\~2 refers to the second-to-last commit).</li> <li>ssh authentication: You can access and write data in repositories on github.com using the ssh(Secure shell protocol). When you connect via ssh, you authenticate using a private key file saved on your local machine.</li> </ul>"},{"location":"git/introduction/#what-is-a-branch","title":"What is a Branch?","text":"<p>Branches are special \"copies\" of the code base which allow you to work on different parts of a project and new features in an isolated environment. Changes made to the files in a branch won't affect the \"main branch\" which is the main project development channel.</p>"},{"location":"git/introduction/#what-is-a-repository","title":"What is a Repository?","text":"<p>A repository or a repo is any location that stores code and the necessary files that allow it to run without errors. A repo can be both local and remote. A local repo is typically a directory on your machine while a remote repo is hosted on servers like GitHub.</p>"},{"location":"git/introduction/#a-note-on-cloning","title":"A Note on Cloning","text":"<p>There are two primary methods of cloning a repository - HTTPS syntax and SSH syntax. While SSH cloning is generally considered a bit more secure because you have to use an SSH key for authentication, HTTPS cloning is much simpler and the recommended cloning option by GitHub.</p> <p>HTTPS: <code>git clone https://github.com/your_username/repo_name.git</code></p> <p>SSH: <code>git clone git@github.com:user_name/repo_name.git</code></p>"},{"location":"git/links/","title":"links","text":""},{"location":"git/links/#git-helper","title":"git Helper","text":"<p>readme viewer online</p>"},{"location":"git/links/#blogs","title":"Blogs","text":"<ul> <li>How Git Version Control Works?</li> <li>Learn git concepts, not commands</li> <li>Syncing a fork</li> <li>git-flight-rules</li> <li>pro git book</li> <li>GitHub flow</li> <li>What is Git and Github?</li> </ul>"},{"location":"git/links/#cheetsheets","title":"Cheetsheets","text":"<ul> <li>Learn Git Fundamentals - A Handbook on Day-to-Day Development Tasks</li> <li>How to Use Git and GitHub - a Guide for Beginners and Experienced Developers</li> </ul>"},{"location":"git/links/#youtube","title":"Youtube","text":""},{"location":"git/links/#introduction","title":"Introduction","text":"<ul> <li>Fireship</li> <li>Git Explained in 100 Seconds</li> <li>Git It? How to use Git and Github</li> <li>GitHub Pull Request in 100 Seconds - Git a FREE sticker \ud83d\udd25</li> <li>13 Advanced (but useful) Git Techniques and Shortcuts</li> <li>ByteByteGo</li> <li>How Git Works: Explained in 4 Minutes</li> <li>Git MERGE vs REBASE: Everything You Need to Know</li> <li>Configure your Git</li> <li>Git Stash In 5 Minutes</li> <li>Git MERGE vs REBASE</li> <li>Difference between git PULL and git FETCH</li> <li>How to be a git expert</li> <li>How GIT works under the HOOD?</li> <li>You Have Been Using Git The WRONG Way</li> </ul>"},{"location":"git/links/#playlists","title":"Playlists","text":"<ul> <li>FreeCodeCamp</li> <li>Git and GitHub for Beginners - Crash Course</li> <li>Git for Professionals Tutorial - Tools &amp; Concepts for Mastering Version Control with Git</li> <li>Advanced Git Tutorial - Interactive Rebase, Cherry-Picking, Reflog, Submodules and more</li> <li>Git</li> <li>The Modern Coder</li> <li>Git</li> <li>Git \u2022 Definitive Guides</li> <li>Git \u2022 Quick Tips</li> <li>GIT - Complete Course Tutorial</li> </ul>"},{"location":"git/links/#tooling","title":"Tooling","text":"<ul> <li>lazygit</li> <li>LazyGit makes you 10x faster while using Git.</li> <li>I'm never using Git the same way again</li> <li>Neovim and Git: SOLVED</li> <li>15 Lazygit Features In Under 15 Minutes</li> <li>Git's Best And Most Unknown Feature</li> </ul>"},{"location":"git/links/#udemy","title":"Udemy","text":"<ul> <li>Master Git and GitHub in 5 Days: Go from Zero to Hero</li> <li>The Git &amp; Github Bootcamp</li> </ul>"},{"location":"git/logs/","title":"logs","text":""},{"location":"git/logs/#git-log","title":"Git log","text":"<p>How to see your commit history in Git: This command shows the commit history for the current repository. </p><pre><code>git log\n</code></pre> <p>Show the commit history of a remote branch: </p><pre><code>git log origin/master\n</code></pre> <p>Show the commit history of a specific branch: </p><pre><code>git log &lt;branch-name&gt;\n</code></pre> <p>Only display commits that have the specified file: </p><pre><code>git log -- &lt;file&gt;\n</code></pre> <p>List commit history of the current branch (-n count limits list to last n commits): </p><pre><code>git log [-n count]\n</code></pre> <p>How to see your commit history including changes in Git: This command shows the commit's history including all files and their changes. </p><pre><code>git log -p\n</code></pre> <p>How to see a minimal git log with commit ID and message only: This command lists commits with commit ID and commit message. </p><pre><code>git log --oneline\n</code></pre> <p>Display all commits from local and remote: </p><pre><code>git log --oneline --all\n</code></pre> <p>List one commit per line (-n tag can limit the number of commits displayed, e.g., -5): </p><pre><code>git log --oneline [-n]\n</code></pre> <p>Log commits after some date: A sample value can be \"2020-10-04\" or keywords such as \"yesterday\", \"last month\", etc. </p><pre><code>git log --oneline --after=\"YYYY-MM-DD\"\n</code></pre> <p>Log commits before some date: Both <code>--after</code> and <code>--before</code> tags can be used for date ranges. </p><pre><code>git log --oneline --before=\"last year\"\n</code></pre> <p>How to show the commit log as a graph in Git: We can use <code>--graph</code> to display the commit log as a graph. Adding <code>--oneline</code> limits commit messages to a single line. </p><pre><code>git log --graph --oneline\n</code></pre> <p>How to show the commit log as a graph of all branches in Git: Does the same as the command above but for all branches. </p><pre><code>git log --graph --oneline --all\n</code></pre> <p>Show a graph with branch names or tags: </p><pre><code>git log --graph --decorate\n</code></pre> <p>An overview with reference labels and history graph (one commit per line): </p><pre><code>git log --oneline --graph --decorate\n</code></pre> <p>How to see log stats in Git: This command will show statistics about changes in each commit, including lines changed and file names. </p><pre><code>git log --stat\n</code></pre> <p>Show all commit logs with indication of any paths that moved: </p><pre><code>git log --stat -M\n</code></pre> <p>Search for commits with a commit message that matches <code>&lt;pattern&gt;</code>: </p><pre><code>git log --grep=&lt;pattern&gt;\n</code></pre> <p>How to check the current commit log of a remote repo in Git: Find out the remote repository log using this command: </p><pre><code>git log origin/main\n</code></pre> <p>Show the commits on <code>branchA</code> that are not on <code>branchB</code>: </p><pre><code>git log branchB..branchA\n</code></pre> <p>Show the commits that changed a file, even across renames: Lists version history for a file, including renames. </p><pre><code>git log --follow [file]\n</code></pre> <p>List commits present on the current branch but not merged into ref (a branch name or a tag name): </p><pre><code>git log ref .\n</code></pre> <p>List commits present on ref but not merged into the current branch: </p><pre><code>git log .ref\n</code></pre> <p>Lists version history for the current branch from a certain author: </p><pre><code>git log --author=[name]\n</code></pre> <p>Shows who changed what and when in a file: </p><pre><code>git blame [file]\n</code></pre> <p>Show the commit log with a custom format: </p><pre><code>git log --pretty\n</code></pre> <p>Show the commit log with file names only: </p><pre><code>git log --name-only\n</code></pre> <p>List operations (e.g., checkouts or commits) made on the local repository: Display all commits, including hidden ones. Show a log of changes to the local repository's <code>HEAD</code>. Add <code>--relative-date</code> flag to show date info or <code>--all</code> to show all refs. <code>reflog</code> shows all the actions we have done in this repo, if we have done any <code>git reset --hard</code> then also we could revert back to the previous state. </p><pre><code>git reflog\n</code></pre>"},{"location":"git/modifying_files/","title":"modifying files","text":""},{"location":"git/modifying_files/#add-a-file-to-the-staging-area","title":"Add a file to the staging area","text":"<p>Add a file to the staging area: The command below will add a file to the staging area. Just replace <code>filename</code> with the name of the file you want to add to the staging area. </p><pre><code>git add fileName\n</code></pre> <p>How to add all files to the staging area in Git: If you want to add all files in your project to the staging area, you can use a wildcard <code>.</code> and every file will be added for you. <code>--all</code> works the same way. </p><pre><code>git add .\n</code></pre> <p>How to add only certain files to the staging area in Git: With the asterisk in the command below, you can add all files starting with 'fil' to the staging area. </p><pre><code>git add fil*\n</code></pre> <p>How to see changes using <code>git add -p</code>: This command opens a prompt and asks if you want to stage changes or not, and includes other options. </p><pre><code>git add -p\n</code></pre>"},{"location":"git/modifying_files/#renameremove-file","title":"Rename/Remove file","text":"<p>How to remove tracked files from the current working tree in Git: This command expects a commit message to explain why the file was deleted. Remove a file from the working directory and staging area. Removes a file from both the working directory and the repository, staging the deletion. </p><pre><code>git rm filename\n</code></pre> <p>Remove a file from the staging area but keep it in the working directory: </p><pre><code>git rm &lt;file-name&gt; --cached\n</code></pre> <p>Forcefully remove a file from the working directory and staging area: </p><pre><code>git rm &lt;file-name&gt; --forced\n</code></pre> <p>Remove Git repository: </p><pre><code>git rm -rf .git/\n</code></pre> <p>How to rename files in Git: This command stages the changes, then it expects a commit message. Moves or renames a file or directory in your Git repository. </p><pre><code>git mv oldfile newfile\n</code></pre>"},{"location":"git/modifying_files/#resetting-the-files","title":"Resetting the files","text":"<p>Revert a commit: It will apply the exact opposite changes that have been done in that commit and add a new commit. The previous changes will also be present in the commit history. </p><pre><code>git revert &lt;commit-hash&gt; \n</code></pre> <p>Example use case: Sarah has been adding additional content to the Hare and Tortoise story. She accidentally commits the current version before actually finishing the story. Help Sarah revert the last commit but retain the unfinished changes so that she can continue to finish her story. The last commit must not be part of the GIT history. </p><pre><code># The changes will be there but the HEAD will be pointing to the HEAD~1\ngit reset --soft HEAD~1\n\n# The changes will also be deleted\ngit reset --hard HEAD~1\n</code></pre> <p>Restore a file from the working directory: </p><pre><code>git restore &lt;file-name&gt;\n</code></pre> <p>Unstage a file while retaining the changes in the working directory: </p><pre><code>git restore --staged &lt;file-name&gt;\n</code></pre> <p>Discard changes and restore the last commit to the working tree: </p><pre><code>git restore &lt;file-name&gt;\n</code></pre> <p>Unstage files: </p><pre><code>git restore --staged &lt;file-name&gt;\n</code></pre> <p>Reset staging area to match the most recent commit, but leave the working directory unchanged: </p><pre><code>git reset\n</code></pre> <p>Reset staging area and working directory to match the most recent commit and overwrite all changes in the working directory: </p><pre><code>git reset --hard\n</code></pre> <p>Unstage a file while retaining the changes in the working directory: </p><pre><code>git reset &lt;file&gt;\n</code></pre> <p>Move the current branch tip backward to <code>commit</code>, reset the staging area to match, but leave the working directory alone: </p><pre><code>git reset &lt;commit&gt;\n</code></pre> <p>Undo the latest commit but leave the working directory unchanged: To change the version of a file [Moving between commits]. Change <code>HEAD</code> [retrieve commit to staging area]. </p><pre><code>git reset HEAD~1\n</code></pre> <p>Clear staging area, rewrite working tree from specified commit: Same as previous, but resets both the staging area &amp; working directory to match. Deletes uncommitted changes, and all commits after <code>commit</code>. Moves the branch pointer to a specified commit, discarding all changes in the staging area and the working directory, effectively resetting the repository to the specified commit. </p><pre><code>git reset --hard &lt;commit&gt;\n</code></pre> <p>Discard all changes of the latest commit (no easy recovery) [Be careful!]: </p><pre><code>git reset --hard HEAD~1\n</code></pre> <p>Switches the current branch to the target reference, leaving a difference as an uncommitted change: When <code>--hard</code> is used, all changes are discarded. It's easy to lose uncommitted changes with <code>--hard</code>. </p><pre><code>git reset [--hard] [target reference]\n</code></pre> <p>Moves the branch pointer to a specified commit, preserving the changes in the staging area and the working directory: </p><pre><code>git reset --soft &lt;commit&gt;\n</code></pre> <p>Instead of <code>HEAD~n</code>, you can provide a commit hash as well: Changes after that commit will be destroyed. How to revert staged changes in Git: You can use the <code>-p</code> option flag to specify the changes you want to reset. </p><pre><code>git reset HEAD &lt;file-name&gt;\ngit reset HEAD -p\n</code></pre>"},{"location":"git/modifying_files/#cherry-picking","title":"Cherry Picking","text":"<p>Apply the changes introduced by some existing commits: </p><pre><code># commit hash you identified in previous step\ngit cherry-pick &lt;commit-hash&gt;\n</code></pre> <p>Example use case:</p> <p>Sarah wants some commit from a master branch into her branch here cherry-pick makes much sense</p> <pre><code>sarah (story/hare-and-tortoise)$ git log master --oneline\ne9fc99d Added the story index file\ndb11317 Updated the story index file\n3eedaaf Added the fox and grapes story\n\nsarah (story/hare-and-tortoise)$ git cherry-pick db11317\n</code></pre>"},{"location":"git/project_status/","title":"project status","text":""},{"location":"git/project_status/#local-repository-status","title":"Local Repository Status","text":"<p>How to check a repository's status in Git This command will show the status of the current repository including staged, unstaged, and untracked files. Show modified files in the working directory, staged for your next commit. </p><pre><code>git status\n</code></pre> <p>For a short brief: </p><pre><code>git status -s\n</code></pre> <p>Displays ignored files in addition to the regular status output: </p><pre><code>git status --ignored\n</code></pre>"},{"location":"git/project_status/#local-changes-in-repository","title":"Local changes in Repository","text":"<p>How to see changes made before committing them using <code>diff</code> in Git </p> <p>View unstaged changes by default: You can pass a file as a parameter to only see changes on a specific file. Shows uncommitted changes since the last commit. Diff of what is changed but not staged. </p><pre><code>git diff\ngit diff all_checks.py\n</code></pre> <p>View changes that are staged but not yet committed: We can call <code>diff</code> with the <code>--staged</code> flag to see any staged changes. </p><pre><code>git diff --staged\n</code></pre> <p>Show difference between staged changes and last commit: </p><pre><code>git diff --cached\n</code></pre> <p>Display the difference between the current directory and the last commit: </p><pre><code>git diff HEAD\n</code></pre> <p>Show the differences between two commits (should provide the commit IDs): </p><pre><code>git diff &lt;commit_id_1&gt; &lt;commit_id_2&gt;\n</code></pre> <p>Compare a single <code>&lt;file&gt;</code> between two branches: </p><pre><code>git diff &lt;branch_1&gt; &lt;branch_2&gt; &lt;file&gt;\n</code></pre> <p>Show the diff of what is in <code>branchA</code> that is not in <code>branchB</code> </p><pre><code>git diff branchB...branchA\n</code></pre> <p>Show difference between working directory and last commit: </p><pre><code>git diff HEAD\n</code></pre>"},{"location":"git/pull_and_synchronize/","title":"pull and synchronize","text":""},{"location":"git/pull_and_synchronize/#pull-changes-fetch-merge","title":"Pull changes (Fetch + Merge)","text":"<p>If other team members are working on your repository, you can retrieve the latest changes made to the remote repository with the command below: Fetch changes from the remote and merge the current branch with its upstream. Fetch the specified remote's copy of the current branch and immediately merge it into the local copy. </p><pre><code>git pull\n</code></pre> <p>Fetch the remote's copy of the current branch and rebase it into the local copy: Uses <code>git rebase</code> instead of merge to integrate the branches. </p><pre><code>git pull --rebase &lt;remote&gt;\n</code></pre>"},{"location":"git/pull_and_synchronize/#synchronizing-repositories","title":"Synchronizing repositories","text":"<p>Retrieves changes from a remote repository, including new branches and commits: </p><pre><code>git fetch\n</code></pre> <p>This command will download the changes from a remote repo but will not perform a merge on your local branch (as <code>git pull</code> does that instead): Download all commits and branches from the remote without applying them on the local repo. Fetch changes from the remote, but not update tracking branches. </p><pre><code>git fetch remote\n</code></pre> <p>Fetch changes from the remote but not merge with the working tree: </p><pre><code>git fetch remote-name\ngit fetch origin\n</code></pre> <p>Fetches a specific <code>&lt;branch&gt;</code> from the repo: Leave off <code>&lt;branch&gt;</code> to fetch all remote refs. </p><pre><code>git fetch &lt;remote&gt; &lt;branch&gt;\n</code></pre> <p>Delete remote refs that were removed from the remote repository: </p><pre><code>git fetch --prune [remote]\n</code></pre> <p>Only download the specified <code>branch</code> from the <code>remote</code>: </p><pre><code>git fetch &lt;remote&gt; &lt;branch&gt;\n</code></pre> <p>Merge the fetched changes if accepted: If the remote repository has changes you want to merge with your local, then this command will do that for you: </p><pre><code>git merge &lt;remote&gt;/&lt;branch&gt;\ngit merge origin/main\n</code></pre> <p>Git <code>pull</code> is the same as fetching <code>origin master</code> and then merging <code>origin/master</code>: </p><pre><code>git fetch origin master\ngit merge origin/master\n\ngit pull origin master\n</code></pre>"},{"location":"git/push/","title":"push","text":""},{"location":"git/push/#push-changes","title":"Push changes","text":"<p>How to push changes to a remote repo in Git: When all your work is ready to be saved on a remote repository, you can push all changes using the command below: </p><pre><code>git push\n</code></pre> <p>Push a copy of local branch named branch to the remote repo </p><pre><code>git push &lt;remote_repo&gt; branch~\n</code></pre> <p>Delete a remote branch named branch (-d tag only works locally) </p><pre><code>git push &lt;remote_repo&gt; :branch\ngit push &lt;remote_repo&gt; --delete branch\n</code></pre> <p>How to push a new branch to a remote repo in Git: If you want to push a branch to a remote repository you can use the command below. Just remember to add <code>-u</code> to create the branch upstream. Push local branch to remote repository. Set its copy as an upstream. </p><pre><code>git push -u origin branch_name\n</code></pre> <p>How to force a push request in Git: (<code>-f</code> flag or <code>--force</code> flag) This command will force a push request. This is usually fine for pull request branches because nobody else should have cloned them. But this isn't something that you want to do with public repos. Do not use the <code>--force</code> flag unless you're absolutely sure you know what you're doing. </p><pre><code>git push -f origin branch_name\n</code></pre> <p>How to remove a remote branch in Git: If you no longer need a remote branch you can remove it using the command below: </p><pre><code>git push --delete origin branch_name_here\n</code></pre> <p>Push all of your local branches to the specified remote. </p><pre><code>git push &lt;remote&gt; --all\n</code></pre> <p>Push local changes to the remote. Use <code>--tags</code> to push tags. Tags aren't automatically pushed when you push a branch or use the <code>--all</code> flag. The <code>--tags</code> flag sends all of your local tags to the remote repo. </p><pre><code>git push [--tags] [remote]\ngit push &lt;remote&gt; --tags\n</code></pre>"},{"location":"git/rebase/","title":"rebase","text":""},{"location":"git/rebase/#git-rebase-rewrite-history","title":"Git rebase (REWRITE HISTORY)","text":"<p>You can transfer completed work from one branch to another using git rebase. Apply commits of the current working branch and apply them to the HEAD of [branch] to make the history of your branch more linear.</p>"},{"location":"git/rebase/#basic-rebase-command","title":"Basic Rebase Command","text":"<p>Rebase the current branch onto <code>&lt;branch_name&gt;</code>: <code>&lt;branch_name&gt;</code> can be a commit ID, branch name, a tag, or a relative reference to HEAD. Reapplies commits on the current branch onto the tip of the specified branch. </p><pre><code>git rebase &lt;branch_name&gt;\n</code></pre> <p>Rebase only the last 4 commits interactively: In the interactive session, use squash. </p><pre><code>git rebase -i HEAD~4 \n</code></pre> <p>Example git rebasing:</p> <p>Run the command <code>git rebase -i HEAD~3</code> to squash the last 3 commits into 1. In the editor that opens, leave the first line as is, and change the second and third lines to use squash instead of pick. Then save the file. This way we pick the first commit and then squash the second and third commits to it. In the next editor window that opens set the commit message to Add hare-and-tortoise story and save it. You can use your own appropriate message.</p> <p>Warning: Git Rebase can get really messy if you don't do it properly. Before using this command, I suggest that you re-read the official documentation here.</p>"},{"location":"git/rebase/#interactive-rebase","title":"Interactive Rebase","text":"<p>Run git rebase interactively using the <code>-i</code> flag: It will open the editor and present a set of commands you can use. Interactively rebase the current branch onto <code>&lt;base&gt;</code>. Launches editor to enter commands for how each commit will be transferred to the new base. </p><pre><code>git rebase -i &lt;base&gt;\n</code></pre> <p>Example: </p><pre><code>git rebase -i master\n</code></pre>"},{"location":"git/rebase/#interactive-rebase-commands","title":"Interactive Rebase Commands","text":"<ul> <li><code>p</code>, <code>pick</code> = use commit</li> <li><code>r</code>, <code>reword</code> = use commit, but edit the commit message</li> <li><code>e</code>, <code>edit</code> = use commit, but stop for amending</li> <li><code>s</code>, <code>squash</code> = use commit, but meld into previous commit</li> <li><code>f</code>, <code>fixup</code> = like \"squash\", but discard this commit's log message</li> <li><code>x</code>, <code>exec</code> = run command (the rest of the line) using shell</li> <li><code>d</code>, <code>drop</code> = remove commit</li> </ul>"},{"location":"git/rebase/#handling-conflicts","title":"Handling Conflicts","text":"<p>There can be conflicts while doing git rebase. Solve the conflicts like normal merge conflicts, then do:</p> <pre><code>git add &lt;resolved_files&gt;\ngit rebase --continue\n</code></pre>"},{"location":"git/tagging/","title":"tagging","text":""},{"location":"git/tagging/#tagging-commits","title":"Tagging Commits","text":""},{"location":"git/tagging/#list-all-tags","title":"List All Tags","text":"<pre><code>git tag\n</code></pre>"},{"location":"git/tagging/#create-a-tag","title":"Create a Tag","text":"<p>Create a tag reference named <code>&lt;tag-name&gt;</code> for the current commit. Add commit SHA to tag a specific commit instead of the current one.</p> <pre><code>git tag [tag-name] [commit sha]\n</code></pre>"},{"location":"git/tagging/#create-an-annotated-tag","title":"Create an Annotated Tag","text":"<p>Create a tag object named <code>&lt;tag-name&gt;</code> for the current commit.</p> <pre><code>git tag -a [tag-name] [commit sha]\n</code></pre>"},{"location":"git/tagging/#remove-a-tag","title":"Remove a Tag","text":"<p>Remove a tag from the local repository.</p> <pre><code>git tag -d [tag-name]\n</code></pre>"},{"location":"git/tagging/#create-an-annotated-tag-with-a-custom-message","title":"Create an Annotated Tag with a Custom Message","text":"<p>Creates an annotated tag at the current commit with a custom message.</p> <pre><code>git tag -a &lt;tag-name&gt; -m &lt;message&gt;\n</code></pre>"},{"location":"github-actions/introduction/","title":"introduction","text":""},{"location":"github-actions/introduction/#macro-rendering-error","title":"Macro Rendering Error","text":"<p>File: <code>github-actions/introduction.md</code></p> <p>UndefinedError: 'secrets' is undefined</p> <pre><code>Traceback (most recent call last):\n  File \"/opt/hostedtoolcache/Python/3.13.8/x64/lib/python3.13/site-packages/mkdocs_macros/plugin.py\", line 688, in render\n    return md_template.render(**page_variables)\n           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n  File \"/opt/hostedtoolcache/Python/3.13.8/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 1295, in render\n    self.environment.handle_exception()\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n  File \"/opt/hostedtoolcache/Python/3.13.8/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 942, in handle_exception\n    raise rewrite_traceback_stack(source=source)\n  File \"&lt;template&gt;\", line 33, in top-level template code\n  File \"/opt/hostedtoolcache/Python/3.13.8/x64/lib/python3.13/site-packages/jinja2/environment.py\", line 490, in getattr\n    return getattr(obj, attribute)\njinja2.exceptions.UndefinedError: 'secrets' is undefined\n</code></pre>"},{"location":"github-actions/links/","title":"links","text":""},{"location":"github-actions/links/#links","title":"Links","text":""},{"location":"github-actions/links/#youtube","title":"Youtube","text":"<ul> <li>github actions personal</li> </ul>"},{"location":"github-actions/links/#udemy","title":"Udemy","text":"<ul> <li>GitHub Actions - The Complete Guide</li> </ul>"},{"location":"jenkins/introduction/","title":"introduction","text":""},{"location":"jenkins/introduction/#sample-jenkins-files","title":"Sample Jenkins files","text":"<p>Jenkins is pretty easy, if we see some couple of action files most of our confusion will be clear</p> <p>Future topic : docker agents for jenkins</p>"},{"location":"jenkins/links/","title":"links","text":""},{"location":"jenkins/links/#jenkins","title":"Jenkins","text":""},{"location":"jenkins/links/#youtube","title":"Youtube","text":"<ul> <li>Jenkins Tutorial for Beginners</li> <li>Jenkins Zero to Hero<ul> <li>iam-veeramalla/Jenkins-Zero-To-Hero</li> </ul> </li> <li>jenkins personal</li> </ul>"},{"location":"jenkins/links/#docker-agent-as-jenkins-worker-node","title":"Docker agent as jenkins worker node","text":"<ul> <li>How to Setup Docker Containers As Build Agents for Jenkins</li> <li>How to Setup Dynamic Jenkins Slave using Docker | Setup Jenkins Build Agent using Docker Container</li> </ul>"},{"location":"jenkins/links/#kubernetes-agent-as-jenkins-worker-node","title":"Kubernetes agent as jenkins worker node","text":"<ul> <li>How to Use Kubernetes Pods As Jenkins Agents<ul> <li>How-to-Use-Kubernetes-Pods-As-Jenkins-Agents.md</li> </ul> </li> </ul>"},{"location":"jenkins/links/#medium","title":"Medium","text":"<ul> <li>Install Jenkins using Docker</li> <li>Dynamic Jenkins Agents with Kubernetes</li> <li>Blue-Green Deployment of Node.js App Using Jenkins and Istio on Minikube kubernetes cluster</li> </ul>"},{"location":"jenkins/links/#udemy","title":"Udemy","text":"<ul> <li>Jenkins: Beginner To Pro</li> <li>Jenkins, From Zero To Hero: Become a DevOps Jenkins Master</li> <li>Jenkins: Jobs, Pipelines, CI/CD and DevOps for Beginners</li> </ul>"},{"location":"jenkins/kubernetes-agent/","title":"Jenkins Helm Chart","text":""},{"location":"jenkins/kubernetes-agent/#jenkins-helm-chart","title":"Jenkins Helm Chart","text":"<p>This Helm chart deploys Jenkins on a Kubernetes cluster with:</p> <ul> <li>A dedicated ServiceAccount and RBAC permissions</li> <li>Persistent storage using <code>hostPath</code> for local environments like Minikube</li> <li>Optional Ingress to expose Jenkins via hostname</li> <li>Support for Kubernetes-based Jenkins agent pods</li> </ul>"},{"location":"jenkins/kubernetes-agent/#folder-structure","title":"Folder Structure","text":"<pre><code>helm/\n\u251c\u2500\u2500 Chart.yaml\n\u251c\u2500\u2500 values.yaml\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 deployment.yaml\n\u2502   \u251c\u2500\u2500 service.yaml\n\u2502   \u251c\u2500\u2500 ingress.yaml\n\u2502   \u251c\u2500\u2500 pv-pvc.yaml\n\u2502   \u251c\u2500\u2500 serviceaccount.yaml\n\u2502   \u2514\u2500\u2500 rbac.yaml\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"jenkins/kubernetes-agent/#reference","title":"\ud83d\udcd8 Reference","text":"<p>We can also use the official helm chart https://github.com/jenkinsci/helm-charts</p> <p>This setup is inspired by:</p> <ul> <li>Dynamic Jenkins Agents with Kubernetes</li> <li>Blue-Green Deployment of Node.js App Using Jenkins and Istio on Minikube</li> <li>How to Use Kubernetes Pods As Jenkins Agents<ul> <li>How-to-Use-Kubernetes-Pods-As-Jenkins-Agents.md</li> </ul> </li> <li>How to Push a Docker Image to Docker Hub Using Jenkins</li> </ul>"},{"location":"jenkins/kubernetes-agent/#prerequisites","title":"\ud83d\udee0\ufe0f Prerequisites","text":"<ul> <li>Kubernetes cluster (e.g., Minikube, Kind)</li> <li>Helm v3+</li> <li>Ingress Controller (NGINX or Istio)</li> <li>Jenkins storage directory created on host</li> </ul>"},{"location":"jenkins/kubernetes-agent/#installation","title":"\ud83d\ude80 Installation","text":"<p>Make sure your Minikube node has the correct hostPath directory created and permission set:</p> <pre><code>minikube ssh\nsudo mkdir -p /mnt/data/jenkins\nsudo chown -R 1000:1000 /mnt/data/jenkins\nexit\n\n# install helm chart\nhelm upgrade --install jenkins . --namespace jenkins --create-namespace \n</code></pre>"},{"location":"jenkins/kubernetes-agent/#access-jenkins","title":"Access Jenkins","text":"<pre><code># Add to /etc/hosts\necho \"$(minikube ip) jenkins.local\" | sudo tee -a /etc/hosts\n\n# Alternative way\nminikube addons enable ingress\n# After the addon is enabled, please run \"minikube tunnel\" and your ingress resources would be available at \"127.0.0.1\"\necho \"$(127.0.0.1) jenkins.local\" | sudo tee -a /etc/hosts\nminikube tunnel\n\n# Access in browser\nhttp://jenkins.local\n</code></pre>"},{"location":"jenkins/kubernetes-agent/#get-jenkins-admin-password","title":"Get Jenkins Admin Password","text":"<pre><code>kubectl exec -n jenkins -it deploy/jenkins -- cat /var/jenkins_home/secrets/initialAdminPassword\n</code></pre>"},{"location":"jenkins/kubernetes-agent/#how-to-install-harbor-in-helm-and-push-oci-image-in-harbor","title":"How to install harbor in helm and push OCI image in harbor","text":"<pre><code>helm upgrade --install harbor harbor/harbor \\\n  --namespace harbor \\\n  --create-namespace \\\n  -f harbor-values.yaml\n\nhelm uninstall harbor -n harbor\n</code></pre>"},{"location":"kubernetes/links/","title":"Kubernetes","text":""},{"location":"kubernetes/links/#kubernetes","title":"Kubernetes","text":""},{"location":"kubernetes/links/#udemy","title":"Udemy","text":"<ul> <li>Certified Kubernetes Application Developer</li> <li>Certified Kubernetes Administrator (CKA) with Practice Tests</li> </ul>"},{"location":"kubernetes/links/#youtube","title":"YouTube","text":""},{"location":"kubernetes/links/#single-videos","title":"Single videos","text":"<ul> <li>What is Kubernetes | Kubernetes explained in 15 mins</li> <li>Kubernetes Explained in 15 Minutes | Hands On (2024 Edition)</li> <li>Do NOT Learn Kubernetes Without Knowing These Concepts...</li> <li>Why is Kubernetes Popular | What is Kubernetes?</li> <li>Kubernetes Architecture: Deep Dive</li> <li>Kubernetes Tutorial with Microservices and Cloud | Kubernetes Course</li> </ul>"},{"location":"kubernetes/links/#playlists","title":"Playlists","text":"<ul> <li>Anton Putra</li> <li>Kubernetes Tutorial for Beginners [Full Course] Architecture deep dive</li> <li>Kubernetes Tutorials Kubernetes components</li> <li>AWS EKS Kubernetes Tutorial [Full Course]</li> <li> <p>Kubernetes Tutorials</p> </li> <li> <p>Kubernetes</p> </li> <li> <p>Kubernetes Tutorial</p> </li> <li>Kubernetes Full Course</li> <li>Kubernetes Tutorial for Beginners</li> </ul>"},{"location":"kubernetes/links/#project","title":"Project","text":"<ul> <li>2-Tier Application Deployment Series</li> </ul>"},{"location":"kubernetes/links/#build-your-own-kubernetes","title":"Build your own kubernetes","text":"<ul> <li>Build Your Own Kubernetes - High Level Design</li> </ul>"},{"location":"kubernetes/links/#ingress-and-load-balancersmetallb","title":"Ingress and Load balancers(MetalLb)","text":"<ul> <li>blogs<ul> <li>Installing MetalLB Load Balancer on Kubernetes Cluster</li> </ul> </li> <li>youtube<ul> <li>Learn about HTTPS, TLS &amp; Certificates</li> <li>MetalLB and NGINX Ingress // Setup External Access for Kubernetes Applications</li> <li>What is Kubernetes' new Gateway API</li> <li>Networking</li> </ul> </li> </ul>"},{"location":"kubernetes/links/#headless-and-statefulset-service","title":"Headless and StatefulSet Service","text":"<ul> <li>Headless Service in Kubernetes</li> <li>StatefulSet in Kubernetes</li> <li>Kubernetes StatefulSet Tutorial</li> <li>Kubernetes Deployment vs. StatefulSet vs. DaemonSet</li> </ul>"},{"location":"kubernetes/links/#kubernetes-resources-crs-crds-operators","title":"Kubernetes Resources, CRs, CRDs, Operators","text":"<ul> <li>youtube<ul> <li>Kubernetes Operator simply explained in 10 mins</li> <li>What The Heck Are Kubernetes Resources, CRs, CRDs, Operators, etc.?</li> </ul> </li> </ul>"},{"location":"kubernetes/links/#kueue","title":"Kueue","text":"<ul> <li>blogs<ul> <li>Introducing Kueue</li> <li>Kueue cohort</li> <li>visibility API</li> <li>Prometheus Metrics</li> <li>Deploy a batch system using Kueue</li> </ul> </li> <li>youtube<ul> <li>Intro to Kueue</li> <li>Kueue: A Kubernetes-native Job Queueing - Abdullah Gharaibeh, Google</li> <li>Resource Orchestration of HPC on Kubernetes: Where We Are Now and... Swati Sehgal &amp; Francesco Romani</li> <li>Dynamic Workload Scheduler and Kueue (Get High-Demand GPUs)</li> </ul> </li> </ul>"},{"location":"kubernetes/links/#batch-workload","title":"Batch Workload","text":"<ul> <li>Building a Batch Workload Platform on Kubernetes</li> </ul>"},{"location":"kubernetes/links/#blogs","title":"Blogs","text":"<ul> <li>13 Kubernetes Configurations You Should Know in 2024</li> <li>13 Kubernetes Tools Your Should Know in 2024</li> <li>17 Kubernetes Libraries You Should be Using In 2024</li> <li>Mastering Kubernetes Security \u2014 My Journey With Admission Controllers</li> <li>11 Best Ways to Optimize Kubernetes Resources and Reduce Costs</li> <li>13 Kubernetes Tricks You Didn't Know</li> <li>A Practical Guide to Running NVIDIA GPUs on Kubernetes</li> <li>What is Kubernetes?</li> <li>Access Cluster API</li> <li>Kubernetes API Structure and Terminology</li> <li>Kubernetes Controller Manager</li> <li>Authentication in Kubernetes (Series of 4 Blogs)</li> <li>Kubernetes Authentication Methods</li> <li>Kubernetes Authentication &amp; Authorization 101</li> <li>Kubernetes RBAC (Role-Based Access Control)</li> <li>RBAC in Kubernetes</li> <li>Kubernetes RBAC Overview</li> <li>Kubernetes Admission Controller</li> <li>Custom Admission Controller</li> <li>Encrypting Secret Data at Rest in Kubernetes</li> </ul>"},{"location":"kubernetes/argo-cd/links/","title":"Argo CD","text":""},{"location":"kubernetes/argo-cd/links/#argo-cd","title":"Argo CD","text":"<p>see this official documentation Getting Started We can create very gitops and devsecops pipeline using git+jenkins+helm+argoCD</p>"},{"location":"kubernetes/argo-cd/links/#configure","title":"Configure","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n</code></pre>"},{"location":"kubernetes/argo-cd/links/#medium","title":"Medium","text":"<ul> <li>Installing ArgoCD on Minikube and deploying a test application</li> <li>Installing Argo CD on Minikube \u2014 GitOps with Argo CD</li> <li>ArgoCD Simplified: Production Deployment Guide</li> <li>Designing a Maintainable GitOps Architecture: How I Scaled My Promotion Flow from a Simple Line to a System That Withstands Change</li> </ul>"},{"location":"kubernetes/argo-cd/links/#youtube","title":"Youtube","text":"<ul> <li>ArgoCD Personal playlist</li> </ul>"},{"location":"kubernetes/argo-cd/links/#udemy","title":"Udemy","text":"<ul> <li>Argo CD Essential Guide for End Users with Practice</li> </ul>"},{"location":"kubernetes/helm/commands/","title":"commands","text":""},{"location":"kubernetes/helm/commands/#applying-a-helm-chart","title":"Applying a Helm Chart","text":"<p>To manage Helm repositories, use the following commands:</p> <p>List existing repositories: </p><pre><code>helm repo list\n</code></pre> <p>Create a new helm chart in your local directory with some preexisting files: </p><pre><code>helm create &lt;chart-name&gt;\n</code></pre> <p>Add a new repository: </p><pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>List repositories again to confirm addition: </p><pre><code>helm repo list\n</code></pre> <p>Remove a repository: </p><pre><code>helm repo remove bitnami\n</code></pre> <p>Add the repository back: </p><pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre> <p>To search the repository, use the following commands:</p> <p>Search for a specific chart: </p><pre><code>helm search repo mysql\n</code></pre> <p>Search for charts related to a keyword: </p><pre><code>helm search repo database\n</code></pre> <p>Search for charts with version information: </p><pre><code>helm search repo database --versions\n</code></pre> <p>Updating the local repositories: </p><pre><code>helm repo update\n</code></pre> <p>To apply a Helm chart, use the following command:</p> <p>Install a Helm chart: </p><pre><code>helm install release-name my-chart -f values.yaml\n</code></pre> <p>Specify a namespace: </p><pre><code>helm install release-name my-chart -f values.yaml -n namespace\n</code></pre> <p>Use a custom values file: </p><pre><code>helm install release-name my-chart -f custom-values.yaml -n namespace\n</code></pre> <p>Install a chart from the current directory: </p><pre><code>helm install release-name . -f &lt;values-path&gt;\n</code></pre>"},{"location":"kubernetes/helm/commands/#upgrading-an-existing-release","title":"Upgrading an Existing Release","text":"<p>To upgrade an existing release, use the following command:</p> <p>Upgrade a Helm chart: </p><pre><code>helm upgrade release-name my-chart -f values.yaml\n</code></pre> <p>Specify a namespace: </p><pre><code>helm upgrade release-name my-chart -f values.yaml -n namespace\n</code></pre> <p>Use a custom values file: </p><pre><code>helm upgrade release-name my-chart -f custom-values.yaml -n namespace\n</code></pre> <p>Reuse the existing values of the previous revision: </p><pre><code>helm upgrade release-name my-chart --reuse-values\n</code></pre> <p>Upgrade a chart from the current directory: </p><pre><code>helm upgrade release-name . -f &lt;values-path&gt;\n</code></pre>"},{"location":"kubernetes/helm/commands/#checking-helm-releases","title":"Checking Helm Releases","text":"<p>To check all the Helm releases, use the following command:</p> <p>List all Helm releases: </p><pre><code>helm ls\n</code></pre> <p>Specify a namespace: </p><pre><code>helm ls -n namespace\n</code></pre> <p>List all releases across all namespaces: </p><pre><code>helm ls -A\n</code></pre>"},{"location":"kubernetes/helm/commands/#uninstalling-a-release","title":"Uninstalling a Release","text":"<p>To uninstall a release, use the following command:</p> <p>Uninstall a Helm release: </p><pre><code>helm uninstall release-name\n</code></pre> <p>Specify a namespace: </p><pre><code>helm uninstall release-name -n namespace\n</code></pre>"},{"location":"kubernetes/helm/commands/#helm-templating-debugging-and-dry-run","title":"Helm Templating Debugging and Dry Run","text":"<p>Helm provides debugging options to inspect how templates are rendered before actual deployment.</p> <p>The key parameters are:</p> <ul> <li><code>--dry-run</code>: Simulates the deployment without applying changes.</li> <li><code>--debug</code>: Provides detailed output, including rendered templates and values.</li> </ul>"},{"location":"kubernetes/helm/commands/#simulating-an-installation-with-debugging","title":"Simulating an Installation with Debugging","text":"<p>To see how Helm renders the templates without deploying: </p><pre><code>helm install my-release my-chart --dry-run --debug\n</code></pre>"},{"location":"kubernetes/helm/commands/#checking-how-an-upgrade-would-modify-an-existing-release","title":"Checking How an Upgrade Would Modify an Existing Release","text":"<p>To check how an upgrade would modify an existing release: </p><pre><code>helm upgrade my-release my-chart --dry-run --debug\n</code></pre>"},{"location":"kubernetes/helm/commands/#rendering-templates-without-applying","title":"Rendering Templates Without Applying","text":"<p>To render templates without applying: </p><pre><code>helm template my-release my-chart --debug\n</code></pre> <p>To compile the output to a file: </p><pre><code>helm install --dry-run --debug &lt;chart-name&gt; . &gt;&gt; compiled-output.yaml\n</code></pre> <p>It generates all the templates without release information and it does not talk to the Kubernetes server, whereas the <code>--dry-run</code> talks to the kube server in order to validate the objects. </p><pre><code>helm template &lt;chart-name&gt; -f &lt;values.yaml-file&gt;\n</code></pre>"},{"location":"kubernetes/helm/commands/#additional-helm-commands","title":"Additional Helm Commands","text":"<p>To install a chart from a repository: </p><pre><code>helm install apache bitnami/apache --namespace=web\n</code></pre> <p>To upgrade a chart from a repository: </p><pre><code>helm upgrade apache bitnami/apache --namespace=web\n</code></pre> <p>To roll back to a previous revision: </p><pre><code>helm rollback apache 1 --namespace=web\n</code></pre> <p>To uninstall a release: </p><pre><code>helm uninstall apache\n</code></pre> <p>Helm stores the release information in the secret. Helm also deletes the release history automatically whenever we uninstall the charts. To maintain the history we have to mention <code>--keep-history</code> flag while uninstalling. </p><pre><code>helm uninstall chart-name --keep-history\n</code></pre> <p>Get the release information from the stored secret: </p><pre><code>kubectl get secret sh.helm.release.v1.&lt;chart-name&gt;.&lt;version-name&gt; -o yaml\n</code></pre> <p>Get the release notes of any installed charts: </p><pre><code>helm get notes &lt;chart-name&gt;\n</code></pre> <p>Get all the values of any installed charts (but only the custom values): </p><pre><code>helm get values &lt;chart-name&gt;\n</code></pre> <p>Get all the values of any installed charts (default + custom): </p><pre><code>helm get values &lt;chart-name&gt; --all\n</code></pre> <p>Get all the values of any installed charts for a particular revision (2nd revision): </p><pre><code>helm get values &lt;chart-name&gt; --revision 2\n</code></pre> <p>Get all the manifests of any installed charts of the current release: </p><pre><code>helm get manifest &lt;chart-name&gt;\n</code></pre> <p>Get all the manifests of any installed charts of a particular revision: </p><pre><code>helm get manifest &lt;chart-name&gt; --revision 2\n</code></pre>"},{"location":"kubernetes/helm/introduction/","title":"introduction","text":""},{"location":"kubernetes/helm/introduction/#what-is-helm","title":"What is Helm?","text":"<p>It is a Kubernetes Package Manager.  Helm is a tool that simplifies the deployment and management of applications on Kubernetes, functioning as a package manager. It uses \"charts,\" which are pre-configured packages of Kubernetes resources, to enable users to deploy applications with a single command, ensuring consistency and reducing errors. Helm can create new charts, package them into chart archive files, interact with chart repositories, and manage the release cycle of charts installed with Helm.</p>"},{"location":"kubernetes/helm/introduction/#main-components","title":"Main Components","text":"<ul> <li> <p>Helm Client:</p> <ul> <li>The Helm client is a command-line interface (CLI) for end users that can be installed on any system.</li> <li>It is responsible for local chart development, managing repositories and releases, interfacing with the Helm library, sending charts to be installed, and requesting upgrades or uninstallations.</li> </ul> </li> <li> <p>Charts:</p> <ul> <li>Charts are Helm's package format.</li> <li>A chart is a bundle of information necessary to create an instance of a Kubernetes application.</li> <li>Charts contain YAML files with metadata and templates that are rendered into Kubernetes manifest files.</li> <li>The <code>Chart.yaml</code> file defines the application metadata, such as name, version, and dependencies.</li> </ul> </li> <li> <p>Repositories:</p> <ul> <li>Repositories are storage locations where Helm charts are stored and shared.</li> <li>They allow teams to share charts of various applications.</li> </ul> </li> <li> <p>Releases:</p> <ul> <li>A release is a running instance of a chart, combined with a specific configuration.</li> <li>It represents a chart deployed into an environment, and Helm manages multiple versions of releases across different environments, including installations, upgrades, and rollbacks.</li> </ul> </li> <li> <p>Helm Library:</p> <ul> <li>The Helm library provides the logic for executing Helm operations.</li> <li>It interfaces with the Kubernetes API server to combine a chart and configuration to build a release.</li> <li>It installs charts, and manages upgrades and uninstallations.</li> </ul> </li> <li> <p>Revision History and Rollbacks:</p> <ul> <li>Helm maintains a history of all releases, allowing users to roll back to previous versions if needed.</li> <li>Each upgrade or installation creates a new revision.</li> <li>Rollbacks can be performed to revert to a specific revision.</li> </ul> </li> </ul>"},{"location":"kubernetes/helm/introduction/#why-should-we-use-helm","title":"Why Should We Use Helm?","text":"<ul> <li> <p>Simplicity: Helm simplifies the deployment and management of Kubernetes applications by providing a single command to deploy complex applications. This reduces the learning curve and makes it easier for teams to manage their applications.</p> </li> <li> <p>Revision History: Helm maintains a history of all releases, allowing easy rollbacks to previous versions. This is particularly useful for recovering from failed deployments or reverting to a known good state.</p> </li> <li> <p>Dynamic Configuration: Helm supports dynamic configuration through values files, enabling users to customize their deployments without modifying the underlying charts. This allows for flexible and reusable configurations.</p> </li> <li> <p>Consistency: Helm ensures consistent deployments across environments by using pre-configured charts. This reduces the risk of human error and ensures that applications are deployed in a predictable manner.</p> </li> <li> <p>Intelligent Deployments: Helm manages the order in which resources are created, ensuring that dependencies are resolved correctly. This prevents issues that can arise from resources being created in the wrong order.</p> </li> <li> <p>Lifecycle Hooks: Helm supports lifecycle hooks, allowing users to execute custom actions at different points during the deployment process. This can be used to perform tasks such as database migrations or configuration updates.</p> </li> <li> <p>Security: Helm allows downloading secured charts from central repositories, ensuring that only trusted and verified charts are used. This enhances the security of the deployment process.</p> </li> </ul>"},{"location":"kubernetes/helm/introduction/#helm-templating-engine","title":"Helm Templating Engine","text":"<p>Helm uses a templating engine to dynamically generate Kubernetes manifests. It allows developers to define templates with placeholders that get replaced with actual values during deployment.</p>"},{"location":"kubernetes/helm/introduction/#key-features","title":"Key Features","text":"<ul> <li>Uses Go templates for defining Kubernetes resources.</li> <li>Supports values.yaml for injecting dynamic configurations.</li> <li>Enables control structures like loops (<code>range</code>) and conditionals (<code>if</code>).</li> <li>Allows reusability with partials and functions.</li> </ul>"},{"location":"kubernetes/helm/introduction/#example","title":"Example","text":"<p>A basic Deployment template (<code>templates/deployment.yaml</code>):</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ .Release.Name }}-app\nspec:\n  replicas: {{ .Values.replicaCount }}\n  template:\n    spec:\n      containers:\n        - name: my-container\n          image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n</code></pre>"},{"location":"kubernetes/helm/introduction/#workflow-of-helm","title":"Workflow of Helm","text":"<ol> <li> <p>Load the Chart and its Dependencies:</p> <ul> <li>Helm fetches the chart from the local directory or a remote repository.</li> <li>It also fetches any dependencies specified in the <code>Chart.yaml</code> file.</li> </ul> </li> <li> <p>Parse the values.yaml:</p> <ul> <li>Helm reads the <code>values.yaml</code> file to get the configuration values.</li> <li>These values can be overridden by passing custom values during the <code>helm install</code> or <code>helm upgrade</code> commands.</li> </ul> </li> <li> <p>Generate the YAML:</p> <ul> <li>Helm uses the templating engine to combine the chart templates with the values from <code>values.yaml</code>.</li> <li>This generates the final Kubernetes manifest files.</li> </ul> </li> <li> <p>Parse the YAML to Kubernetes Objects and Validate:</p> <ul> <li>Helm parses the generated YAML files into Kubernetes objects.</li> <li>It validates these objects to ensure they are correctly formatted and can be accepted by the Kubernetes API server.</li> </ul> </li> <li> <p>Generate YAML and Send to Kubernetes:</p> <ul> <li>Helm sends the validated Kubernetes objects to the Kubernetes API server.</li> <li>The API server processes these objects and creates the corresponding resources in the cluster.</li> </ul> </li> </ol>"},{"location":"kubernetes/helm/links/","title":"links","text":""},{"location":"kubernetes/helm/links/#website-and-blogs","title":"Website and blogs","text":"<ul> <li>GO templating language for Helm Charts</li> </ul>"},{"location":"kubernetes/helm/links/#youtube","title":"Youtube","text":"<ul> <li>What is Helm? | Helm Concepts Explained | KodeKloud</li> <li>What is Helm in Kubernetes? Helm and Helm Charts explained | Kubernetes Tutorial 23</li> <li>An Introduction to Helm - Matt Farina, Samsung SDS &amp; Josh Dolitsky, Blood Orange</li> <li>What is Helm? | Helm Concepts Explained | Deploy Spring Boot in k8s using Helm-Chart</li> </ul>"},{"location":"kubernetes/helm/links/#udemy","title":"Udemy","text":"<ul> <li>Helm Kubernetes Packaging Manager for Developers and DevOps</li> <li>Helm Masterclass: 50 Practical Demos for Kubernetes DevOps</li> <li>Practical Helm Charts For Beginners</li> </ul>"},{"location":"kubernetes/manifests/etcd/etcd/","title":"Etcd","text":""},{"location":"kubernetes/manifests/etcd/etcd/#deploying-etcd","title":"Deploying ETCD","text":"<ul> <li>Demystifying stateful apps on Kubernetes by deploying an etcd cluster</li> <li>deploying-etcd-cluster</li> </ul>"},{"location":"kubernetes/manifests/mongodb-master-slave/","title":"Index","text":""},{"location":"kubernetes/manifests/mongodb-master-slave/#start-the-statefullset","title":"start the statefullset:","text":"<ul> <li>kubectl apply -f mongo-secret.yaml</li> <li>kubectl apply -f mongo-configmap.yaml</li> <li>kubectl apply -f mongo-storage-class.yaml</li> <li>kubectl apply -f mongo-headless-service.yaml</li> <li>kubectl apply -f mongo-statefullset.yaml</li> </ul>"},{"location":"kubernetes/manifests/mongodb-master-slave/#stop-the-statefullset","title":"stop the statefullset:","text":"<ul> <li>kubectl delete -f mongo-statefullset.yaml</li> <li>kubectl delete -f mongo-secret.yaml</li> <li>kubectl delete -f mongo-configmap.yaml</li> <li>kubectl delete -f mongo-storage-class.yaml</li> <li>kubectl delete -f mongo-headless-service.yaml</li> </ul> <p>https://www.mongodb.com/docs/manual/reference/method/rs.initiate/</p> <pre><code>kubectl exec -it mongo-0 -- mongo\n\nrs.initiate(\n    {\n      _id: \"rs0\",\n      version: 1,\n      members: [\n         { _id: 0, host : \"mongo-0.mongo.default.svc.cluster.local:27017\" },\n         { _id: 1, host : \"mongo-1.mongo.default.svc.cluster.local:27017\" },\n         { _id: 2, host : \"mongo-2.mongo.default.svc.cluster.local:27017\" }\n      ]\n   }\n)\nrs.status()\nuse persons\n\ndb.person.insert({\"firstName\": \"Abhishek\",\"lastName\": \"Ghosh\",\"age\": 25,\"gender\": \"Male\"})\ndb.person.insert({\"firstName\": \"Nasim\",\"lastName\": \"Molla\",\"age\": 26,\"gender\": \"Male\"})\n</code></pre> <pre><code>kubectl exec -it mongo-1 -- mongo\nrs.slaveOk()\nrs.status()\nuse persons\ndb.person.findOne()\n</code></pre> <pre><code>kubectl run -i --tty mongo --image=mongo:4.0.8 --restart=Never -- bash\n\nmongo \"mongodb://mongo-0.mongo.default.svc.cluster.local:27017,mongo-1.mongo.default.svc.cluster.local:27017,mongo-1.mongo.default.svc.cluster.local:27017/?replicaSet=rs0\"\n</code></pre> <pre><code>kubectl run -i --tty mongo --image=mongo:4.0.8 --restart=Never -- bash\n\nmongo \"mongodb://mongo-0.mongo.default.svc.cluster.local:27017\"\n</code></pre> <p>minikube ssh</p> <p>cd /tmp/hostpath-provisioner/default</p>"},{"location":"kubernetes/manifests/mongodb-master-slave/#find-a-way-to-connect-to-the-statefull-set-from-mongodb-atlas","title":"Find a way to connect to the statefull set from mongodb atlas","text":""},{"location":"kubernetes/manifests/prometheus-grafana-stack/","title":"Prometheus and grafana stack on kubernetes","text":""},{"location":"kubernetes/manifests/prometheus-grafana-stack/#prometheus-and-grafana-stack-on-kubernetes","title":"Prometheus and grafana stack on kubernetes","text":""},{"location":"kubernetes/manifests/prometheus-grafana-stack/#observability-in-kubernetes","title":"Observability in kubernetes","text":"<ul> <li>youtube</li> <li>Server Monitoring with Grafana Prometheus and Loki</li> <li>Beautiful Dashboards with Grafana and Prometheus - Monitoring Kubernetes Tutorial</li> <li>Prometheus Operator Kubernetes Tutorial: ServiceMonitor - PodMonitor - Step-by-Step</li> <li>Prometheus Monitoring Full Tutorial</li> <li>Prometheus</li> <li>Kubernetes Monitoring guide for beginners</li> <li>Prometheus</li> <li>Observability</li> </ul>"},{"location":"kubernetes/resources/command-and-args/","title":"command and args","text":""},{"location":"kubernetes/resources/command-and-args/#commands-and-args","title":"Commands and Args","text":"<p>In docker we have ENTRYPOINT and CMD for giving command line arguments, but we can override that using --entrypoint and the extra parameters passed in the docker run command. Same way we can override the existing command in the pod definition file with command and args. ENTRYPOINT will associated with command and CMD will be associated with args. </p> <p>Let's say we have a dockerfile like this </p><pre><code># ubuntu-sleeper dockerfile\n\nFROM UBUNTU\nENTRYPOINT [\"sleep\"]\nCMD [\"10\"]\n</code></pre> <p>And pod definition like this </p><pre><code>name: ubuntu\nimage: ubuntu-sleeper\ncommand: [\"sleeper2.0\"]\nargs: [\"100\"]\n</code></pre> <p>The pod definition file is same as this docker run command <code>docker run --entry-point=sleeper2.0 ubuntu-sleeper 100</code></p> <p>Copy the content of existing pod into a yaml -&gt; kubectl get pod  -o yaml &gt; pod-definition.yaml</p>"},{"location":"kubernetes/resources/container-logging/","title":"container logging","text":""},{"location":"kubernetes/resources/container-logging/#container-logging","title":"Container logging","text":"<pre><code>docker run -d kodecloud/event-simulator \ndocker log -f &lt;container-name&gt;\n\n# this is how we can check the logs of a docker container after running it in detached mode. \n# We can do the same with Kubernetes.\nkubectl create -f event-simulator.yaml\n\n# To get logs of a specific pod\nkubectl logs -f event-simulator-pod\n\n# To get the logs of a specific container of a specific pod\nkubectl logs -f &lt;pod-name&gt; &lt;container-name&gt;\n</code></pre>"},{"location":"kubernetes/resources/container-logging/#monitoring-solutions","title":"Monitoring Solutions:","text":"<ol> <li>Prometheus</li> <li>Elastic stack</li> <li>Datadog</li> <li>Dynatree</li> </ol> <p>Kubernetes runs an agent on each node known as kubelet which is responsible for receiving instruction from Kubernetes master server. It has a subcomponent known as container advisor, it is responsible for retrieving performance metrics from pods and exposing them to kubelet api.</p> <pre><code># To enable the metric server on minikube\nminikube addons enable metrics-server\n\n# To find the node with max using the resources\nkubectl top node\n\n# To find the pod with the max using resources.\nkubectl top pod\n</code></pre>"},{"location":"kubernetes/resources/deployment/","title":"deployment","text":""},{"location":"kubernetes/resources/deployment/#deployment","title":"Deployment","text":"<p>Kubernetes deployment create creates one deployment kind of object.</p>"},{"location":"kubernetes/resources/deployment/#sample-nginx-deploymentyaml","title":"Sample nginx-deployment.yaml","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  labels:\n    app: nginx-deployment\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      name: nginx-pod\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n</code></pre>"},{"location":"kubernetes/resources/deployment/#some-imperative-commands","title":"Some imperative commands","text":"<pre><code># It will fetch all the deployments.\nkubectl get deployments\n\n# It will give us the description of the specific deployment\nkubectl describe deployments &lt;deployment-name&gt;\n\n# To get the resources like pod, replicasets, services, deployments\nkubectl get all\n\n# it will create one deployment named ad http-frontend and its image will be httpd:2.4.alpine and count of the pod will be 1.\nkubectl create deployment http-frontend --image=httpd:2.4.alpine\n\n# it will scale the deployment and change the number of the pods \nkubectl scale deployment --replicas=3 httpd-frontend\n\n# It will create a deployment and also record all the changes of the deployment for rollout history.\nkubectl create -f nginx-deployment.yaml --record\n\n# If we want to revert to previous version, we must add this record flag\n# Update and roll back\n\n# it will give the current rollout status of the deployment. When we change the replicas or the image at that time it will give the information.\nkubectl rollout status deployment/nginx-deployment\n\n# It will give us all the rollout history of the Kubernetes deployment rollout\nkubectl rollout history deployment/nginx-deployment\n\n# Kubernetes deployment object creates another replicaset and then it will add the pods into the replicaset\n\n# It will revert the latest changes back to the previous version.\nkubectl rollout undo deployment/nginx-deployment\n\n# Changes to the existing deployment\n# It will change the image of the deployment of the nginx deployment and will record it\nkubectl set image deployment nginx-deployment nginx:old-image=nginx:new-image --record\n\n# it will open vi editor and open the current configuration of the nginx-deployment\nkubectl edit deployment nginx-deployment\n</code></pre>"},{"location":"kubernetes/resources/deployment/#for-rollout-kubernetes-has-2-types-of-strategy","title":"For rollout Kubernetes has 2 types of strategy","text":"<ul> <li>Recreate strategy</li> <li>Rolling strategy</li> </ul>"},{"location":"kubernetes/resources/environment-variables/","title":"environment-variables","text":""},{"location":"kubernetes/resources/environment-variables/#environment-variables","title":"Environment variables","text":"<p>We can pass environment variables in these 3 ways. - Using env  - Using config map key valueFrom -&gt; configMapKeyRef - Using secret key valueFrom -&gt;secretKeyRef</p> <p></p><pre><code># Using env in the pod specification\nspec:\n  name: postgres\n  image: postgres\n  env:\n  - name: POSTGRES_PASSWORD\n    value: MY_SECRET_PASSWORD\n\n# Using configMapKeyRef in the pod specification\nspec:\n  name: postgres\n  image: postgres\n  env:\n  - name: POSTGRES_PASSWORD\n    valueFrom:\n      configMapKeyRef:\n        name: posgtres-config\n        key: password\n        name: postgres-secret\n        key: password\n\n# Using secretKeyRef in the pod specification\nspec:\n  name: postgres\n  image: postgres\n  env:\n  - name: POSTGRES_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: postgres-secret\n        key: password\n</code></pre> We can either use configMapKeyRef where we can pass keys, or we can use configMapRef where we can directly pass the configMap. Same goes with Secrets. We can either use secretKeyRef or secretRef. <p>We can create configMap with these 3 ways - By passing all the values in command. - By passing the properties file or yaml file in the command - By using the deifinition file.</p>"},{"location":"kubernetes/resources/environment-variables/#some-imperative-commands","title":"Some imperative commands","text":"<pre><code>#  It will create a configmap with color blue\nKubectl create configmap app-config --from-literal app-color=blue\n\n# It weill create a config file with theapp.properties content\nkubectl create configmap app-config --from-file /path/app.properties\n\n# It will create a configMap with app-config definition file\nkubectl apply -f app-config.yaml\n\n# It will fetch all the config maps\nkubectl get cm/configmaps\n\n# It will describe the config map app-config\nkubectl describe configmap app-config\n</code></pre>"},{"location":"kubernetes/resources/environment-variables/#sample-config-map-and-pod-definition-yaml","title":"Sample config map and pod definition yaml","text":"<pre><code># nginx config map\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nginx-configmap\ndata:\n  name: nginx\n  tier: frontend\n  work: loadbalancing\n\n# pod defination\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      envFrom:\n        - configMapRef:\n            name: nginx-configmap\n</code></pre>"},{"location":"kubernetes/resources/environment-variables/#config-map-stores-everything-in-plain-text-format-which-is-not-suitable-for-storing-password-thats-why-we-need-secrets","title":"config map stores everything in plain text format, which is not suitable for storing password, that's why we need secrets.","text":"<p>We can create secret with these 3 ways. - By passing all the values in command. - By passing the properties file or yaml file in the command - By using the deifinition file.</p> <pre><code># It will create a secret with color blue.\nKubectl create secret generic app-secret --from-literal app-color=blue\n\n# It will create a secret with the app-secret.properties file.\nKubectl create secret generic app-secret --from-file app-secret.properties\n</code></pre> <p></p><pre><code># app.properties\ndb_host=postgres\ndb_password=root\ndb_name=database\n\n# app-sercret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secret\ndata:\n  db_host: cG9zdGdyZXMNCg==\n  db_password: cm9vdA0K\n  db_name: ZGF0YWJhc2UNCg==\n\n# nginx-with-secret\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  labels:\n    name: nginx-pod\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    envFrom:\n      - secretRef:\n          name: app-secret\n</code></pre> There are some other ways to store the information. That is Helm secrets, HarshiCorp vault etc."},{"location":"kubernetes/resources/gateway-api/","title":"gateway-api","text":""},{"location":"kubernetes/resources/gateway-api/#gateway-api","title":"Gateway API","text":""},{"location":"kubernetes/resources/gateway-api/#medium","title":"Medium","text":"<ul> <li>Gateway API vs Ingress in Kubernetes: A Deep Dive with Architecture Diagrams-Part1</li> <li>Using Kong to access Kubernetes services, using a Gateway resource with no cloud provided LoadBalancer</li> <li>Why do I need an API Gateway on a Kubernetes cluster</li> </ul>"},{"location":"kubernetes/resources/gateway-api/#youtube","title":"Youtube","text":"<ul> <li>Mastering Kubernetes: Service and Network APIs (Service, Ingress, GatewayAPI)</li> <li>GATEWAY API - Ingress is DEAD! Long live Ingress</li> <li>Gateway API Explained: The Future of Kubernetes Networking</li> </ul>"},{"location":"kubernetes/resources/gateway-api/#refferences","title":"Refferences","text":"<ul> <li>Gateway API documentation</li> <li>From kubernetes documentation</li> <li>From google cloud</li> <li>Gateway API v1.2: WebSockets, Timeouts, Retries, and More</li> </ul>"},{"location":"kubernetes/resources/gateway-api/#implementations","title":"Implementations","text":"<ul> <li>Implementations</li> <li>Envoy Gateway</li> <li>nginx</li> </ul>"},{"location":"kubernetes/resources/gateway-api/#installation","title":"Installation","text":"<pre><code># Use nginx instead of envoy, as it is giving me error\n# but later check the issue\n# Use minikube tunnel for exposing the service because in a minikube setup we don't have loadbalancer like Metallb\n\n# Gateway API related CRDs\nkubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/latest/download/standard-install.yaml\n\n# Deploy the NGINX Gateway Fabric CRDs\nkubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.2/deploy/crds.yaml\n\n# Deploy NGINX Gateway Fabric\nkubectl apply -f https://raw.githubusercontent.com/nginx/nginx-gateway-fabric/v1.6.2/deploy/default/deploy.yaml\n\n# Verify the Deployment\nkubectl get pods -n nginx-gateway\n\n# Use minikube tunnel to expose and add the hostname into your /etc/hosts\nminukube tunnel\n</code></pre>"},{"location":"kubernetes/resources/gateway-api/#premise","title":"Premise","text":"<p>To be generally available, web applications running on Kubernetes need to be exposed outside the cluster. This can be achieved in several ways. The simplest method to expose a service or API endpoint outside a Kubernetes cluster is to assign a service type of NodePort. The drawback of this approach is that the service runs on a non-standard port number. Additionally, each cluster has a limited number of ports assigned to its NodePort pool. These limitations can be overcome by using either the Ingress API or the Gateway API. While the Ingress API has been widely adopted in the Kubernetes world, it has its own drawbacks. Notably, a strong reliance on annotations in Ingress manifests makes the Ingress API inflexible and difficult to write generic templates for in Helm charts. The Gateway API overcomes these drawbacks by providing a generic, easy-to-templatize, and extensible way to define resources that provide an ingress of traffic to Kubernetes-hosted endpoints. In the following sections, we will compare the two approaches.</p>"},{"location":"kubernetes/resources/gateway-api/#ingress-api","title":"Ingress API","text":"<p>HTTP and HTTPS network services can be exposed outside the Kubernetes cluster using Ingress resources, provided as part of the Ingress API. Traffic is routed using rules defined within the Ingress resource. Before Ingress resources can be defined, the cluster must have at least one Ingress Controller running. This Ingress Controller service is typically exposed using a LoadBalancer service type.</p> <p>The Ingress API is widely adopted by Kubernetes users and well-supported by vendors with many implementations (Ingress controllers) available.</p> <p>Ingress API resource organization</p> <p></p>"},{"location":"kubernetes/resources/gateway-api/#limitations-of-the-ingress-api","title":"Limitations of the Ingress API","text":"<ul> <li>Limited features: The Ingress API only supports TLS termination and simple content-based request routing of HTTP traffic.</li> <li>Reliance on annotations for extensibility: The annotations approach to extensibility leads to limited portability as every implementation has its own supported extensions that may not translate to any other implementation.</li> <li>Insufficient permission model: The Ingress API is not well-suited for multi-team clusters with shared load-balancing infrastructure.</li> </ul>"},{"location":"kubernetes/resources/gateway-api/#gateway-api_1","title":"Gateway API","text":"<p>The Gateway API is an official Kubernetes project being worked on by the Kubernetes Network SIG, representing the next generation of Ingress, Load balancing, and Service Mesh APIs. It focuses on L4 and L7 routing within Kubernetes. It has been designed from the outset to be generic, expressive, and role-oriented.</p>"},{"location":"kubernetes/resources/gateway-api/#features-of-the-gateway-api","title":"Features of the Gateway API","text":"<p>The following design goals drive the concepts of Gateway API. These demonstrate how Gateway aims to improve upon current standards like Ingress.</p> <ul> <li>Role-oriented: Gateway is composed of API resources that model organizational roles that use and configure Kubernetes service networking.</li> <li>Portable: This isn't an improvement but rather something that should stay the same. Just as Ingress is a universal specification with numerous implementations, Gateway API is designed to be a portable specification supported by many implementations.</li> <li>Expressive: Gateway API resources support core functionality for things like header-based matching, traffic weighting, and other capabilities that were only possible in Ingress through custom annotations.</li> <li>Extensible: Gateway API allows for custom resources to be linked at various layers of the API. This makes granular customization possible at the appropriate places within the API structure.</li> </ul>"},{"location":"kubernetes/resources/gateway-api/#gateway-api-resource-model","title":"Gateway API Resource Model","text":"<p>GatewayClass - GatewayClass is a cluster-scoped resource that defines a set of Gateways that share a common configuration and behavior. A cluster needs to have at least one GatewayClass, which is handled by a Gateway Controller, similar to an Ingress Controller. A Gateway Controller may handle more than one GatewayClass.</p> <p>Gateway - A Gateway describes how traffic can be translated within a cluster. As is evident from the name, it acts as a gateway between traffic outside the cluster and traffic within the cluster.</p> <p>Route Resources define protocol-specific rules for mapping requests from a Gateway to Kubernetes Services. Route resources included in the API currently are: - HTTPRoute - Used for multiplexing HTTP or terminated HTTPS connections. - GRPCRoute - Used for idiomatically routing gRPC traffic. - TLSRoute (experimental) - Used for multiplexing TLS connections, discriminated via SNI. - TCPRoute and UDPRoute (experimental) - used for mapping one or more TCP or UDP ports to a single backend. These may be used to terminate TLS, where appropriate.</p> <p></p>"},{"location":"kubernetes/resources/imperative-commands/","title":"imperative commands","text":""},{"location":"kubernetes/resources/imperative-commands/#imperative-commands","title":"Imperative Commands","text":"<p>Before we begin, familiarize with the two options that can come in handy while working with the below commands: - <code>--dry-run</code>: By default, as soon as the command is run, the resource will be created. If you simply want to test your command, use the <code>--dry-run=client</code> option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right. - <code>-o yaml</code>: This will output the resource definition in YAML format on screen.</p> <p>Use the above two in combination to generate a resource definition file quickly, that you can then modify and create resources as required, instead of creating the files from scratch.</p>"},{"location":"kubernetes/resources/imperative-commands/#pod","title":"POD","text":"<p>Create an NGINX Pod: </p><pre><code>kubectl run nginx --image=nginx\n</code></pre> <p>Generate POD Manifest YAML file (-o yaml). Don't create it (--dry-run): </p><pre><code>kubectl run nginx --image=nginx --dry-run=client -o yaml\n</code></pre>"},{"location":"kubernetes/resources/imperative-commands/#deployment","title":"Deployment","text":"<p>Create a deployment: </p><pre><code>kubectl create deployment --image=nginx nginx\n</code></pre> <p>Generate Deployment YAML file (-o yaml). Don't create it (--dry-run): </p><pre><code>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml\n</code></pre> <p>Generate Deployment with 4 Replicas: </p><pre><code>kubectl create deployment nginx --image=nginx --replicas=4\n</code></pre> <p>You can also scale a deployment using the kubectl scale command: </p><pre><code>kubectl scale deployment nginx --replicas=4\n</code></pre> <p>Another way to do this is to save the YAML definition to a file and modify: </p><pre><code>kubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre> <p>You can then update the YAML file with the replicas or any other field before creating the deployment.</p>"},{"location":"kubernetes/resources/imperative-commands/#service","title":"Service","text":"<p>Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379: </p><pre><code>kubectl expose pod redis --port=6379 --name=redis-service --dry-run=client -o yaml\n</code></pre> <p>Or: </p><pre><code>kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml\n</code></pre> <p>Create a service and save it to a file: </p><pre><code>kubectl expose pod redis --port=6379 --name=redis-service --dry-run=client -o yaml &gt; redis-service.yaml\n</code></pre> <p>Create a dry run of the redis service and save it to a file: </p><pre><code>kubectl create service clusterip redis-service --tcp=6379:6379 --dry-run=client -o yaml &gt; redis-service.yaml\n</code></pre> <p>Expose a pod named nginx-pod of type NodePort and save it to a file: </p><pre><code>kubectl expose pod nginx-pod --port=80 --name=nginx-service --type=NodePort --dry-run=client -o yaml &gt; nginx-service.yaml\n</code></pre> <p>Create a service of type NodePort with port 80 and target port 80 with node port 30080 and save it to a file: </p><pre><code>kubectl create service nodeport nginx-service --tcp=80:80 --node-port=30080 --dry-run=client -o yaml &gt; nginx-service.yaml\n</code></pre> <p>Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes: </p><pre><code>kubectl expose pod nginx --port=80 --name=nginx-service --type=NodePort --dry-run=client -o yaml\n</code></pre> <p>Or: </p><pre><code>kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml\n</code></pre> <p>Note: - The <code>kubectl expose</code> command automatically uses the pod's labels as selectors, but you cannot specify the node port. You must add that in the definition file, then you can add the node port. - The <code>kubectl create service</code> command will not use the pod labels as selectors; instead, it will assume selectors as <code>app: service-name</code>, and you cannot pass selectors in the definition file.</p>"},{"location":"kubernetes/resources/imperative-commands/#hpa","title":"HPA","text":"<p>Horizontal Pod Autoscaler (HPA) automatically scales the number of pods in a deployment based on observed CPU utilization or other select metrics.</p> <p>Create an HPA for a deployment: </p><pre><code>kubectl autoscale deployment &lt;deployment-name&gt; --cpu-percent=50 --min=1 --max=10\n</code></pre> <p>Check the status of the HPA: </p><pre><code>kubectl get hpa\n</code></pre>"},{"location":"kubernetes/resources/ingress/","title":"ingress","text":""},{"location":"kubernetes/resources/ingress/#ingress","title":"Ingress","text":""},{"location":"kubernetes/resources/ingress/#reference","title":"Reference","text":"<ul> <li>Mastering Kubernetes Ingress | Hands-on Example with Spring Boot &amp; NGINX</li> </ul>"},{"location":"kubernetes/resources/introduction/","title":"introduction","text":""},{"location":"kubernetes/resources/introduction/#introduction","title":"Introduction","text":"<p>Cluster contains multiple nodes but there will be only one master node.</p> <pre><code># to get version of kubernetes\nkubectl version\n</code></pre> <p>For more go to the refference</p>"},{"location":"kubernetes/resources/kubernetes-cluster-setup/","title":"Kubernetes cluster setup","text":"<p>check the pdf in the pdf folder</p>"},{"location":"kubernetes/resources/kueue/","title":"kueue","text":""},{"location":"kubernetes/resources/kueue/#kueue","title":"Kueue","text":""},{"location":"kubernetes/resources/kueue/#refferences","title":"Refferences","text":"<ul> <li>Documentation</li> <li>kubernetes-sigs/kueue</li> </ul>"},{"location":"kubernetes/resources/kueue/#why-kueue","title":"Why Kueue?","text":"<p>Kueue is a kubernetes-native system that manages quotas and how jobs consume them. Kueue decides when a job should wait, when a job should be admitted to start (as in pods can be created) and when a job should be preempted (as in active pods should be deleted).</p>"},{"location":"kubernetes/resources/kueue/#why-use-kueue","title":"Why use Kueue","text":"<p>Kueue can be installed on a vanilla Kubernetes cluster without replacing any existing Kubernetes components. It is compatible with cloud environments where:</p> <ul> <li>Compute resources are elastic and can be scaled up and down.</li> <li>Compute resources are heterogeneous (in architecture, availability, price, etc.).</li> </ul> <p>Kueue APIs allow you to define:</p> <ul> <li>Quotas and policies for fair sharing among tenants.</li> <li>Resource fungibility: if a resource flavor is fully utilized, Kueue can admit the job using a different flavor.</li> </ul> <p>A core design principle for Kueue is to avoid duplicating mature functionality in Kubernetes components and well-established third-party controllers. Autoscaling, pod-to-node scheduling, and job lifecycle management are the responsibility of cluster-autoscaler, kube-scheduler, and kube-controller-manager, respectively. Advanced admission control can be delegated to controllers such as gatekeeper.</p>"},{"location":"kubernetes/resources/kueue/#features-overview","title":"Features overview","text":"<ul> <li>Job management: Support job queueing based on priorities with different strategies: StrictFIFO and BestEffortFIFO.</li> <li>Advanced Resource management: Comprising resource flavor fungibility, fair sharing, cohorts, and preemption with a variety of policies between different tenants.</li> <li>Integrations: Built-in support for popular jobs, e.g., BatchJob, Kubeflow training jobs, RayJob, RayCluster, JobSet, AppWrappers, plain Pod, and Pod Groups.</li> <li>System insight: Built-in Prometheus metrics to help monitor the state of the system, and on-demand visibility endpoint for monitoring of pending workloads.</li> <li>AdmissionChecks: A mechanism for internal or external components to influence whether a workload can be admitted.</li> <li>Advanced autoscaling support: Integration with cluster-autoscaler's provisioningRequest via admissionChecks.</li> <li>All-or-nothing with ready Pods: A timeout-based implementation of All-or-nothing scheduling.</li> <li>Partial admission and dynamic reclaim: mechanisms to run a job with reduced parallelism, based on available quota, and to release the quota when the pods complete.</li> <li>Mixing training and inference: Simultaneous management of batch workloads along with serving workloads (such as Deployments or StatefulSets).</li> <li>Multi-cluster job dispatching: called MultiKueue, allows searching for capacity and off-loading the main cluster.</li> <li>Topology-Aware Scheduling: Allows optimizing the pod-pod communication throughput by scheduling aware of the data-center topology.</li> </ul>"},{"location":"kubernetes/resources/kueue/#high-level-kueue-operation","title":"High-level Kueue operation","text":""},{"location":"kubernetes/resources/kueue/#concepts","title":"Concepts","text":""},{"location":"kubernetes/resources/kueue/#resource-flavor","title":"Resource Flavor","text":"<p>Note: For further reference</p> <p>A ResourceFlavor is an object that represents these resource variations and allows you to associate them with cluster nodes through labels, taints, and tolerations.  We created Resource Flavors for interactive jobs.</p> <p>To associate a ResourceFlavor with nodes, the ResourceFlavor object specifies the .spec.nodeLabels that should match the labels on the target nodes. If using cluster autoscaler, it must be configured to add these labels when adding new nodes.</p> <pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"interactive-resource-flavor\"\nspec:\n  nodeLabels:\n    node-role.kubernetes.io/interactive-worker: \"true\"\n</code></pre>"},{"location":"kubernetes/resources/kueue/#cluster-queue","title":"Cluster Queue","text":"<p>A ClusterQueue is a cluster-scoped object that governs a pool of resources such as pods, CPU, memory, and hardware accelerators. A ClusterQueue defines:</p> <ul> <li>The quotas for the Resources Flavors that the ClusterQueue manages, with usage limits and order of consumption.</li> <li>Fair sharing rules across the multiple ClusterQueues in the cluster.</li> </ul> <p>We configured a single cluster queue for workloads using the <code>interactive-resource-flavor</code> Resource Flavor, assigning CPU, memory, and ephemeral storage. The configuration is as follows:</p> <p>ClusterQueue for interactive jobs </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-interactive\"\nspec:\n  ...\n  cohort: \"workload-queues\"\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"interactive-resource-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 6\n      - name: \"memory\"\n        nominalQuota: 6Gi\n      - name: \"ephemeral-storage\"\n        nominalQuota: 50Gi\n</code></pre>"},{"location":"kubernetes/resources/kueue/#localqueue","title":"LocalQueue","text":"<p>A LocalQueue is a namespace object that groups closely related workloads that belong to a single namespace. A namespace is typically assigned to a tenant (team or user) of the organization. A LocalQueue points to one ClusterQueue from which resources are allocated to run its Workloads.</p> <p>We created a LocalQueue for ClusterQueue <code>cluster-queue-interactive</code>.</p> <p>LocalQueue for interactive and headless ClusterQueues </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"interactive-workload\"\n  name: \"interactive-local-queue\"\nspec:\n  clusterQueue: \"cluster-queue-interactive\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: LocalQueue\nmetadata:\n  namespace: \"headless-workload\"\n  name: \"headless-local-queue\"\nspec:\n  clusterQueue: \"cluster-queue-headless\"\n</code></pre> <p>To deploy a job using Kueue, we need to associate jobs to the specific LocalQueue first. Update the job manifest with the queue name in the job metadata. When you launch the job, it will be assigned to the <code>interactive-local-queue</code> queue. You can check the number of admitted jobs in a LocalQueue with the following command.</p> <p><code>kubectl get localQueue -n &lt;namespace&gt;</code></p> <p>Jobs manifest with assigned queue name </p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: \"interactive-workload\"\n  labels:\n    ...\n    kueue.x-k8s.io/queue-name: interactive-local-queue\n  name: \"jobname\"\nspec:\n  parallelism: 1\n  completions: 1\n  ...\n</code></pre>"},{"location":"kubernetes/resources/kueue/#workloadpriorityclass","title":"WorkloadPriorityClass","text":"<p>A WorkloadPriorityClass allows you to control the <code>Workload's priority</code> without affecting the <code>pod's priority</code>. This feature is useful for these cases:</p> <ul> <li>Want to prioritize workloads that remain inactive for a specific duration.</li> <li>Want to set a lower priority for development workloads and higher priority for production workloads.</li> </ul>"},{"location":"kubernetes/resources/kueue/#cohort","title":"Cohort","text":"<p>A Cohort is essentially a pool of ClusterQueues that can lend or borrow unused quota resources to each other. This setup helps in efficiently utilizing the cluster's resources, especially when some namespaces might have underutilized quotas while others could be running out.</p> <p>When a ClusterQueue is part of a Cohort, Kueue tries to schedule workloads (jobs) onto the cluster resources managed by the ClusterQueues in the cohort. If a particular ClusterQueue doesn't have enough resources to accommodate a workload, it can \"borrow\" unused quota from other ClusterQueues within the same cohort.</p> <p>This borrowing mechanism is subject to certain rules, such as the total requested resources fitting within the combined unused quota of the Cohort members plus a borrowing limit. This ensures fair usage and prevents over-allocation of resources.</p>"},{"location":"kubernetes/resources/kueue/#deployment-strategies-examples","title":"Deployment strategies Examples","text":"<p>We aimed to establish two ClusterQueues tailored to different job types: Headless and Interactive. Within the science platform, users can directly launch interactive jobs, whereas headless jobs must be initiated through API calls. Consequently, we configured the ClusterQueues to accommodate these distinctions\u2014one queue dedicated to interactive jobs and another to headless jobs. This setup enables us to allocate resources appropriately to each job type and share resources between the queues as necessary. Additionally, we aimed to assign worker nodes in the Kubernetes cluster to specific ClusterQueues, ensuring that interactive and headless jobs run on their respective nodes.</p>"},{"location":"kubernetes/resources/kueue/#single-clusterqueue-with-workload-priority-class","title":"Single ClusterQueue with Workload Priority Class","text":"<p>This strategy allows you to use a single cluster queue and prioritize specific jobs or types of jobs over other jobs using the workload priority class. This has an advantage of simplicity and different types of jobs or jobs scheduled by different groups can have different relative priorities and use the same shared resources assigned to the ClusterQueue. This is a suitable strategy for a small cluster with few resources. The disadvantage of this approach is that in a case where there is a high volume of jobs with higher priority, jobs with lower priorities may never get executed.</p> <p>We configured the priority class for interactive workloads, based on headless and interactive jobs. Assign the priority class to the jobs with:</p> <p>WorkloadPriorityClass for interactive jobs </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: interactive-jobs-priority\nvalue: 10000\ndescription: \"Interactive jobs priority\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: WorkloadPriorityClass\nmetadata:\n  name: headless-job-priority\nvalue: 5000\ndescription: \"Headless jobs priority\"\n</code></pre> <p>Jobs with interactive WorkloadPriorityClass </p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: \"interactive-workload\"\n  labels:\n    ...\n    kueue.x-k8s.io/queue-name: interactive-local-queue\n    kueue.x-k8s.io/priority-class: interactive-job-priority\n  name: \"jobname\"\nspec:\n  parallelism: 1\n  completions: 1\n  ...\n</code></pre> <p>Given the assigned priorities, the scheduler will prioritize the interactive jobs first, and headless jobs may be preempted if resources are needed for interactive jobs.</p>"},{"location":"kubernetes/resources/kueue/#multiple-cluster-queues-with-multiple-resources","title":"Multiple Cluster Queues with multiple resources","text":"<p>This strategy uses multiple cluster queues, each associated with their own independent resources. This strategy addresses the drawback of the single queue with workload priority classes by ensuring that jobs are not starved of resources. This approach also enables Kubernetes administrators to associate specific worker nodes with cluster queues. One such example of this would be to associate general purpose compute nodes with one queue while assigning worker nodes with GPUs to another queue. This approach can also lead to some worker nodes being under-utilized while observing 100% utilization on other nodes. We will see how this problem can be addressed using a \"cohort\", a feature in Kueue.</p> <p>We created multiple ClusterQueues with different ResourceFlavors to segregate resource types across the cluster. We configured two ResourceFlavors based on job types (Interactive and Headless) and updated the ClusterQueues accordingly.</p> <p>ResourceFlavors for interactive and headless jobs </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"interactive-resource-flavor\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"headless-resource-flavor\"\n</code></pre> <p>Interactive and headless ClusterQueues with their respective ResourceFlavor </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-interactive\"\nspec:\n  ...\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"interactive-resource-flavor\"\n     ...   \n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-headless\"\nspec:\n  ...\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"headless-resource-flavor\"\n     ... \n</code></pre> Both ClusterQueues refer to different ResourceFlavors. Jobs on each queue will refer to the resources associated with the given ResourceFlavor."},{"location":"kubernetes/resources/kueue/#sharing-resources-of-clusterqueue-with-cohort","title":"Sharing resources of ClusterQueue with cohort","text":"<p>When admitting a job, if the workload's requested resources exceed the resources available on a ClusterQueue, the job will remain in a \"Suspended\" state until currently running jobs have run to completion, thereby freeing up resources. However, when a ClusterQueue is part of a Cohort, Kueue will examine the list of ResourceFlavors associated with the ClusterQueue. For each ResourceFlavor, Kueue attempts to fit a workload's pod set within the quotas defined for each resource type within that ResourceFlavor. If it is unable to fit, Kueue will evaluate the next ResourceFlavor on the list.</p> <p>We configured our ClusterQueues to share resources via a cohort by adding a common ResourceFlavor.</p> <p>ResourceFlavor for Cohort </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"common-flavor\"\n</code></pre> <p>Manifest for ClusterQueues with Cohort </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-interactive\"\nspec:\n  ...\n  cohort: \"workload-queues\"\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"common-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 6\n        borrowingLimit: 4\n      - name: \"memory\"\n        nominalQuota: 6Gi\n    ...\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-headless\"\nspec:\n  ...\n  cohort: \"workload-queues\"\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"common-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 4\n      - name: \"memory\"\n        nominalQuota: 4Gi\n        lendingLimit: 2Gi\n      ...\n</code></pre> <p>With these configured ClusterQueues, when jobs on <code>cluster-queue-interactive</code> have occupied all resources, new jobs can run on <code>cluster-queue-headless</code>.</p> <p>Within a cohort, Kueue prioritizes scheduling workloads that fit under the nominal quota. By default, if multiple workloads require borrowing, Kueue schedules higher priority workloads first. If the feature gate <code>PrioritySortingWithinCohort=false</code> is set, Kueue schedules workloads with the earliest <code>.metadata.creationTimestamp</code>.</p> <p>You can set the borrowing limit and lending limit to control resource sharing. For example, <code>cluster-queue-interactive</code> can admit workloads using up to 10 CPUs (6 + 4 borrowing limit). <code>cluster-queue-headless</code> can lend up to 2Gi of memory, allowing <code>cluster-queue-interactive</code> to use up to 8Gi of memory (6Gi + 2Gi lending limit).</p>"},{"location":"kubernetes/resources/kueue/#sharing-resources-with-different-resourceflavor","title":"Sharing resources with different ResourceFlavor","text":"<p>Next, we wanted to assign worker nodes to specific ClusterQueues in order to run the specific jobs on specific nodes. We assigned nodes with labels using the below command. With this, we assigned worker1, worker2, worker3 to interactive jobs and worker4, worker5 for the headless jobs.</p> <p>Script to add labels to node </p><pre><code>kubectl label nodes k8s-worker1 k8s-worker2 k8s-worker3 node-role.kubernetes.io/interactive-worker=true\nkubectl label nodes k8s-worker4 k8s-worker5 node-role.kubernetes.io/headless-worker=true\n</code></pre> <p>We will add this label in the resource flavor, in order to associate them with nodes.</p> <p>ResourceFlavors with node labels </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"interactive-resource-flavor\"\nspec:\n  nodeLabels:\n    node-role.kubernetes.io/interactive-worker: \"true\"\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ResourceFlavor\nmetadata:\n  name: \"headless-resource-flavor\"\nspec:\n  nodeLabels:\n    node-role.kubernetes.io/headless-worker: \"true\"\n</code></pre> <p>Jobs assigned to the interactive queue will launch on nodes labeled interactive-worker, and headless jobs on nodes labeled headless-worker. These two different ResourceFlavors won't work with a Cohort unless multiple ResourceFlavors are added to the resource groups.</p> <p>Two ClusterQueues with two ResourceFlavors configured with Cohort </p><pre><code>apiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-interactive\"\nspec:\n  ...\n  cohort: \"workload-queues\"\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"interactive-resource-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 30\n      - name: \"memory\"\n        nominalQuota: 30Gi\n      - name: \"ephemeral-storage\"\n        nominalQuota: 300Gi\n    - name: \"headless-resource-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 0\n      - name: \"memory\"\n        nominalQuota: 0Gi\n      - name: \"ephemeral-storage\"\n        nominalQuota: 0Gi\n---\napiVersion: kueue.x-k8s.io/v1beta1\nkind: ClusterQueue\nmetadata:\n  name: \"cluster-queue-headless\"\nspec:\n  ...\n  cohort: \"workload-queues\"\n  resourceGroups:\n  - coveredResources: [\"cpu\", \"memory\", \"ephemeral-storage\"]\n    flavors:\n    - name: \"headless-resource-flavor\"\n      resources:\n      - name: \"cpu\"\n        nominalQuota: 4\n      - name: \"memory\"\n        nominalQuota: 4Gi\n      - name: \"ephemeral-storage\"\n        nominalQuota: 50Gi\n</code></pre> <p>Now that <code>headless-resource-flavor</code> is mentioned in the resource groups for <code>cluster-queue-interactive</code>, the nodes and resources assigned to <code>cluster-queue-headless</code> can be used by <code>cluster-queue-interactive</code>.</p>"},{"location":"kubernetes/resources/multi-container-pod/","title":"multi-container pod","text":""},{"location":"kubernetes/resources/multi-container-pod/#multi-container-pod","title":"Multi container pod","text":""},{"location":"kubernetes/resources/multi-container-pod/#a-multi-container-is-something-that-has-more-than-one-container-in-pod","title":"A multi container is something that has more than one container in pod.","text":"<ul> <li>Side car -&gt; Log agent with a web server</li> <li>Adapter -&gt; Log agent that process different type of web server and pushes in the central log server.</li> <li>Ambassador-&gt; Microservice with DB agent which connects to different type and environments of database.</li> </ul>"},{"location":"kubernetes/resources/namespace/","title":"namespace","text":""},{"location":"kubernetes/resources/namespace/#namespace","title":"Namespace","text":"<p>NameSpace is a way to segregate different resources like dev,qa,prod etc. Default namespace is default.</p>"},{"location":"kubernetes/resources/namespace/#sample-dev-namespaceyaml","title":"Sample dev-namespace.yaml","text":"<pre><code>apiVersion: v1\nkind: NameSpace\nmetadata:\n  name: dev\n</code></pre>"},{"location":"kubernetes/resources/namespace/#some-imperative-commands","title":"Some imperative commands","text":"<pre><code># to create a namespace\nKubectl create -f dev-namepace.yaml\n\n# To get all the pod inside dev namespace\nkubectl get pods --namespace=dev\n\n# If we have to set the dev namespace permanently then we can keep it inside the KubeConfig\nKubectl config set-context $( kubectl config current-context ) -namespace=dev\n\n# To limit the resources using in a specific namespace we can use resource quota\n# to get all the namspaces\nkubectl get ns\n\n# To get count of the namespaces\nkubectl get ns --no-headers | wc -l\n</code></pre> <p>In pod-definition.yaml in metadata we can add namespace to mention the namespace where the pod will be deployed </p><pre><code># nginx-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: dev\n  labels:\n    app: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre> <pre><code># to get the pods in dev namespace \nkubectl -n dev get pods --no-header\n\n# In which namespace the nginx pod is deployed?\nAns: kubectl get pods --all-namespaces | grep nginx\n\n# In the same namespace we can connect to another pod via pod name but to connect with other resources in another namespace we have to maintain we proper format.\n# Resource-name.namespace.resource-type.domain\n# example: db-service.dev.svc.cluster.local\n# db-service is the name of the resource, dev is the namespace, svc is the resource type, cluster.local is he domain\n</code></pre>"},{"location":"kubernetes/resources/networking/","title":"Networking","text":""},{"location":"kubernetes/resources/networking/#kubernetes-networking-ebpf-in-action","title":"Kubernetes Networking: eBPF in Action","text":""},{"location":"kubernetes/resources/networking/#what-is-ebpf","title":"What is eBPF?","text":"<p>eBPF stands for extended Berkeley Packet Filter. It's a way to run small programs inside the Linux kernel \u2014 the core of your operating system. Back in the 1990s, BPF was just for filtering network packets. Now, eBPF does way more, like watching system activity or changing how traffic moves, all without messing up your system.</p> <p>Think of it as a helper built into Linux. You write a program, load it into the kernel, and it runs when something specific happens \u2014 like a network packet arriving. The kernel makes sure your code is safe so it won't crash anything. It's a powerful way to control what's going on under the hood.</p>"},{"location":"kubernetes/resources/networking/#medium","title":"Medium","text":"<ul> <li>Kubernetes Networking: eBPF in Action</li> </ul>"},{"location":"kubernetes/resources/node-selectors-and-node-affinity/","title":"node selectors and node affinity","text":""},{"location":"kubernetes/resources/node-selectors-and-node-affinity/#node-selectors-and-node-affinity","title":"Node selectors and Node Affinity","text":""},{"location":"kubernetes/resources/node-selectors-and-node-affinity/#to-label-a-node","title":"To label a node","text":"<pre><code>kubectl label node &lt;node-name&gt; key=value\n</code></pre>"},{"location":"kubernetes/resources/node-selectors-and-node-affinity/#the-specific-pod-with-specific-node-with-nodeselector-with-key-value-will-be-placed-on-that-node","title":"The specific pod with specific node with nodeSelector with key value will be placed on that node.","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  nodeSelector:\n    app: color\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre>"},{"location":"kubernetes/resources/node-selectors-and-node-affinity/#node-selector-is-easy-to-use-but-lack-the-advanced-features-that-is-why-we-are-using-node-affinity-here-also-at-first-we-must-label-the-node","title":"Node selector is easy to use but lack the advanced features that is why we are using node Affinity. Here also at first, we must label the node","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod \nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n          - matchExpressions:\n            - key: size\n              operator: In\n              values:\n              - \"Large\"\n              - \"Medium\"\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre> To place this nginx pod we must label the node with Large or Medium else pod will not be executed in the node."},{"location":"kubernetes/resources/node-selectors-and-node-affinity/#there-are-3-types-of-operators","title":"There are 3 types of operators:","text":"<ul> <li>In: If the any value from values is there in the labels in the node</li> <li>No: If the values from the values are there not in labels of thr node</li> <li>Exists: If the key exists in the label of the node</li> </ul>"},{"location":"kubernetes/resources/node-selectors-and-node-affinity/#there-are-3-types-of-nodeaffinity-selectors","title":"There are 3 types of nodeAffinity selectors.","text":"<ul> <li>requiredDuringSchedulingIgnoredDuringExecution:<ul> <li>During scheduling : Label is required in the node other wise pod will not be executed</li> <li>During execution : In if there is any change in label in node then pod will ignore that and continue executing</li> </ul> </li> <li>prefferedDuringSchedulingIgnoredDuringExecution:<ul> <li>During scheduling : It will try to find the node with the label. If it does not find the node, then it tries to place the pod in any node</li> <li>During execution : In if there is any change in label in node then pod will ignore that and continue executing</li> </ul> </li> <li>requiredDuringSchedulingRequiredDuringExecution:<ul> <li>During scheduling : Label is required in the node otherwise pod will not be executed</li> <li>During execution : In if there is any change in label in node then pod should have that label other wise it will stop executing</li> </ul> </li> </ul> <p>With the combination of taints and node affinity we can specify that which pod will place on which node.</p>"},{"location":"kubernetes/resources/pod-design/","title":"pod design","text":""},{"location":"kubernetes/resources/pod-design/#pod-design","title":"Pod Design","text":"<p>Labels and selectors are standard methods to group things together. We can keep these labels under metadata. These labels are used to identify a specific pod by the service, replica set, and deployment. Though one label is sufficient to identify, we can use as many labels as we want.</p> <p>Alongside labels, we can also add annotations. They are used to save build information and other documentation purposes.</p>"},{"location":"kubernetes/resources/pod-design/#finding-pods-by-label","title":"Finding Pods by Label","text":"<p>Find the pods where the label is <code>env=dev</code>: </p><pre><code>kubectl get pods -l env=dev\n</code></pre> <p>Find the count of the pods with this label: </p><pre><code>kubectl get pods -l env=dev --no-headers | wc -l\n</code></pre> <p>Find all the objects in the <code>prod</code> environment: </p><pre><code>kubectl get all -l env=prod\n</code></pre> <p>Count all the objects in the <code>prod</code> environment: </p><pre><code>kubectl get all -l env=prod --no-headers | wc -l\n</code></pre> <p>Find all the pods with all these labels: </p><pre><code>kubectl get pods -l env=prod,bu=finance,tier=frontend\n</code></pre>"},{"location":"kubernetes/resources/pod-design/#deployment-rollouts","title":"Deployment Rollouts","text":"<p>When we first create a deployment, it creates a rollout, and a new rollout creates a new deployment revision. When there is a change in deployment like an image update, it creates a new rollout which again creates a revision. This helps us to keep track of the changes.</p> <p>There are two types of deployment strategies:</p> <ul> <li>Recreate: Destroy all the containers, then create the containers with changes. During the period when the older version is down, the application becomes inaccessible.</li> <li>Rolling update: In this strategy, some of the old containers are down and new containers with changes are started in their place. If there is no mention of the deployment strategy in the YAML file, this strategy becomes the default.</li> </ul> <p>If we perform the <code>kubectl describe deployment &lt;deployment-name&gt;</code>, we can see the events of how containers went down and up. When there is an upgrade, the deployment object internally creates a replica set and then fills that with the new containers and deletes the containers from the old replica set.</p>"},{"location":"kubernetes/resources/pod-design/#rollback","title":"Rollback","text":"<p>To undo a change, use the following command: </p><pre><code>kubectl rollout undo deployment/&lt;deployment-name&gt;\n</code></pre> <p>In rollback, the deployment object creates another replica set and fills that.</p>"},{"location":"kubernetes/resources/pod-design/#summary-of-commands","title":"Summary of Commands","text":"<p>Create a deployment from a definition file: </p><pre><code>kubectl create -f &lt;deployment-definition.yml&gt;\n</code></pre> <p>Get the list of deployments: </p><pre><code>kubectl get deployments\n</code></pre> <p>Apply changes to a deployment from a definition file: </p><pre><code>kubectl apply -f &lt;deployment-definition.yml&gt;\n</code></pre> <p>Set a new image for a deployment: </p><pre><code>kubectl set image deployment/&lt;deployment-name&gt; nginx=nginx:1.9.1\n</code></pre> <p>Check the status of a rollout: </p><pre><code>kubectl rollout status deployment/&lt;deployment-name&gt;\n</code></pre> <p>View the rollout history of a deployment: </p><pre><code>kubectl rollout history deployment/&lt;deployment-name&gt;\n</code></pre> <p>Undo the last rollout: </p><pre><code>kubectl rollout undo deployment/&lt;deployment-name&gt;\n</code></pre>"},{"location":"kubernetes/resources/pod-design/#updating-a-deployment","title":"Updating a Deployment","text":"<p>Here are some handy examples related to updating a Kubernetes Deployment:</p> <p>Creating a deployment, checking the rollout status and history:</p> <p>In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:</p> <pre><code>master $ kubectl create deployment nginx --image=nginx:1.16\ndeployment.apps/nginx created\n\nmaster $ kubectl rollout status deployment nginx\nWaiting for deployment \"nginx\" rollout to finish: 0 of 1 updated replica are available...\ndeployment \"nginx\" successfully rolled out\n\nmaster $ kubectl rollout history deployment nginx\ndeployment.extensions/nginx\nREVISION CHANGE-CAUSE\n1     &lt;none&gt;\n</code></pre> <p>Using the --revision flag:</p> <p>Here the revision 1 is the first version where the deployment was created.</p> <p>You can check the status of each revision individually by using the --revision flag:</p> <pre><code>master $ kubectl rollout history deployment nginx --revision=1\ndeployment.extensions/nginx with revision #1\n\nPod Template:\n Labels:    app=nginx    pod-template-hash=6454457cdb\n Containers:  nginx:  Image:   nginx:1.16\n  Port:    &lt;none&gt;\n  Host Port: &lt;none&gt;\n  Environment:    &lt;none&gt;\n  Mounts:   &lt;none&gt;\n Volumes:   &lt;none&gt;\nmaster $\n</code></pre>"},{"location":"kubernetes/resources/pod/","title":"pods","text":""},{"location":"kubernetes/resources/pod/#pods","title":"Pods","text":"<pre><code># it will run a new pod with image.\nkubectl run &lt;pod_name&gt; --image=&lt;image_name&gt;\n\n# it will run a new pod called nginx with image as nginx\nkubectl run nginx --image=nginx\n\n# to get all the pods\nkubectl get pods\n\n# to get more details for the pods\nkubectl get pods -o wide\n\n# it will give us the detailed info about the specific pod\nkubectl describe pod &lt;pod-name&gt;\n</code></pre> <p>Kubernetes yaml file must contain these fields -&gt; apiVersion, kind, metadata, spec</p>"},{"location":"kubernetes/resources/pod/#sample-pod-configuration-yaml","title":"Sample pod configuration yaml","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  tier: frontend\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre>"},{"location":"kubernetes/resources/pod/#some-other-commands-to-interact-with-the-pod","title":"Some other commands to interact with the pod","text":"<pre><code># to create a pod with this yaml file.\nkubectl apply/create -f nginx-pod.yaml\n\n# to give more description of the nginx pod\nkubectl describe pod nginx\n\n# it will open a vi editor where we can change the pod definition file also this is a in memory pod definition file which is maintained by Kubernetes.\nkubectl edit pod nginx\n\n# to delete the nginx pod\nkubectl delete pod nginx\n\n# delete all the pods.\nkubectl delete --all pods\n\n# delete all resources in namespace.\nkubectl delete all --all -n &lt;name-space&gt;\n\n# It will not create any pod rather it's a imperative style of writing definition file where a pod definition file will created with the necessary fields.\nkubectl run redis --image=redis --dry-run=client -o yaml &gt; redis-pod.yaml\n</code></pre>"},{"location":"kubernetes/resources/readiness-and-liveness-probe/","title":"readiness and liveness probe","text":""},{"location":"kubernetes/resources/readiness-and-liveness-probe/#readiness-and-liveness-probe","title":"Readiness and liveness probe","text":"<p>Readiness probe: Sometimes container is up that does not mean that the container is ready for external communication like Jenkins takes time to boot up. So, we can configure an api or a script to check the container is ready or not.</p> <p>Liveness probe: It is way to check periodically that the application is healthy or not, otherwise it destroys the container and starts a new one.</p> <p>There are 3 ways to check: 1.  httpGet  2.  tcpSocket 3.  start-up script</p> <p>we can also add <code>initialDelaySeconds</code>, <code>periodSeconds</code>, <code>failureThreshold</code> configure other options</p>"},{"location":"kubernetes/resources/readiness-and-liveness-probe/#with-http-api","title":"With http api","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      readinessProbe:\n        httpGet:\n          path: /api/ready\n          port: 8080\n          httpHeaders:\n          - name: Custom-Header\n            value: Awesome\n        initialDelaySeconds: 10\n        periodSeconds: 5\n        failureThreshold: 8\n      livenessProbe:\n        httpGet:\n          path: /api/ready\n          port: 8080\n          httpHeaders:\n          - name: Custom-Header\n            value: Awesome\n        initialDelaySeconds: 10\n        periodSeconds: 5\n        failureThreshold: 8\n</code></pre>"},{"location":"kubernetes/resources/readiness-and-liveness-probe/#with-a-socket-connection-check","title":"With a socket connection check","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      readinessProbe:\n        tcpSocket:\n          port: 8080\n      livenessProbe:\n        tcpSocket:\n          port: 8080\n</code></pre>"},{"location":"kubernetes/resources/readiness-and-liveness-probe/#checking-by-starting-a-process","title":"checking by starting a process","text":"<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      readinessProbe:\n        exec:\n        - \"cat\"\n        - \"/app/ready\"\n      livenessProbe:\n        exec:\n        - \"cat\"\n        - \"/app/ready\"\n</code></pre>"},{"location":"kubernetes/resources/replica-set/","title":"replica set","text":""},{"location":"kubernetes/resources/replica-set/#replicaset","title":"ReplicaSet","text":"<p>ReplicaSet is a group of same pods where we can scale in(reduce) or scale out(increase) the number of the pods. </p><pre><code># it will create a replicaset from the definition file.\nkubectl create/apply -f &lt;replicaset-definition.yaml&gt;\n\n# to get all the replicaset in the default namespace\nkubectl get replicateset\n\n# to get all the replicaset in the default namespace\nkubectl describe replicaset\n\n# to delete the replicaset\nkubectl delete replicaset &lt;replicaset-name&gt;\n</code></pre>"},{"location":"kubernetes/resources/replica-set/#sample-replica-set-configuration-yaml","title":"Sample replica set configuration yaml","text":"<pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: nginx-replicaset\n  labels:\n    type: frontend\nspec:\n  selector:\n    matchLabels:\n      name: nginx-pod\n      type: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: nginx-pod\n        type: frontend\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n</code></pre> <p>labels under template section and mathLabels under selector should be same. That is how replicate set identifies the pod and controls the number of the pods. If we try to delete any pod or anyhow any pod got crashed, then Kubernetes automatically brings another pod in.</p>"},{"location":"kubernetes/resources/replica-set/#scale-commands","title":"Scale commands","text":"<pre><code># replace the previous nginx-repicaset with the current replicas given in the definition file\nkubectl replace -f nginx-replicaset.yaml\n\n# it will override the replicas given in the yaml\nkubectl scale --replicas=6 -f nginx-replicaset.yaml\n\n# it will open an editor and show the current configuration of the replicaset then we can easily scale out.\nkubectl edit replicaset nginx-replicaset\n\n# it will scale out an existing replicaset\nkubectl scale --replicas=6 replicaset nginx-replicaset\n</code></pre>"},{"location":"kubernetes/resources/resource-requirements/","title":"resource requirements","text":""},{"location":"kubernetes/resources/resource-requirements/#resource-requirements","title":"Resource requirements","text":"<pre><code># By default, in Kubernetes there is not restriction on usage and cpu usage.  \n# With the Limit range we can set the default memory and cpu of a pod in any namespace. \n# With resource block of the pod definition, we can also restrict the default and maximum memory usage.\n# It will create the memory limit of the container in any namespace\nKubectl apply -f mem-limit-range.yaml\n</code></pre> <pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: mem-limit-range\n  namespace: dev\nspec:\n  limits:\n  - default:\n      memory: \"512Mi\" #cpu usage\n    defaultRequest:\n      memory: \"256Mi\" #Memory usage\n    type: Container\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  containers:\n    - name: nginx\n      image: nginx\n      resources:\n        requests:\n          memory: \"64Mi\"\n          cpu: \"250m\"\n        limits:\n          memory: \"128Mi\"\n          cpu: \"500m\"\n</code></pre>"},{"location":"kubernetes/resources/security-context/","title":"security context","text":""},{"location":"kubernetes/resources/security-context/#security-context","title":"Security context","text":"<p>In each container there can be multiple processes running and there can be multiple processes running. </p> <p>These processes are separated by their namesapces. By default, docker runs every process as root user, but we can change it with the following command. <code>docker run --user=1000 ubuntu sleep 10</code></p> <p>docker limits the capabilities of the root user inside the container. It is not the same root user as the host machine root user. Via linux capabilities we can add or remove capabilities of the root user inside the container.</p> <pre><code># It will run ubuntu container with MAC_ADMIN privilege\nDocker run --cap-add MAC_ADMIN ubuntu\n\n# It will drop the privilege of kill a process\ndocker run --cap-drop kill ubuntu\n\n# It will add all the privileges\ndocker run --privileged ubuntu\n</code></pre> <p>This can be configured in the Kubernetes as well. In the pod level and in the container level. </p><pre><code>apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: ubuntu-pod\n    image: ubuntu\n    securityContext:\n      runAsUser: 1000\n      capabilities:\n        add: [\"MAC_ADMIN\",\"KILL\"]\n</code></pre> <p>There are 2 types of accounts in Kubernetes space: - User Account : Human user such as admin or developer - Service Account : Account created by application to interact with the Kubernetes clusters like monitoring app or build tools such as Jenkins.</p> <pre><code># It will create a service account named as dashboard-serviceaccount. \n# It will also create a secret with a token automatically. Now with this token we can call kubernetes api. \nKubectl create serviceaccount dashboard-serviceaccount\n\n# Like the following \n# curl https://192.168.56.70/api -insecure --header \"Authorization Bearer #TOKEN\"\n</code></pre> <p>We can create a service account, assign that right permission using role-based access control mechanism and expose the service account token and use it to configure the third-party application to authenticate Kubernetes api.</p> <p>But in latest version we have to create the token manually and then we have to bind it with service account.  kubectl create sa dashboard-sa</p>"},{"location":"kubernetes/resources/security-context/#we-can-create-a-secret-from-the-service-account","title":"We can create a secret from the service account","text":"<pre><code>apiVersion: v1\nkind: Secret\ntype: kubernetes.io/service-account-token\nmetadata:\n  name: dashboard-sa-token\n  annotations:\n    kubernetes.io/service-account.name: \"dashboard-sa\"\n</code></pre> If the third-party application is hosted in the same Kubernetes, then we can mount the service account token as volume. <p>We can edit the service account inside the POD, but we cannot change the service account of the deployment.  After change there will be a new rollout deployment.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n      serviceAccountName: nginx-serviceaccount\n      automountServiceAccountToken: false\n</code></pre> <p></p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - image: nginx\n    name: nginx\n    volumeMounts:\n    - mountPath: /var/run/secrets/tokens\n      name: vault-token\n  serviceAccountName: build-robot\n  volumes:\n  - name: vault-token\n    projected:\n      sources:\n      - serviceAccountToken:\n          path: vault-token\n          expirationSeconds: 7200\n          audience: vault\n</code></pre> <pre><code># By default, there is service account that is default service account.\n# It will fetch all the service account\nKubectl get serviceaccount\n\n# It will describe the default service account\nkubectl describe serviceaccount default\n\n# With this command we can fetch the token from sa.\nkubectl describe secret &lt;token-from- sa&gt;\n</code></pre> <p>This default service account is associated with image pull registry and image pull secrets.  We can also change it. For reference check this</p>"},{"location":"kubernetes/resources/service-mesh/","title":"service mesh","text":""},{"location":"kubernetes/resources/service-mesh/#links","title":"Links","text":""},{"location":"kubernetes/resources/service-mesh/#youtube","title":"Youtube","text":"<ul> <li>Istio Service Mesh Explained</li> <li>Service Mesh Explained | Sidecar Proxy &amp; Microservices Communication</li> <li>What is a service mesh?</li> <li>Service Mesh In Kubernetes Explained</li> <li>Istio &amp; Service Mesh - simply explained in 15 mins</li> <li>Service Mesh and its Architecture | How Microservices Communicate?</li> <li>Consul Service Mesh Tutorial for Beginners [Crash Course]</li> <li>Service Mesh explained in 60 minutes | Istio mTLS and Canary Demo | Complete beginner level guide</li> </ul>"},{"location":"kubernetes/resources/service-mesh/#blogs","title":"Blogs","text":"<ul> <li>What is a Service Mesh</li> <li>What is a Service Mesh?</li> <li>Service Mesh Architecture with Istio</li> <li>Service Mesh in Microservices</li> </ul>"},{"location":"kubernetes/resources/service/","title":"service","text":""},{"location":"kubernetes/resources/service/#service","title":"Service","text":"<p>For accessing the pods from the outside of the container we need service.</p> <p>There are three types of services. - NodePort - ClusterIP : Internal communication of pods. Service definition is almost same as the NodePort. Here type is ClusterIP. TargetPort is where the backend is exposed, and Port is where the service is exposed. Type ClusterIp is the default service type. - LoadBalancer : With the NodePort service we can make external facing application available on the port of the worker nodes. Let's say we have four cluster and one each server there are one frontend app deployed. With NodePort we can make external traffic forwarded to frontend pod but again for that we will have 4 urls for 4 services. So, need to have a loadbalancer here. We can have an external VM where nginx is deployed and then it will loadbalance 4 urls but it will be a complicated thing to manage. So we can use the inbuilt loadbalancer of different cloud platforms like Azure,GCP or AWS.</p>"},{"location":"kubernetes/resources/service/#sample-nginx-service-yaml","title":"Sample nginx service yaml","text":"<pre><code>#nginx-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\nspec:\n  type: NodePort\n  ports:\n    - port: 80  \n      targetPort: 80    # where the pod will listen (for nginx its 80)\n      nodePort: 30008   # In this port the service will be accesible\n  selector:\n    app: nginx-pod  # to connect with specific pods via pod's label\n</code></pre> <p>NodePort can range from 30000 to 32767. The work of the NodePort is to listen to a particular port and forward it to another node.</p>"},{"location":"kubernetes/resources/service/#some-service-commands","title":"Some service commands","text":"<pre><code># for creating nginx-service with nginx-deployment.yaml\nKubectl create -f nginx-deployment.yaml\n\n# To get all the services\nkubectl get services\n\n# To get details of the specific service, if there is no endpoint then the service is not attached to any pod.\nkubectl describe service &lt;service_name&gt;\n\n# to get the url of the NodePort service, but only for minikube\nminikube service nginx-service --url\n\n# Imperative style of creating service:\n\n# It will create a service named as nginx-service of type NodePort with specific port and targetPort and it will match labels of the deployment of nginx-deplpyment \n# and NodePort will be be assigned randomly in the range of 30000 to 32767\nkubectl expose deployment nginx-deployment --name=nginx-service --target-port=80 --port=80 --type=NodePort\n\n# It will do just the same as previous just that it will save all the configurations in the nginx-service.yaml\nkubectl expose deployment nginx-deployment --name=nginx-service --target-port=80 --port=80 --type=NodePort --dry-run=client -o yaml &gt; nginx-service.yaml\n</code></pre> <p>If we have a connection like this: - Voting-app -&gt; frontend app for gathering the votes which will save the votes in redis - Result-app -&gt; frontend app for showing the votes which will fetch votes from postgres - Redis -&gt; save the votes from in a in memory store - Postgres -&gt; save the votes in relational db - Worker-app -&gt; It will constantly fetch the vote count from redis and constantly update the vote count in postgres</p> <p>So, our setup will be like this: - Deploy the pods/deployment - Create the ClusterIP service for Redis and postgres - Create the NodePort service for Voting-app and Result-app</p>"},{"location":"kubernetes/resources/taints-and-tolerations/","title":"taints and tolerations","text":""},{"location":"kubernetes/resources/taints-and-tolerations/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>It helps to set restrictions to what pod to schedule in which node.</p> <p>Suppose we have 3 nodes and 4 pods, and we have applied a taint blue on node-A and we have added tolerant to pod-B so only pod-B is capable to be allocated in node-A. But in the same time pod-B can also be allocated to another node. Taints imposes a rule on the node that it will only be accepting pods with specific tolerations, but it does not impose any rule on pods. </p> <p>There are 3 taint effects. - NoExecute - PrefferNoSchedule - NoSchedule</p> <pre><code># It will impose a taint with key value with taint effect\nKubectl nodes &lt;node-name&gt; key=value:taint-effect\n\nkubectl nodes node-a color:blue:NoSchedule\n</code></pre> <p></p><pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-pod\n  namespace: default\n  labels:\n    app: nginx-pod\nspec:\n  tolerations:\n    - key: \"color\"\n      operator: \"Equal\"\n      value: \"blue\"\n      effect: \"NoSchedule\"\n  containers:\n    - name: nginx\n      image: nginx\n</code></pre> <pre><code># By default, the master node has some taints that prevents any pods to schedule on master.\n\n# to find the taints applied on the node\nKubectl describe node &lt;node-name&gt; | grep Taint\n\n# It will impose a taint with key value with taint effect.\nkubectl nodes &lt;node-name&gt; key=value:taint-effect -\n\n# To see a pod in which node\nKubectl get pods -o wide\n</code></pre>"},{"location":"linux/commands/","title":"commands","text":""},{"location":"linux/commands/#linux-most-used-commands-cheatsheet","title":"Linux Most Used Commands Cheatsheet","text":""},{"location":"linux/commands/#update-and-upgrade","title":"Update and Upgrade","text":"<pre><code>apt-get update &amp; apt-get upgrade -y\n</code></pre>"},{"location":"linux/commands/#user-management-commands","title":"User Management Commands","text":"<p>sudo - execute a command as another user or with elevated privileges</p> <p>Run command with the security privileges of the superuser (Super User DO). </p><pre><code>sudo\n</code></pre> <p>whoami - display the current user name</p> <p>Displays the username of the current user. </p><pre><code>whoami\n</code></pre> <p>Display who is logged in</p> <p>Shows a list of users currently logged into the system. </p><pre><code>who \n</code></pre> <p>Show the user you are logged in as and the groups you are part of</p> <p>Displays the current user and the groups they belong to. </p><pre><code>id\n</code></pre> <p>Show the groups you are part of</p> <p>Lists all the groups the current user is a member of. </p><pre><code>groups\n</code></pre> <p>useradd - add a new user to the system</p> <p>Creates a new user account. </p><pre><code>sudo useradd &lt;user-name&gt;\nsudo adduser &lt;user-name&gt; &lt;other-parameters&gt;\nsudo useradd harry\n</code></pre> <p>passwd - change the password for a user</p> <p>Changes the password for a specified user. </p><pre><code>sudo passwd &lt;user-name&gt;\n</code></pre> <p>To change the password of a user</p> <p>Locks the password of a specified user. </p><pre><code>sudo passwd -l &lt;user-name&gt;\n</code></pre> <p>To remove a newly created user</p> <p>Deletes a user account and their home directory. </p><pre><code>sudo userdel -r &lt;user-name&gt;\n</code></pre> <p>userdel - delete a user from the system</p> <p>Deletes a user account. </p><pre><code>sudo userdel harry\n</code></pre> <p>To add a user to a group</p> <p>Adds a user to a specified group. </p><pre><code>sudo usermod -a -G GROUPNAME USERNAME\n</code></pre> <p>To remove a user from a group</p> <p>Removes a user from a specified group. </p><pre><code>sudo deluser USER GROUPNAME\n</code></pre> <p>su - switch user to become another user</p> <p>Switches to another user account. </p><pre><code>su &lt;user-name&gt;\nsu john\n</code></pre> <p>finger - displays all the information about user</p> <p>Displays information about all users logged in. </p><pre><code>apt-get install finger\n</code></pre> <p>Shows information of all the users logged in</p> <p>Displays information about all users logged in. </p><pre><code>finger\n</code></pre> <p>Gives information of a particular user</p> <p>Displays information about a specified user. </p><pre><code>finger &lt;user-name&gt;\n</code></pre> <p>to exit from a logged in shell</p> <p>Exits the current shell session. </p><pre><code>exit\n</code></pre>"},{"location":"linux/commands/#information-about-a-program","title":"Information About a Program","text":"<p>which - locate a program or command in the system path</p> <p>Locates the path of a specified command. </p><pre><code>which &lt;command-name&gt;\nwhich vim\n</code></pre> <p>Find binary / source / manual for command</p> <p>Finds the binary, source, and manual page for a specified command. </p><pre><code>where &lt;command-name&gt;\nwhatis &lt;command-name&gt;\n</code></pre> <p>man - manual for a command</p> <p>Displays the manual page for a specified command. </p><pre><code>man &lt;command-name&gt;\nman ls\n</code></pre> <p>To navigate and search</p> <p>Navigates and searches within the manual page. </p><pre><code>ctrl + f\nctrl + b\ng\nG\n/string  -&gt; search\nh  -&gt; to display help\nq  -&gt; for quit\n</code></pre> <p>Search all man files for ifconfig</p> <p>Searches all manual pages for the term \"ifconfig\". </p><pre><code>man -k ifconfig\n</code></pre> <p>Search all man files for the string in quotes</p> <p>Searches all manual pages for the specified string. </p><pre><code>man -k \"copy files\"\n</code></pre>"},{"location":"linux/commands/#screen-shortcuts","title":"Screen Shortcuts","text":"<p>Start a screen session</p> <p>Starts a new screen session. </p><pre><code>screen\n</code></pre> <p>Resume a screen session</p> <p>Resumes a detached screen session. </p><pre><code>screen -r\n</code></pre> <p>Show your current screen sessions</p> <p>Lists all current screen sessions. </p><pre><code>screen -list\n</code></pre> <p>Activate commands for screen</p> <p>Activates screen commands. </p><pre><code>CTRL-A\n</code></pre> <p>Create a new instance of terminal</p> <p>Creates a new terminal instance within the screen session. </p><pre><code>CTRL-A c\n</code></pre> <p>Go to the next instance of terminal</p> <p>Switches to the next terminal instance within the screen session. </p><pre><code>CTRL-A n\n</code></pre> <p>Go to the previous instance of terminal</p> <p>Switches to the previous terminal instance within the screen session. </p><pre><code>CTRL-A p\n</code></pre> <p>Show current instances of terminals</p> <p>Lists all terminal instances within the screen session. </p><pre><code>CTRL-A \"\n</code></pre> <p>Rename the current instance</p> <p>Renames the current terminal instance within the screen session. </p><pre><code>CTRL-A A\n</code></pre>"},{"location":"linux/commands/#history-and-command-execution","title":"History and Command Execution","text":"<p>Clears the screen</p> <p>Clears the terminal screen. </p><pre><code>clear\n</code></pre> <p>Resets the terminal display</p> <p>Resets the terminal display settings. </p><pre><code>reset\n</code></pre> <p>history - display a list of previously executed commands</p> <p>Displays a list of previously executed commands. </p><pre><code>history\n</code></pre> <p>Shows the stuff typed - add a number to limit the last n items</p> <p>Displays the last n commands executed. </p><pre><code>history n\n</code></pre> <p>Execute a specific command from history</p> <p>Executes a specific command from the history list. </p><pre><code>!&lt;history-number-of-command&gt;\n!102\n</code></pre> <p>Add timestamp to history</p> <p>Adds a timestamp to each command in the history list. </p><pre><code>HISTTIMEFORMAT=\"%Y-%m-%d %T \"\n# Add this variable to .bashrc/.zshrc to make it permanent\n</code></pre> <p>Interactively search through previously typed commands</p> <p>Searches interactively through the command history. </p><pre><code>ctrl + r\n</code></pre> <p>To forward search (works in ZSH for me but not bash)</p> <p>Searches forward through the command history. </p><pre><code>ctrl + s\n</code></pre> <p>Execute the last command typed that starts with 'value'</p> <p>Executes the last command that starts with the specified value. </p><pre><code>![value]\n# Run last command starting with cd\n!cd\n</code></pre> <p>Print to the console the last command typed that starts with 'value'</p> <p>Prints the last command that starts with the specified value. </p><pre><code>![value]:p\n# Print last command starting with cd\n!cd:p\n</code></pre> <p>Execute previous command</p> <p>Executes the previous command. </p><pre><code>!!\n</code></pre> <p>Print to the console the last command typed</p> <p>Prints the previous command. </p><pre><code>!!:p\n</code></pre> <p>Execute previous command in sudo mode</p> <p>Executes the previous command with sudo. </p><pre><code>sudo !!\n</code></pre> <p>Last argument of previous command</p> <p>Uses the last argument of the previous command. </p><pre><code>!$\n</code></pre> <p>Last argument of previous command</p> <p>Uses the last argument of the previous command. </p><pre><code>ALT-.\n</code></pre> <p>All arguments of previous command</p> <p>Uses all arguments of the previous command. </p><pre><code>!*\n</code></pre> <p>Run previous command, replacing abc with 123</p> <p>Replaces a string in the previous command and executes it. </p><pre><code>^abc^123\n</code></pre>"},{"location":"linux/commands/#bash-variables","title":"Bash Variables","text":"<p>Show environment variables</p> <p>Displays all environment variables. </p><pre><code>env\n</code></pre> <p>Output value of $NAME variable</p> <p>Displays the value of the specified variable. </p><pre><code>echo $NAME\n</code></pre> <p>Create a new variable</p> <p>Creates a new variable. </p><pre><code>VARIABLE_NAME=value\n</code></pre> <p>Remove a variable</p> <p>Removes a specified variable. </p><pre><code>unset\n</code></pre> <p>Set $NAME to value, to set value of an environment variable</p> <p>Sets the value of an environment variable. </p><pre><code>export NAME=value\n</code></pre> <p>Executable search path</p> <p>Displays the executable search path. </p><pre><code>$PATH\n</code></pre> <p>Home directory</p> <p>Displays the home directory. </p><pre><code>$HOME \n~\n</code></pre> <p>Current shell</p> <p>Displays the current shell. </p><pre><code>$SHELL\n</code></pre> <p>Remove a variable</p> <p>Removes a specified variable. </p><pre><code>unset\n</code></pre>"},{"location":"linux/commands/#command-execution","title":"Command Execution","text":"<p>Run command A and then B, regardless of success of A</p> <p>Runs command A and then command B, regardless of the success of command A. </p><pre><code>[command-a]; [command-b]\n</code></pre> <p>Run command B if A succeeded</p> <p>Runs command B if command A succeeded. </p><pre><code>[command-a] &amp;&amp; [command-b]\n</code></pre> <p>Run command B if A failed</p> <p>Runs command B if command A failed. </p><pre><code>[command-a] || [command-b]\n</code></pre> <p>Run command A in background</p> <p>Runs command A in the background. </p><pre><code>[command-a] &amp;\n</code></pre> <p>Run command A and then pass the result to command B</p> <p>Runs command A and then passes the result to command B. </p><pre><code>[command-a] | [command-b]\n</code></pre> <p>stderr of cmd1 to cmd2</p> <p>Redirects stderr of command A to command B. </p><pre><code>[command-a] |&amp; [command-b]\n</code></pre> <p>Run cmd in a subshell</p> <p>Runs a command in a subshell. </p><pre><code>cmd &amp;\n</code></pre>"},{"location":"linux/commands/#help-and-information","title":"Help and Information","text":"<p>Offers help</p> <p>Displays help for a specified command. </p><pre><code>[command] -h\n</code></pre> <p>Offers help</p> <p>Displays help for a specified command. </p><pre><code>[command] --help\n</code></pre> <p>Offers help</p> <p>Displays help for a specified command. </p><pre><code>info [command]\n</code></pre> <p>Show the help manual for [command]</p> <p>Displays the manual page for a specified command. </p><pre><code>man [command]\n</code></pre> <p>Gives a one-line description of [command]</p> <p>Displays a one-line description of a specified command. </p><pre><code>whatis [command]\n</code></pre> <p>Searches for command with keywords in description</p> <p>Searches for commands with keywords in their descriptions. </p><pre><code>apropos [search-pattern]\n</code></pre> <p>Open line in the editor to write a command</p> <p>Opens a line in the editor to write a command. </p><pre><code>ctrl + x then ctrl + e\n</code></pre> <p>Matrix style animation in command line</p> <p>Displays a matrix-style animation in the command line. </p><pre><code>cmatrix\n</code></pre> <p>Zoom in to the command prompt</p> <p>Zooms in to the command prompt. </p><pre><code>ctrl +\n</code></pre> <p>Zoom out in the command prompt</p> <p>Zooms out in the command prompt. </p><pre><code>ctrl -\n</code></pre>"},{"location":"linux/commands/#file-and-directory-management","title":"File and Directory Management","text":"<p>ls - list the files and directories in the current directory</p> <p>Lists the files and directories in the current directory. </p><pre><code>ls\n</code></pre> <p>In a list format, Long listing format</p> <p>Lists files and directories in a long listing format. </p><pre><code>ls -l\nls -l &lt;file-name&gt;\n</code></pre> <p>Long listing of parent directory</p> <p>Lists files and directories in the parent directory in a long listing format. </p><pre><code>ls -l ..\n</code></pre> <p>-a means files in the current directory including hidden files</p> <p>Lists all files, including hidden files, in the current directory. </p><pre><code>ls -a\n</code></pre> <p>All files in list format</p> <p>Lists all files in the current directory in a list format. </p><pre><code>ls -al\n</code></pre> <p>Long listing with Human readable file sizes</p> <p>Lists files and directories in a long listing format with human-readable file sizes. </p><pre><code>ls -lh\n</code></pre> <p>Entire content of folder recursively</p> <p>Lists the entire content of a folder recursively. </p><pre><code>ls -R\n</code></pre> <p>Entire content of folder recursively in reverse order</p> <p>Lists the entire content of a folder recursively in reverse order. </p><pre><code>ls -r\n</code></pre> <p>Sort by last modified</p> <p>Sorts files and directories by last modified date. </p><pre><code>ls -t\n</code></pre> <p>Sort by file size</p> <p>Sorts files and directories by file size. </p><pre><code>ls -S\n</code></pre> <p>One file per line</p> <p>Lists one file per line. </p><pre><code>ls -1\n</code></pre> <p>Comma separated output</p> <p>Lists files and directories in a comma-separated format. </p><pre><code>ls -m\n</code></pre> <p>Quoted output</p> <p>Lists files and directories with quoted output. </p><pre><code>ls -Q\n</code></pre> <p>List files in /etc</p> <p>Lists files in the /etc directory. </p><pre><code>ls /etc\n# List files in the /var directory\nls -a /var/\n</code></pre> <p>tee - redirect output to both a file and the console</p> <p>Redirects output to both a file and the console. </p><pre><code>ls | tee file.txt\n</code></pre>"},{"location":"linux/commands/#traverse-directory","title":"Traverse Directory","text":"<p>pwd - print the current working directory</p> <p>Prints the current working directory. </p><pre><code>pwd\n</code></pre> <p>cd - change the current directory</p> <p>Changes the current directory.</p> <p>Go to Home directory </p><pre><code>cd\n</code></pre> <p>Change directory e.g. cd Documents</p> <p>Changes to the specified directory. </p><pre><code>cd [folder]\ncd /usr/bin\ncd /&lt;click-tab&gt;\n</code></pre> <p>Go to Home directory </p><pre><code>cd ~\n</code></pre> <p>Go to the root of drive </p><pre><code>cd /\n</code></pre> <p>Go to the previous directory </p><pre><code>cd -\n</code></pre> <p>Move 1 levels up </p><pre><code>cd ..\n</code></pre> <p>Push and pop directories</p> <p>Pushes and pops directories from the directory stack. </p><pre><code>pushd &lt;dir-name&gt;\npopd &lt;dir-name&gt;\n</code></pre> <p>Root directory marker(#) and user directory marker(/)</p> <p>. represent Current folder, e.g. ls .</p> <p>Represents the current folder. .. represent Parent/enclosing directory, e.g. ls ..</p> <p>Represents the parent/enclosing directory.</p>"},{"location":"linux/commands/#file-operations","title":"File Operations","text":"<p>Opens a file (as if you double clicked it)</p> <p>Opens a file as if you double-clicked it. </p><pre><code>open .\nxdg-open\n</code></pre> <p>Opens the file using the nano editor</p> <p>Opens a file using the nano editor. </p><pre><code>nano [file]\n</code></pre> <p>Opens the file using the vim editor</p> <p>Opens a file using the vim editor. </p><pre><code>vim [file]\n</code></pre> <p>touch - create a new empty file or update the timestamp of an existing file</p> <p>Creates a new empty file or updates the timestamp of an existing file. </p><pre><code>touch &lt;file-name&gt;\ntouch shayan.txt\n</code></pre> <p>Multiple files at one time</p> <p>Creates multiple files at one time. </p><pre><code>touch &lt;file-name-1&gt; &lt;file-name-2&gt;\n</code></pre> <p>Create 10 files in a single go</p> <p>Creates 10 files in a single go. </p><pre><code>touch &lt;file-name&gt;{0..10}\n</code></pre> <p>Do not update anything just change the modified timestamp</p> <p>Changes the modified timestamp of an existing file without updating its content. </p><pre><code>touch &lt;already-existed-file&gt;\n</code></pre> <p>echo - display text or variables to the console</p> <p>Displays text or variables to the console. </p><pre><code>echo \"hello world\"\n</code></pre> <p>Single arrow (&gt;) will override the content</p> <p>Overrides the content of a file. </p><pre><code>echo \"hello world\" &gt; test.txt\n</code></pre> <p>Double arrow (&gt;&gt;) will append the content</p> <p>Appends content to a file. </p><pre><code>echo \"hello world\" &gt;&gt; test.txt\n</code></pre> <p>Truncate a file</p> <p>Truncates a file to zero size. </p><pre><code>truncate -s 0 &lt;file-name&gt;\n</code></pre> <p>rm - remove files or directories</p> <p>Removes files or directories. </p><pre><code>rm &lt;file-name&gt;\n</code></pre> <p>-v for verbose</p> <p>Removes files or directories with verbose output. </p><pre><code>rm -v &lt;file-name&gt;\nrm &lt;file-name-1&gt; &lt;file-name-2&gt;\nrm example.txt\n</code></pre> <p>-r for recursive, delete directly and all its contents</p> <p>Removes directories and their contents recursively. </p><pre><code>rm -r &lt;non-empty-dir&gt;\n</code></pre> <p>-i for interactive, Remove with confirmation</p> <p>Removes files or directories with confirmation. </p><pre><code>rm -ri &lt;non-empty-dir&gt;\n</code></pre> <p>Force removal without confirmation</p> <p>Forces removal of files or directories without confirmation. </p><pre><code>rm -f [file]\n</code></pre> <p>This command will delete everything in the system</p> <p>Deletes everything in the system (use with caution). </p><pre><code>rm -rf /\n</code></pre> <p>mkdir - create a new directory</p> <p>Creates a new directory. </p><pre><code>mkdir &lt;dir-name&gt;\n</code></pre> <p>Create a new directory called test</p> <p>Creates a new directory called test. </p><pre><code>mkdir test\n</code></pre> <p>-p will create all the nested folders if that is not present already</p> <p>Creates nested directories if they do not already exist. </p><pre><code>mkdir -p /dir1/dir2/dir3\n</code></pre> <p>Make directory and subdirectory in a single command</p> <p>Creates a directory and subdirectory in a single command. </p><pre><code>mkdir -p test2/test2\n</code></pre> <p>Make multiple directories</p> <p>Creates multiple directories. </p><pre><code>mkdir test2 test3 \n</code></pre> <p>rmdir - Remove directory (only operates on empty directories)</p> <p>Removes an empty directory. </p><pre><code>rmdir &lt;dir-name&gt;\n</code></pre> <p>Deletes the text.txt file in the directory called test</p> <p>Deletes a file in a specified directory. </p><pre><code>rmr test/text.txt\n</code></pre> <p>cp - copy files or directories</p> <p>Copies files or directories. </p><pre><code>cp [file] [dir]\ncp &lt;file-name&gt; &lt;file-name-with-path&gt;\n</code></pre> <p>Copies text.txt to a new file called text2.txt</p> <p>Copies a file to a new file. </p><pre><code>cp [file] [newfile]\ncp text.txt text2.txt\n</code></pre> <p>Copy dir-1 content to dir-1-copy folder, -r is needed as folder is not empty</p> <p>Copies the content of a directory to another directory. </p><pre><code>cp -r &lt;dir-1&gt; &lt;dir-1-copy&gt;\n</code></pre> <p>mv - move or rename files or directories</p> <p>Moves or renames files or directories. </p><pre><code>mv &lt;file-name&gt; &lt;file-name-with-path&gt;\n</code></pre> <p>Renames the file to a new filename</p> <p>Renames a file to a new filename. </p><pre><code>mv [file] [new-filename]\nmv example.txt backup/\n</code></pre> <p>Moves text.txt to a different directory</p> <p>Moves a file to a different directory. </p><pre><code>mv text.txt test/\n</code></pre> <p>Moves all txt files to a different directory</p> <p>Moves all files with a .txt extension to a different directory. </p><pre><code>mv *.txt test/\n</code></pre> <p>List all files and subfolders and files within subfolders within the test directory</p> <p>Lists all files and subfolders within a specified directory. </p><pre><code>tree /etc\n</code></pre> <p>Divides the file into x columns</p> <p>Divides the content of a file into x columns. </p><pre><code>pr -x\n</code></pre> <p>Assigns a header to the file</p> <p>Assigns a header to the content of a file. </p><pre><code>pr -h\n</code></pre> <p>Denotes the file with Line Numbers</p> <p>Adds line numbers to the content of a file. </p><pre><code>pr -n\n</code></pre> <p>Prints \"c\" copies of the File</p> <p>Prints multiple copies of a file. </p><pre><code>lp -nc , lpr c\n</code></pre> <p>Specifies name of the printer</p> <p>Specifies the name of the printer. </p><pre><code>lp-d lp-P\n</code></pre>"},{"location":"linux/commands/#file-content-operations","title":"File Content Operations","text":"<p>cat - concatenate and display files</p> <p>Concatenates and displays files. </p><pre><code>cat &lt;file-name&gt;\ncat &lt;file-name&gt; | sort\ncat example.txt\ncat &lt;file-1&gt; &lt;file-2&gt; &lt;file-3&gt;\n</code></pre> <p>Creates a new file</p> <p>Creates a new file. </p><pre><code>cat &gt; filename\n</code></pre> <p>Joins two files (file1, file2) and stores the output in a new file (file3)</p> <p>Joins two files and stores the output in a new file. </p><pre><code>cat file1 file2 &gt; file3\n</code></pre> <p>Concatenate multiple files</p> <p>Concatenates multiple files. </p><pre><code>cat &lt;file-1&gt; &lt;file-2&gt; &lt;file-3&gt; &gt; &lt;file-name&gt;\n</code></pre> <p>Copies file contents to clipboard</p> <p>Copies file contents to the clipboard. </p><pre><code>pbcopy &lt; [file]\n</code></pre> <p>Paste clipboard contents</p> <p>Pastes clipboard contents. </p><pre><code>pbpaste\n</code></pre> <p>Paste clipboard contents into file</p> <p>Pastes clipboard contents into a file. </p><pre><code>pbpaste &gt; [file]\n</code></pre> <p>sort - sort lines of text in a file or input</p> <p>Sorts lines of text in a file or input. </p><pre><code>cat file.txt\n# banana\n# orange\n# apple\nsort file.txt\n# apple\n# banana\n# orange\n</code></pre> <p>Sort numeric values</p> <p>Sorts numeric values. </p><pre><code>sort -n numeric-files\n</code></pre> <p>uniq - remove duplicate lines from a file or input</p> <p>Removes duplicate lines from a file or input. </p><pre><code>cat file.txt\n# apple\n# orange\n# banana\n# apple\n# banana\nuniq file.txt\n# apple\n# orange\n# banana\n</code></pre> <p>Output file content delivered in screensize chunks</p> <p>Outputs file content in screensize chunks. </p><pre><code>less &lt;file-name&gt;\n</code></pre> <p>Get type of file</p> <p>Gets the type of a file. </p><pre><code>file &lt;file-1&gt;\n</code></pre> <p>head/tail - display the first/last few lines of a file or input</p> <p>Displays the first few lines of a file or input. </p><pre><code>head &lt;file-name&gt;\nhead -f &lt;file-name&gt;\nhead file.txt\n</code></pre> <p>Displays the first 10 lines of the file</p> <p>Displays the first 10 lines of a file. </p><pre><code>head cloud-init.log\n</code></pre> <p>Displays the first 5 lines of the file</p> <p>Displays the first 5 lines of a file. </p><pre><code>head -n 5 cloud-init.log\n</code></pre> <p>Show distribution</p> <p>Displays the distribution information. </p><pre><code>head -n1 /etc/issue\n</code></pre> <p>Display last 10 lines</p> <p>Displays the last 10 lines of a file. </p><pre><code>tail &lt;file-name&gt;\n</code></pre> <p>Output last lines of file as it changes, follow the file</p> <p>Outputs the last lines of a file as it changes. </p><pre><code>tail -f &lt;file-name&gt;\n</code></pre> <p>Compare two files</p> <p>Compares two files. </p><pre><code>cmp &lt;file-name-1&gt; &lt;file-name-2&gt;\n</code></pre> <p>Compares the two text files</p> <p>Compares two text files. </p><pre><code>diff &lt;file-name-1&gt; &lt;file-name-2&gt;\n</code></pre>"},{"location":"linux/commands/#io-redirection","title":"IO Redirection","text":"<p>Tell command to read content from a file</p> <p>Tells a command to read content from a file. </p><pre><code>[command] &lt; [file]\n</code></pre> <p>Output of cmd2 as file input to cmd1</p> <p>Uses the output of one command as the input to another command. </p><pre><code>cmd1 &lt;(cmd2)\n</code></pre> <p>Push output to file, keep in mind it will get overwritten</p> <p>Pushes the output of a command to a file, overwriting the file. </p><pre><code>[command] &gt; [file]\n</code></pre> <p>Append output to existing file</p> <p>Appends the output of a command to an existing file. </p><pre><code>[command] &gt;&gt; [file]\n</code></pre> <p>Discard stdout of cmd</p> <p>Discards the stdout of a command. </p><pre><code>cmd &gt; /dev/null\n</code></pre> <p>Error output (stderr) of cmd to file</p> <p>Redirects the stderr of a command to a file. </p><pre><code>cmd 2&gt; file\n</code></pre> <p>stdout to same place as stderr</p> <p>Redirects the stdout of a command to the same place as stderr. </p><pre><code>cmd 1&gt;&amp;2\n</code></pre> <p>stderr to same place as stdout</p> <p>Redirects the stderr of a command to the same place as stdout. </p><pre><code>cmd 2&gt;&amp;1\n</code></pre> <p>Every output of cmd to file</p> <p>Redirects all output of a command to a file. </p><pre><code>cmd &amp;&gt; file\n</code></pre>"},{"location":"linux/commands/#find-and-locate","title":"Find and Locate","text":"<p>Find the files in a directory</p> <p>Finds files in a directory. </p><pre><code>find &lt;dir-name&gt; -name &lt;name-of-file&gt;\nfind /Users -name \"file.txt\"\nfind &lt;other-parameters&gt;\n</code></pre> <p>Find files starting with name in dir</p> <p>Finds files starting with a specified name in a directory. </p><pre><code>find /dir/ -name name*\n</code></pre> <p>Find files owned by name in dir</p> <p>Finds files owned by a specified user in a directory. </p><pre><code>find /dir/ -user name\n</code></pre> <p>Searches within /var and subdirectories</p> <p>Searches for files within a specified directory and its subdirectories. </p><pre><code>find /var -name *.log\n</code></pre> <p>Find files modified less than num minutes ago in dir</p> <p>Finds files modified less than a specified number of minutes ago in a directory. </p><pre><code>find /dir/ -mmin num\n</code></pre> <p>locate - locate any file on the system</p> <p>Locates any file on the system. </p><pre><code>locate file.txt\n</code></pre> <p>Displays directory containing cloud-init.log</p> <p>Displays the directory containing a specified file. </p><pre><code>locate cloud-init.log\n</code></pre> <p>Displays directory containing cloud-init.log and ignores case</p> <p>Displays the directory containing a specified file, ignoring case. </p><pre><code>locate -I cloud-init.log\n</code></pre>"},{"location":"linux/commands/#search-and-grep","title":"Search and Grep","text":"<p>Search for all lines that contain the pattern</p> <p>Searches for all lines that contain a specified pattern. </p><pre><code>grep [search_pattern] [files]\ngrep \"Tom\" file.txt\n</code></pre> <p>Search for all lines that contain the case-insensitive pattern</p> <p>Searches for all lines that contain a specified pattern, ignoring case. </p><pre><code>grep -i [search_pattern] [file]\n</code></pre> <p>Recursively search in all files in specified directory for all lines that contain the pattern</p> <p>Recursively searches for all lines that contain a specified pattern in a directory. </p><pre><code>grep -r [search_pattern] [dir]\n</code></pre> <p>Search for all lines that do NOT contain the pattern</p> <p>Searches for all lines that do not contain a specified pattern. </p><pre><code>grep -v [search_pattern] [file]\n</code></pre> <p>Show matched part of file only</p> <p>Shows only the matched part of a file. </p><pre><code>grep -o [search_pattern] [file]\n</code></pre> <p>Spotlight search for files (names, content, other metadata)</p> <p>Performs a spotlight search for files based on names, content, and other metadata. </p><pre><code>mdfind [search_pattern]\nmdfind skateboard\n</code></pre> <p>Spotlight search for files named like pattern in the given directory</p> <p>Performs a spotlight search for files named like a specified pattern in a directory. </p><pre><code>mdfind -onlyin [dir] -name [pattern]\n</code></pre>"},{"location":"linux/commands/#file-permissions","title":"File Permissions","text":"<p>chmod - change the permissions of a file or directory</p> <p>Changes the permissions of a file or directory. </p><pre><code>chmod &lt;file-name&gt; &lt;file-mod&gt;\nchmod +x &lt;file-name&gt;\nchmod 700 file.txt\n</code></pre> <p>Change mode of file to 775</p> <p>Changes the mode of a file to 775. </p><pre><code>chmod 775 file\n</code></pre> <p>Recursively chmod folder to 600</p> <p>Recursively changes the mode of a folder to 600. </p><pre><code>chmod -R 600 folder\n</code></pre> <p>chown - change the owner of a file or directory</p> <p>Changes the owner of a file or directory. </p><pre><code>chown &lt;user-name&gt; &lt;file-name&gt;\n</code></pre> <p>Change file owner to user and group to group</p> <p>Changes the owner and group of a file. </p><pre><code>chown user:group file\n</code></pre> <p>Change the user as well as group for a file or directory</p> <p>Changes the owner and group of a file or directory. </p><pre><code>chown user:group filename\n</code></pre> <pre><code>chown new_owner example.txt\n</code></pre>"},{"location":"linux/commands/#network-commands","title":"Network Commands","text":"<p>Print or set system name</p> <p>Prints or sets the system name. </p><pre><code>hostname\n</code></pre> <p>ifconfig - display or configure network interfaces</p> <p>Displays or configures network interfaces. </p><pre><code>ifconfig\n</code></pre> <pre><code>ip address\nip address | grep eth0\nip address | grep eth0 | grep inet\nip address | grep eth0 | grep inet | awk '{print $2}'\n</code></pre> <p>ssh - connect to a remote server securely</p> <p>Connects to a remote server securely. </p><pre><code>ssh username@ip-address\n</code></pre> <p>scp - securely copy files between systems</p> <p>Securely copies files between systems. </p><pre><code>scp myfile.txt user@remotehost:/home/user/\n</code></pre> <p>Display files in the current directory of a remote computer</p> <p>Displays files in the current directory of a remote computer. </p><pre><code>dir\n</code></pre> <p>Change directory to \"dirname\" on a remote computer</p> <p>Changes the directory to a specified directory on a remote computer. </p><pre><code>cd \"dirname\"\n</code></pre> <p>Upload 'file' from local to remote computer</p> <p>Uploads a file from the local computer to a remote computer. </p><pre><code>put file\n</code></pre> <p>Download 'file' from remote to local computer</p> <p>Downloads a file from a remote computer to the local computer. </p><pre><code>get file\n</code></pre> <p>Logout</p> <p>Logs out from the remote computer. </p><pre><code>quit\n</code></pre>"},{"location":"linux/commands/#networking-commands","title":"Networking Commands","text":"<p>For dns</p> <p>Displays DNS information. </p><pre><code>cat /etc/resolv.conf\nresolvectl status\n</code></pre> <p>ping - test network connectivity</p> <p>Tests network connectivity. </p><pre><code>ping &lt;ip-address&gt;\nping 8.8.8.8\n</code></pre> <p>Test the destination at 8.8.8.8 by sending five ICMP packets</p> <p>Tests network connectivity by sending a specified number of ICMP packets. </p><pre><code>ping -c 5 8.8.8.8\n</code></pre> <p>Print the route packets take to network host</p> <p>Prints the route packets take to a network host. </p><pre><code>traceroute &lt;url&gt;\n</code></pre> <p>netstat - display network connection information</p> <p>Displays network connection information. </p><pre><code>netstat\nnetstat -tulpn\n</code></pre> <p>Display the route table</p> <p>Displays the route table. </p><pre><code>netstat -r\n</code></pre> <p>Display open connections for a specific port</p> <p>Displays open connections for a specific port. </p><pre><code>netstat -np | grep \"80\"\n</code></pre> <pre><code>ss \nss -tulpn\n</code></pre> <p>route - view or configure network routing tables</p> <p>Views or configures network routing tables. </p><pre><code>route [options] [add/delete/show]\n</code></pre> <p>Allow port 80 to the system</p> <p>Allows port 80 to the system. </p><pre><code>sudo ufw allow 80\nsudo ufw enable\nsudo ufw status\n</code></pre> <pre><code>wget &lt;downloadable-url&gt;\n</code></pre> <pre><code>curl &lt;url&gt;\ncurl &lt;downloadable-url&gt; &gt; &lt;file-name&gt;\n</code></pre>"},{"location":"linux/commands/#system-information","title":"System Information","text":"<p>uname - display system information</p> <p>Displays system information. </p><pre><code>uname\nuname -r\n</code></pre> <p>Show system and kernel</p> <p>Displays system and kernel information. </p><pre><code>uname -a\n</code></pre> <p>Gives free RAM on your system</p> <p>Displays free RAM on the system. </p><pre><code>free\n</code></pre>"},{"location":"linux/commands/#disk-usage","title":"Disk Usage","text":"<p>df - display disk space usage</p> <p>Displays disk space usage. </p><pre><code>df\ndf -H\n</code></pre> <p>du - display disk usage by file or directory</p> <p>Displays disk usage by file or directory. </p><pre><code>du\n</code></pre> <p>Display file or file system status</p> <p>Displays file or file system status. </p><pre><code>stat\n</code></pre> <p>mount - mount a file system</p> <p>Mounts a file system. </p><pre><code>mount\nmount | column -t\nsudo mount /dev/sdb1 /mnt/usb\n</code></pre> <p>umount - unmount a file system</p> <p>Unmounts a file system. </p><pre><code>sudo umount /mnt/usb\n</code></pre>"},{"location":"linux/commands/#process-commands","title":"Process Commands","text":"<p>ps - display information about running processes</p> <p>Displays information about running processes. </p><pre><code>ps\nps aux\n</code></pre> <p>Gives the status of a particular process</p> <p>Displays the status of a particular process. </p><pre><code>ps PID\n</code></pre> <p>To send a process to the background</p> <p>Sends a process to the background. </p><pre><code>bg\n</code></pre> <p>To run a stopped process in the foreground</p> <p>Runs a stopped process in the foreground. </p><pre><code>fg\n</code></pre> <p>Display current jobs</p> <p>Displays current jobs. </p><pre><code>jobs\n</code></pre> <p>Run a command immune to hangups</p> <p>Runs a command immune to hangups. </p><pre><code>nohup\n</code></pre> <p>Gives the Process ID (PID) of a process</p> <p>Displays the Process ID (PID) of a process. </p><pre><code>pidof\n</code></pre> <p>Starts a process with a given priority</p> <p>Starts a process with a given priority. </p><pre><code>nice\n</code></pre> <p>Changes priority of an already running process</p> <p>Changes the priority of an already running process. </p><pre><code>renice\n</code></pre> <p>top - display system resource usage and processes</p> <p>Displays system resource usage and processes. </p><pre><code>top\n</code></pre> <p>htop - an interactive process viewer and system monitor</p> <p>Displays an interactive process viewer and system monitor. </p><pre><code>htop\n</code></pre> <p>kill - terminate a process</p> <p>Terminates a process. </p><pre><code>kill [PID]\n</code></pre> <p>kill can take different flags, -9 is for SIGKILL, -15 is for SIGTERM</p> <p>Terminates a process with a specified signal. </p><pre><code>kill -9 &lt;process-id&gt;\nkill -15 &lt;process-id&gt;\n</code></pre> <p>-l to list all the flags</p> <p>Lists all the flags for the kill command. </p><pre><code>kill -l\n</code></pre> <p>Kill process with name name</p> <p>Terminates a process with a specified name. </p><pre><code>pkill &lt;process-name&gt;\n</code></pre> <p>Kill process with name name forcefully</p> <p>Terminates a process with a specified name forcefully. </p><pre><code>pkill -9 &lt;process-name&gt;\n</code></pre>"},{"location":"linux/links/","title":"links","text":""},{"location":"linux/links/#linux","title":"Linux","text":""},{"location":"linux/links/#udemy","title":"Udemy","text":""},{"location":"linux/links/#introduction","title":"Introduction","text":"<ul> <li>Linux for Beginners</li> <li>Linux Foundation Certified Systems Administrator - LFCS</li> <li>Complete Linux Training Course to Get Your Dream IT Job</li> <li>Mastering Linux: The Comprehensive Guide</li> </ul>"},{"location":"linux/links/#terminal-commands","title":"Terminal Commands","text":"<ul> <li>Linux Command Line Basics</li> <li>Learn Linux in 5 Days and Level Up Your Career</li> <li>Linux Mastery: Master the Linux Command Line in 11.5 Hours</li> <li>The Linux Command Line Bootcamp: Beginner To Power User</li> </ul>"},{"location":"linux/links/#freecodecamp-series","title":"Freecodecamp Series","text":"<ul> <li>Linux Operating System - Crash Course for Beginners</li> <li>The 50 Most Popular Linux &amp; Terminal Commands - Full Course for Beginners</li> <li>The Linux Command Handbook - Learn Linux Commands for Beginners</li> <li>Introduction to Linux - Full Course for Beginners</li> <li>Bash Scripting Tutorial - Linux Shell Script and Command Line for Beginners</li> <li>Linux Server Course - System Configuration and Operation</li> <li>How to Configure and Operate Linux Servers - Full Course</li> <li>Linux Device Drivers Development Course for Beginners</li> </ul>"},{"location":"linux/links/#websites","title":"Websites","text":"<ul> <li>terminal-mac-cheatsheet</li> <li>cheatography</li> <li>guru99</li> <li>Keyboard shortcuts in Terminal on Mac</li> </ul> <p>Refactor the following portion later</p>"},{"location":"linux/links/#youtube","title":"youtube","text":""},{"location":"linux/links/#origin","title":"Origin","text":"<ul> <li>Why Linux Rules the Data Center</li> <li>Why so many distros? The Weird History of Linux</li> <li>The Secret OS That Really Runs The World</li> <li>The Making of GNU: The World's First Open-Source Software</li> <li>The Making of Linux: The World's First Open-Source Operating System</li> <li>Linux: The Origin Story</li> <li>The mind behind Linux | Linus Torvalds | TED</li> <li>Even Microsoft Uses Linux, So Why Don't We??</li> </ul>"},{"location":"linux/links/#introduction_1","title":"introduction","text":"<ul> <li>The 5 Things That Taught Me The Most About Linux</li> <li>5 Common Mistakes New Linux Users Often Make</li> <li>New Linux User: 10 Things I Wish I Knew When I First Started</li> <li>10 Typical Linux Problems and How to Fix Them</li> <li>Why Linux Is Better For Programming</li> <li>Why Linux is better for (most) developers!</li> <li>Why I Code on Linux Instead of Windows</li> <li>10 ways Linux is just better!</li> <li>10 Things I Wish I Knew When I First Started With Linux</li> <li>100+ Linux Things you Need to Know</li> <li>13 Linux Commands Every Engineer Should Know (Real-World Example)</li> </ul>"},{"location":"linux/links/#terminal-command","title":"Terminal command","text":"<ul> <li>Become a shell wizard in ~12 mins</li> <li>Unleash the Power of These 6 Linux Commands - Say Goodbye to Boring Terminal Screens</li> <li>Linux commands : Clear your Linux Basics in 25 min for beginners</li> <li>18 Commands That Will Change The Way You Use Linux Forever</li> <li>60 Linux Commands you NEED to know (in 10 minutes)</li> <li>50 Linux Commands Every Developer Must Know</li> <li>5 Command Line Tools That Boost Developer Productivity (2023)</li> </ul>"},{"location":"linux/links/#linux-command-line-tips-tricks","title":"Linux Command-Line Tips &amp; Tricks","text":"<ul> <li>Linux Command-Line Tips &amp; Tricks: Over 15 Examples!</li> <li>10 Linux Terminal Tips and Tricks to Enhance Your Workflow</li> </ul>"},{"location":"linux/links/#linux-filesystem","title":"Linux filesystem","text":"<ul> <li>How Does Linux Boot Process Work?</li> <li>Linux Directories Explained in 100 Seconds</li> <li>Linux File System/Structure Explained!</li> <li>The Linux Filesystem Explained | How Each Directory is Used</li> </ul>"},{"location":"linux/links/#linux-dns-server","title":"Linux DNS Server","text":"<ul> <li>Linux DNS Server</li> </ul>"},{"location":"linux/links/#sockets","title":"Sockets","text":"<ul> <li>Unix Domain Socket in 100 seconds</li> </ul>"},{"location":"linux/links/#different-linux-distributions","title":"different linux distributions","text":"<ul> <li>you need to learn Virtual Machines RIGHT NOW!! (Kali Linux VM, Ubuntu, Windows)<ul> <li>Virtual Machines Pt. 2 (Proxmox install w/ Kali Linux)</li> </ul> </li> <li>Switching to Linux</li> <li>Choosing the Right Linux Distro</li> <li>Kali Linux on Windows in 5min (WSL 2 GUI)</li> <li>The Linux Tier List</li> <li>Ranking Linux Distributions for 2023: not your average tier list!</li> <li> <p>You Only NEED 3 Linux Distributions</p> </li> <li> <p>series</p> </li> <li>Linux Essential Tools</li> <li>Linux Commands for Beginners</li> <li>Linux Crash Course</li> <li>Full Guides</li> <li>Top Docs | Learn the Basics</li> </ul>"},{"location":"linux/others/","title":"others","text":""},{"location":"linux/others/#other-resources","title":"Other resources","text":""},{"location":"linux/others/#table-of-contents","title":"Table of contents","text":""},{"location":"linux/others/#linux-vm-in-your-own-system","title":"Linux VM in your own system","text":""},{"location":"linux/others/#command-to-start-the-wsl-2-from-windows","title":"Command to start the wsl 2 from windows","text":"<pre><code>wsl --set-default-version 2\nsudo apt-get update &amp;&amp; sudo apt-get upgrade -y\nsudo apt install xrdp -y\nsudo service xrdp start\nip add\n</code></pre>"},{"location":"linux/others/#ubuntu-server-for-arm","title":"Ubuntu Server for ARM","text":""},{"location":"linux/others/#setup-ubuntu-server-in-utm","title":"Setup ubuntu server in UTM","text":"<pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\nsudo apt install ubuntu-desktop\nsudo reboot\n\n# for directory sharing between host os and guest os (127.0.0.1:9843)\nsudo apt install spice-vdagent spice-webdavd -y \n\n# https://docs.getutm.app/guest-support/linux/\nsudo mkdir -p /media/shared\nsudo mount -t 9p -o trans=virtio share /media/shared -oversion=9p2000.L\n# You can also modify `sudo vi /etc/fstab` and add the following line to automatically mount the share on startup\nshare   /media/shared   9p  trans=virtio,version=9p2000.L,rw,_netdev,nofail 0   0\nsudo chown -R $USER /media/shared\n</code></pre>"},{"location":"linux/others/#steps-for-creating-my-personalised-vm-in-mac","title":"Steps for creating my personalised VM in mac","text":"<ul> <li>Create the VM from ubuntu image [[linux]]</li> <li>Add git ssh keys</li> <li>Add vm user name host in your ~/.ssh/config</li> <li>Setup ssh agent on vscode<ul> <li>Linux VMs on an M1-based Mac with VScode and UTM</li> </ul> </li> <li>Download and set zsh</li> <li>Install luajit then install neovim by luajit</li> </ul>"},{"location":"linux/scripting/","title":"Scripting","text":""},{"location":"linux/scripting/#youtube","title":"Youtube","text":"<ul> <li>Bash in 100 Seconds</li> <li>Bash Scripting Tutorial for Beginners</li> <li>Bash Scripting on Linux</li> <li>MASTER BASH SCRIPTING FOR FREE: Automate Tasks, Land High-Paying Jobs &amp; Become a Linux Wizard</li> <li>you need to learn BASH Scripting RIGHT NOW!!</li> <li>Shell Scripting Tutorial for Beginners</li> </ul>"},{"location":"n8n/links/","title":"n8n","text":""},{"location":"n8n/links/#n8n","title":"n8n","text":""},{"location":"n8n/links/#youtube","title":"Youtube","text":"<ul> <li> <p>n8n Explained In 2 Minutes</p> </li> <li> <p>You NEED to Use n8n RIGHT NOW!! (Free, Local, Private)</p> </li> <li>How to Run n8n Locally (Full On-Premise Setup Tutorial)</li> </ul>"},{"location":"networking/introduction/","title":"introduction","text":""},{"location":"networking/introduction/#introduction","title":"Introduction","text":""},{"location":"networking/introduction/#prerequisites-of-ethical-hacking","title":"Prerequisites of Ethical Hacking","text":"<p>To become proficient in ethical hacking, you should have a solid foundation in the following areas:</p> <ul> <li>Networking: Gain a deep understanding of how data is transmitted across networks, including the intricacies of IP addressing, routing, switching, subnetting, and the various protocols that govern communication. This knowledge is essential for identifying potential vulnerabilities, understanding how attacks can spread, and effectively defending networked systems.</li> <li>Learn Linux: Most penetration testing tools and hacking environments are built for Linux. Mastering the Linux command-line interface, understanding the file system hierarchy, managing permissions, and writing shell scripts are crucial skills. This expertise enables you to automate tasks, customize tools, and operate efficiently in real-world scenarios.</li> <li>Learn Programming (Shell Scripting, Python, JavaScript): Programming knowledge empowers you to create custom scripts for automation, analyze and modify exploit code, and understand the logic behind attacks. Shell scripting is vital for automating repetitive tasks in Linux, Python is widely used for developing security tools and exploits, and JavaScript is key for understanding and testing web application vulnerabilities.</li> <li>Use Platforms like TryHackMe, HackTheBox, or OverTheWire: These online platforms offer interactive labs and real-world scenarios where you can practice ethical hacking in a legal, controlled environment. They provide challenges ranging from beginner to advanced, helping you build practical skills and stay updated with the latest techniques.</li> </ul>"},{"location":"networking/introduction/#theory","title":"Theory","text":""},{"location":"networking/introduction/#ip-address","title":"IP Address","text":"<p>An IP address is a unique numerical label assigned to each device on a network, enabling devices to locate and communicate with each other. There are two main types:</p> <ul> <li>Public IP Address: Assigned by your Internet Service Provider (ISP), this address identifies your network on the global internet. Public IPs are globally unique and allow devices to be accessed from anywhere on the internet, making them critical for hosting services.</li> <li>Private IP Address: Used within local networks (LANs), these addresses are not routable on the internet and are reserved for internal communication. Common ranges include <code>192.168.x.x</code>, <code>10.x.x.x</code>, and <code>172.16.x.x</code> to <code>172.31.x.x</code>. Devices with private IPs rely on NAT to access the internet.</li> </ul> <p>A LAN (Local Area Network) connects devices within a limited area, such as a home, office, or campus. NAT (Network Address Translation) allows multiple devices on a LAN to share a single public IP address for internet access, translating private IPs to the public IP and vice versa, thus conserving public IP addresses and adding a layer of security.</p>"},{"location":"networking/introduction/#osi-and-tcpip-models","title":"OSI and TCP/IP Models","text":""},{"location":"networking/introduction/#osi-model","title":"OSI Model","text":"<p>The OSI (Open Systems Interconnection) model is a seven-layer conceptual framework that standardizes the functions of a communication system. Each layer serves a specific purpose:</p> <ol> <li>Physical Layer: Handles the physical connection between devices, including cables, switches, and the transmission of raw binary data as electrical or optical signals.</li> <li>Data Link Layer: Manages direct node-to-node data transfer, error detection and correction, and organizes data into frames. It uses MAC addresses for local delivery.</li> <li>Network Layer: Responsible for logical addressing (IP addresses), routing packets across different networks, and handling fragmentation and reassembly of data.</li> <li>Transport Layer: Ensures reliable (TCP) or unreliable (UDP) data transfer, manages flow control, segmentation, and error recovery between end systems.</li> <li>Session Layer: Establishes, manages, and terminates sessions or connections between applications, ensuring continuous data exchange.</li> <li>Presentation Layer: Translates data between application and network formats, handles encryption, decryption, and data compression to ensure data is readable by the receiving system.</li> <li>Application Layer: Closest to the end user, it provides network services to applications, such as web browsers and email clients, using protocols like HTTP, FTP, and SMTP.</li> </ol>"},{"location":"networking/introduction/#tcpip-model","title":"TCP/IP Model","text":"<p>The TCP/IP model is a practical, four-layer model used in real-world networking:</p> <ol> <li>Link Layer: Combines the OSI's Physical and Data Link layers, handling hardware addressing, framing, and the physical transmission of data over the network medium.</li> <li>Internet Layer: Corresponds to the OSI's Network layer, responsible for logical addressing, routing, and forwarding packets using protocols like IP.</li> <li>Transport Layer: Manages end-to-end communication, reliability, sequencing, and flow control using protocols such as TCP and UDP.</li> <li>Application Layer: Encompasses the OSI's Session, Presentation, and Application layers, providing protocols and services for specific data communications like HTTP, FTP, and DNS.</li> </ol>"},{"location":"networking/introduction/#mac-address-vs-ip-address","title":"MAC Address vs. IP Address","text":"<ul> <li>MAC Address: A unique, permanent hardware identifier assigned to a network interface card (NIC) by the manufacturer. Operating at the Data Link Layer, MAC addresses are used for local network communication within a LAN. They are typically written in hexadecimal format (e.g., <code>00:1A:2B:3C:4D:5E</code>) and rarely change.</li> <li>IP Address: A logical, often dynamic address assigned to devices on a network, used for identifying and locating devices at the Network Layer. IP addresses can be static (manually assigned) or dynamic (assigned by DHCP), and are essential for routing data across networks (e.g., <code>192.168.1.1</code> for IPv4 or <code>2001:0db8::1</code> for IPv6).</li> </ul>"},{"location":"networking/introduction/#every-protocol-explained-as-quickly-as-possible","title":"Every Protocol Explained As QUICKLY As Possible!","text":""},{"location":"networking/introduction/#core-networking-protocols","title":"Core Networking Protocols","text":"<ul> <li>HTTP (HyperText Transfer Protocol): The primary protocol for transferring web pages and data on the internet. It defines how clients (browsers) and servers communicate, including request and response formats.</li> <li>HTTPS (HTTP Secure): An extension of HTTP that uses SSL/TLS encryption to secure data transmission, protecting sensitive information from interception and tampering.</li> <li>TCP (Transmission Control Protocol): Provides reliable, ordered, and error-checked delivery of data between applications. It establishes connections, ensures data integrity, and retransmits lost packets.</li> <li>IP (Internet Protocol): Handles addressing and routing of packets across networks, ensuring data reaches the correct destination.</li> <li>DNS (Domain Name System): Translates human-readable domain names (like <code>example.com</code>) into IP addresses. Attacks like DNS spoofing can redirect users to malicious sites by providing false DNS responses.</li> <li>DHCP (Dynamic Host Configuration Protocol): Automatically assigns IP addresses, subnet masks, gateways, and DNS settings to devices on a network, simplifying network management.</li> <li>NAT (Network Address Translation): Allows multiple devices on a private network to share a single public IP address, translating internal addresses to the public address for internet communication.</li> <li>VLAN (Virtual LAN): Segments a physical network into multiple logical networks, improving security, performance, and management by isolating traffic.</li> <li>ARP (Address Resolution Protocol): Resolves IP addresses to MAC addresses within a local network, enabling devices to communicate on the same LAN.</li> <li>ICMP (Internet Control Message Protocol): Used for network diagnostics and error reporting, such as with the <code>ping</code> command to test connectivity.</li> <li>VOIP (Voice Over IP): Enables voice communication and multimedia sessions over IP networks, replacing traditional phone systems.</li> <li>NFC (Near Field Communication): Facilitates short-range wireless communication between devices, commonly used for contactless payments and data exchange.</li> </ul>"},{"location":"networking/introduction/#networking-concepts","title":"Networking Concepts","text":"<ul> <li>VPN (Virtual Private Network): Establishes a secure, encrypted tunnel over an untrusted network (like the internet), protecting data privacy and enabling remote access to private networks.</li> <li>Routers: Devices that forward data packets between different networks, making routing decisions based on IP addresses and network topology.</li> <li>Switches: Network devices that connect multiple devices within a LAN, using MAC addresses to forward data only to the intended recipient, improving efficiency and security.</li> <li>Firewall: Hardware or software that monitors and controls incoming and outgoing network traffic based on predefined security rules, protecting networks from unauthorized access and attacks.</li> <li>OSI Model: The seven-layer model for understanding and designing network protocols and interactions, aiding in troubleshooting and network design.</li> <li>WAN (Wide Area Network): A network that spans large geographic areas, connecting multiple LANs and enabling communication over long distances, such as between cities or countries.</li> <li>MAC (Media Access Control): Refers to both the MAC address and the sublayer responsible for controlling how devices access the network medium and avoid collisions.</li> <li>LAN (Local Area Network): A network covering a small geographic area, such as a home, office, or building, enabling high-speed communication between connected devices.</li> <li>MPLS (Multiprotocol Label Switching): A technique for directing and managing data traffic efficiently across complex networks, improving speed and reliability.</li> <li>SD-WAN (Software-Defined Wide Area Network): Uses software to centrally manage and optimize WAN connections, enhancing performance, flexibility, and security.</li> <li>Proxy Server: Acts as an intermediary between clients and servers, providing services such as content filtering, caching, anonymity, and access control.</li> <li>QoS (Quality of Service): Mechanisms that prioritize certain types of network traffic (like voice or video) to ensure consistent performance and minimize latency or packet loss.</li> </ul>"},{"location":"networking/introduction/#cyber-security-and-hacking-protocols","title":"Cyber Security and Hacking Protocols","text":"<ul> <li>SSH (Secure Shell): Provides secure, encrypted remote login and command execution over unsecured networks, commonly used for server administration.</li> <li>Telnet: Allows remote login to devices but transmits data, including passwords, in plaintext, making it vulnerable to interception and generally considered insecure.</li> <li>SMB (Server Message Block): A protocol for sharing files, printers, and other resources between computers on a network, commonly used in Windows environments.</li> <li>FTP (File Transfer Protocol): Used for transferring files between client and server, but lacks encryption, exposing data to potential interception.</li> <li>SFTP (SSH File Transfer Protocol): A secure alternative to FTP, using SSH to encrypt file transfers and protect data in transit.</li> <li>RDP (Remote Desktop Protocol): Enables remote access to the graphical desktop of a Windows computer, allowing full control over the system from another location.</li> <li>SNMP (Simple Network Management Protocol): Used for monitoring, managing, and configuring network devices such as routers, switches, and servers.</li> </ul>"},{"location":"networking/introduction/#email-and-authentication-protocols","title":"Email and Authentication Protocols","text":"<ul> <li>SMTP (Simple Mail Transfer Protocol): The standard protocol for sending emails between servers and from clients to servers.</li> <li>IMAP (Internet Message Access Protocol): Allows clients to access, manage, and organize email messages stored on a mail server, supporting multiple devices and folders.</li> <li>POP3 (Post Office Protocol 3): Downloads emails from the server to the client and typically deletes them from the server, making messages accessible offline.</li> <li>LDAP (Lightweight Directory Access Protocol): Used for accessing and managing distributed directory information services, such as user authentication and organizational hierarchies.</li> </ul>"},{"location":"networking/introduction/#wireless-and-security-protocols","title":"Wireless and Security Protocols","text":"<ul> <li>WPA2 (Wi-Fi Protected Access 2): A widely used security protocol for wireless networks, employing AES encryption to protect data and prevent unauthorized access.</li> <li>WPA3: The latest Wi-Fi security standard, offering stronger encryption, improved protection against brute-force attacks, and enhanced security for public networks.</li> <li>IPSec (Internet Protocol Security): A suite of protocols for securing IP communications by authenticating and encrypting each IP packet, commonly used in VPNs.</li> <li>BGP (Border Gateway Protocol): The protocol used to exchange routing information between autonomous systems on the internet, playing a critical role in determining how data is routed globally.</li> </ul>"},{"location":"networking/introduction/#emerging-protocols","title":"Emerging Protocols","text":"<ul> <li>QUIC (Quick UDP Internet Connections): A modern transport layer protocol developed by Google, designed for faster, more secure web traffic by reducing latency and improving connection reliability.</li> <li>MQTT (Message Queuing Telemetry Transport): A lightweight messaging protocol optimized for small sensors and mobile devices, ideal for IoT applications due to its low bandwidth and high latency tolerance.</li> </ul>"},{"location":"networking/introduction/#inside-tcp-packets","title":"Inside TCP Packets","text":"<p>A TCP packet (segment) contains several fields that ensure reliable communication:</p> <ul> <li>Source Port &amp; Destination Port: Identify the sending and receiving applications, allowing multiple services to run on a single device.</li> <li>Sequence Number &amp; Acknowledgment Number: Track the order of data and confirm receipt, ensuring data is delivered accurately and in sequence.</li> <li>Flags: Control bits (such as SYN, ACK, FIN, RST, PSH, URG) that manage the state of the connection, including establishing, maintaining, and terminating sessions.</li> <li>Window Size: Specifies how much data can be sent before requiring an acknowledgment, enabling flow control and efficient data transfer.</li> <li>Checksum: Validates the integrity of the header and data, detecting errors during transmission.</li> <li>Urgent Pointer: Indicates if any part of the data is urgent and should be prioritized.</li> <li>Options (optional): Allow for additional parameters, such as specifying the maximum segment size or enabling advanced features.</li> <li>Data Payload: The actual application data being transmitted.</li> </ul> <p>TCP\u2019s mechanisms provide reliable, ordered, and error-checked delivery of data between applications, making it suitable for most internet communications.</p>"},{"location":"networking/introduction/#inside-udp-packets","title":"Inside UDP Packets","text":"<p>A UDP packet (datagram) is simpler and designed for speed:</p> <ul> <li>Source Port &amp; Destination Port: Identify the sending and receiving applications.</li> <li>Length: Specifies the total length of the UDP header and data.</li> <li>Checksum: Provides error-checking for the header and data, though it is optional in IPv4.</li> <li>Data Payload: The actual data being transmitted.</li> </ul> <p>UDP is connectionless and does not guarantee delivery, order, or error correction, making it faster and more efficient for applications like streaming and gaming, where speed is prioritized over reliability.</p>"},{"location":"networking/introduction/#inside-ip-packet","title":"Inside IP Packet","text":"<p>An IP packet contains the following fields:</p> <ul> <li>Version: Indicates whether the packet uses IPv4 or IPv6.</li> <li>Header Length: Specifies the size of the header.</li> <li>Type of Service: Defines the priority and handling instructions for the packet.</li> <li>Total Length: The entire size of the packet, including header and data.</li> <li>Identification, Flags, Fragment Offset: Used for fragmenting large packets and reassembling them at the destination.</li> <li>Time to Live (TTL): Limits the packet\u2019s lifespan, preventing it from circulating indefinitely in the network.</li> <li>Protocol: Specifies the protocol used in the data portion (e.g., TCP, UDP, ICMP).</li> <li>Header Checksum: Ensures the integrity of the header.</li> <li>Source IP Address: The sender\u2019s IP address.</li> <li>Destination IP Address: The recipient\u2019s IP address.</li> <li>Options (optional): Additional settings for advanced features.</li> <li>Data Payload: The encapsulated data, such as a TCP or UDP segment.</li> </ul>"},{"location":"networking/introduction/#hacking-tools","title":"Hacking Tools","text":"<ul> <li>Nmap: A powerful and versatile network scanner used to discover hosts, services, open ports, and vulnerabilities on a network. It supports advanced features like OS detection, scriptable interaction, and network mapping.</li> <li>Wireshark: A leading network protocol analyzer that captures and inspects packets in real time. It is invaluable for troubleshooting, analyzing network traffic, and investigating security incidents by providing deep visibility into network protocols.</li> <li>Metasploit: A comprehensive penetration testing framework that enables security professionals to find, exploit, and validate vulnerabilities. It includes a vast library of exploits, payloads, and auxiliary modules for automating attacks and testing defenses.</li> <li>Burp Suite: An integrated platform for web application security testing, offering tools for scanning, crawling, intercepting, and manipulating HTTP/S traffic to identify and exploit vulnerabilities in web applications.</li> <li>SQL Injection: Not a tool, but a technique used to exploit vulnerabilities in web applications by injecting malicious SQL queries. This can allow attackers to manipulate databases, extract sensitive data, or bypass authentication mechanisms.</li> </ul>"},{"location":"networking/links/","title":"links","text":""},{"location":"networking/links/#computer-networking-basics","title":"computer networking basics","text":""},{"location":"networking/links/#books","title":"Books","text":""},{"location":"networking/links/#researches","title":"Researches","text":"<ul> <li>Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google's Datacenter Network</li> </ul>"},{"location":"networking/links/#youtube","title":"Youtube","text":""},{"location":"networking/links/#introduction","title":"Introduction","text":"<ul> <li>Every Protocol Explained As QUICKLY As Possible!</li> <li>Every Networking Concept Explained In 8 Minutes</li> <li>Networking For Hackers! (Common Network Protocols)</li> <li> <p>OSI and TCP IP Models - Best Explanation</p> </li> <li> <p>Google's 1.3Pb/s \"Jupiter\" Network | System Design</p> </li> </ul>"},{"location":"networking/links/#playlists","title":"Playlists","text":"<ul> <li>General Security Education</li> <li>Computer Networking Full Course - OSI Model Deep Dive with Real Life Examples</li> <li>Computer Networking Course - Network Engineering - CompTIA Network+ Exam Prep</li> <li>Computer Networks (Complete Playlist)</li> <li>Computer Networking Tutorial - Bits and Bytes of the Networking</li> <li>Sunny Classroom</li> </ul>"},{"location":"networking/links/#udemy","title":"Udemy","text":""},{"location":"networking/links/#introduction_1","title":"Introduction","text":"<ul> <li>Cisco - TCP/IP &amp; OSI Network Architecture Models 1.5 hr</li> <li>Subnetting Made Easy 1hr</li> <li>IT Networking Fundamentals For Complete Beginners 3 hr</li> <li>Complete Introduction to Cybersecurity 2024 2.5 hr</li> <li>IP Addressing and Subnetting - Hands-on Learning Approach 4 hr</li> <li>Introduction to Computer Networking - 2 Hour Crash Course 4.5 hr</li> <li>Computer Network: Networking fundamentals + Wireshark Basics 3.5 + 2.5 hr</li> </ul>"},{"location":"networking/links/#intermediate","title":"Intermediate","text":"<ul> <li>OSPF for the Real World - From Zero to Hero 6.5 hr</li> <li>Cyber Security: From Beginner to Expert (2024) 7 hr</li> <li>Introduction to Networking 7.5 hr</li> <li>Introduction to Computer Networks for Non-Techies 18.5 hr</li> <li>Cisco BGP Masterclass for Enterprise Network Engineers 16.5 hr</li> </ul>"},{"location":"networking/links/#ccna-200-301","title":"CCNA 200-301","text":"<ul> <li>Cisco CCNA 200-301 - The Complete Guide to Getting Certified 40 hr</li> <li>The Complete Networking Fundamentals Course. Your CCNA start 79.5 hr</li> </ul>"},{"location":"observibility/introduction/","title":"introduction","text":""},{"location":"observibility/introduction/#stacks","title":"Stacks","text":"<ul> <li>LGTM: Loki, Grafana, Tempo, Mimir</li> </ul> <p>OTLP -&gt; Open telemetry protocol Collectors -&gt; Open telemtery collectors, Grafana Alloy Instrumentation -&gt; Automatic, custom, library</p> <p>Theory of OpenTelemtry Signal -&gt; Metrics, Logs, Traces Context -&gt; Semantic Conventions -&gt;</p> <p>Prometheus is a database for metrics Graphana Mimir is also for metrics for it is scalable</p> <p>Loki is a optimized Logs database</p> <p>Tempo is for tacing database.</p> <p>Continuous Profiling is information about how much computing power/resources are used by an application collected in a interval Pyroscope is a profiling database</p>"},{"location":"observibility/links/","title":"links","text":""},{"location":"observibility/links/#opentelemetry-and-observibility","title":"Opentelemetry and Observibility","text":""},{"location":"observibility/links/#official-documentation","title":"Official documentation","text":"<ul> <li>Documentation</li> <li>OpenTelemetry Demo Docs<ul> <li>open-telemetry/opentelemetry-demo</li> </ul> </li> </ul>"},{"location":"observibility/links/#youtube","title":"Youtube","text":""},{"location":"observibility/links/#single-videos","title":"Single videos","text":"<ul> <li>What is OpenTelemetry?</li> <li>What is OpenTelemetry? - Explanation and Demo</li> <li>OpenTelemetry Collector: EVERYTHING you need to know [to get started]</li> <li>Monitoring Made EASY with Grafana and Prometheus!</li> <li>Creating Grafana Dashboards for Prometheus: A Beginner's Guide</li> <li>Grafana is the goat... Let's deploy the LGTM stack</li> <li>Beautiful Dashboards with Grafana and Prometheus - Monitoring Kubernetes Tutorial</li> <li>How Prometheus Monitoring Works | Explaining Prometheus Architecture | KodeKloud</li> </ul>"},{"location":"observibility/links/#tutorials","title":"Tutorials","text":"<ul> <li> <p>Open Source Observability Explained - The Grafana Stack</p> </li> <li> <p>Learn Observability in 5 hours | Tool wise Demo + Complete Demo using Open Telemetry</p> <ul> <li>iam-veeramalla/observability-zero-to-hero</li> </ul> </li> <li> <p>Server Monitoring with Grafana Prometheus and Loki</p> </li> <li> <p>OpenTelemetry Course - Understand Software Performance</p> <ul> <li>Instrumenting Your Node.js Apps with OpenTelemetry</li> <li>kubowania/opentelemetry-tracing</li> <li>kubowania/opentelemetry-movies-microservices</li> <li>newrelic/newrelic-opentelemetry-examples</li> </ul> </li> <li> <p>Observability Devops Project - Opentelemetry Demo Application</p> <ul> <li>Otel Astronomy Shop Demo App</li> <li>\ud835\udde2\ud835\udde7\ud835\uddf2\ud835\uddf9 \ud835\uddd4\ud835\uddfd\ud835\uddfd \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf7\ud835\uddf2\ud835\uddf0\ud835\ude01 \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd \ud835\uddef\ud835\ude06 \ud835\ude00\ud835\ude01\ud835\uddf2\ud835\uddfd \ud835\uddd7\ud835\uddfc\ud835\uddf0\ud835\ude02\ud835\uddfa\ud835\uddf2\ud835\uddfb\ud835\ude01</li> </ul> </li> <li> <p>Prometheus Fundamentals</p> </li> <li>Prometheus</li> <li> <p>Prometheus Monitoring Full Tutorial</p> </li> <li> <p>OpenTelemetry</p> </li> <li>Observability</li> </ul>"},{"location":"observibility/links/#udemy","title":"Udemy","text":"<ul> <li>Prometheus | The Complete Hands-On for Monitoring &amp; Alerting</li> <li>Grafana</li> <li>Mastering Prometheus and Grafana (Including Loki &amp; Alloy)</li> <li> <p>Observability with Grafana, Prometheus, Loki, Alloy and Tempo</p> </li> <li> <p>OpenTelemetry Foundations: Hands-On Guide to Observability</p> </li> <li>Observability in Cloud Native apps using OpenTelemetry</li> </ul>"},{"location":"observibility/links/#blogs","title":"Blogs","text":"<ul> <li>Golang, OpenTelemetry, Jaeger, ElasticSearch and Kubernetes setup for application traces</li> <li>Everything You Should Know About OpenTelemetry Collector Contrib</li> </ul>"},{"location":"observibility/links/#medium","title":"Medium","text":"<ul> <li>The Observability in Distributed Systems</li> <li>Observability with OpenTelemetry and Go</li> <li>Observability: Logs vs Traces vs Metrics!</li> <li> <p>Introduction to Observability using OpenTelemetry in Golang</p> </li> <li> <p>Golang Metrics with Prometheus and Grafana</p> </li> <li>Deploying OpenTelemetry Collector, Jaeger, and Prometheus with Docker Compose for Observability</li> <li>Monitor Your Go Process Internal Metrics in Minutes</li> <li>Implementing the logging stack using Promtail, Loki, and Grafana using Docker-Compose</li> <li>Behind the Scenes: A Deep Dive into Distributed Tracing with Grafana, Tempo, and Jaeger</li> <li>Introduction to Tracing in Go with Jaeger &amp; OpenTelemetry</li> <li>HTTP Tracing with Spring Boot, Zipkin and Open Telemetry</li> <li>Monitoring Spring Boot Microservices (Prometheus, Grafana &amp; Zipkin)</li> <li>Monitoring the Golang App with Prometheus, Grafana, New Relic and Sentry</li> <li>A Guide to Deploying Jaeger on Kubernetes in Production</li> <li>Introducing native support for OpenTelemetry in Jaeger</li> </ul>"},{"location":"observibility/prometheus/","title":"Prometheus, Grafana, and Go API Monitoring Stack","text":""},{"location":"observibility/prometheus/#prometheus-grafana-and-go-api-monitoring-stack","title":"Prometheus, Grafana, and Go API Monitoring Stack","text":"<p>This project provides a complete, containerized environment for monitoring a simple Go REST API using Prometheus for metric collection and Grafana for visualization. It also includes a Python-based load tester to simulate traffic and generate metrics.</p>"},{"location":"observibility/prometheus/#components","title":"Components","text":""},{"location":"observibility/prometheus/#1-go-rest-api-go-rest-api","title":"1. Go REST API (<code>go-rest-api</code>)","text":"<p>A simple Go application with two HTTP endpoints: <code>/hello</code> and <code>/delayed_hello</code>. It is instrumented with Prometheus client libraries to expose custom metrics.</p> <p>Metrics Used:</p> <ul> <li> <p><code>go_api_http_requests_total</code>:     A Counter metric that increments each time an HTTP request is received. Useful for tracking the total number of requests served.</p> </li> <li> <p><code>go_api_active_connections</code>:     A Gauge metric representing the number of currently active HTTP connections. Ideal for measuring current connections, memory usage, or queue size.</p> </li> <li> <p><code>go_api_http_duration_seconds</code>:     A Summary metric measuring the duration of each HTTP request. Useful for calculating quantiles (e.g., p99, p95) and understanding request latency.</p> </li> </ul>"},{"location":"observibility/prometheus/#2-prometheus","title":"2. Prometheus","text":"<p>Prometheus is an open-source monitoring system that collects and stores metrics as time-series data. It works on a \"pull\" model, scraping metrics from configured targets (our Go API) at regular intervals.</p> <ul> <li><code>prometheus.yml</code>:     The configuration file for Prometheus. It tells Prometheus where to find our Go API's <code>/metrics</code> endpoint and how often to scrape it.</li> </ul>"},{"location":"observibility/prometheus/#3-grafana","title":"3. Grafana","text":"<p>Grafana is an open-source data visualization and analytics platform. It connects to various data sources (like Prometheus) and allows you to create customizable dashboards.</p> <ul> <li><code>grafana-dashboard.json</code>:     A pre-configured Grafana dashboard for our Go API metrics. Import this file into Grafana to get started quickly.</li> </ul>"},{"location":"observibility/prometheus/#4-load-tester","title":"4. Load Tester","text":"<p>A simple Python script using the <code>requests</code> library to simulate traffic to our Go API. It makes concurrent requests to both the <code>/hello</code> and <code>/delayed_hello</code> endpoints to generate realistic load and populate the metrics in Prometheus.</p>"},{"location":"observibility/prometheus/#getting-started","title":"Getting Started","text":"<p>Follow these steps to set up and run the entire monitoring stack.</p>"},{"location":"observibility/prometheus/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker and Docker Compose installed on your machine.</li> </ul>"},{"location":"observibility/prometheus/#step-1-set-up-the-go-api","title":"Step 1: Set up the Go API","text":"<p>Navigate to the <code>go-rest-api</code> directory and initialize the Go module:</p> <pre><code>cd go-rest-api\ngo mod init go-rest-api\ngo mod tidy\n</code></pre>"},{"location":"observibility/prometheus/#step-2-run-the-docker-compose-stack","title":"Step 2: Run the Docker Compose Stack","text":"<p>From the root directory of the project, run:</p> <pre><code>docker-compose up -d --build\n</code></pre> <p>This command will:</p> <ul> <li>Build the Go API and Load Tester Docker images.</li> <li>Start Prometheus, Grafana, and both custom services.</li> <li>The load tester will immediately start sending requests to the Go API.</li> </ul>"},{"location":"observibility/prometheus/#step-3-access-the-services","title":"Step 3: Access the Services","text":"<ul> <li>Go API: http://localhost:8080</li> <li>Prometheus UI: http://localhost:9090     Check the \"Targets\" page to confirm that the go-api service is being scraped.</li> <li>Grafana UI: http://localhost:3000     Default login: <code>admin</code> / <code>admin</code> (you will be prompted to change the password on first login).</li> </ul>"},{"location":"observibility/prometheus/#step-4-import-the-grafana-dashboard","title":"Step 4: Import the Grafana Dashboard","text":"<ol> <li>In Grafana, click the Dashboards icon on the left.</li> <li>Click New Dashboard \u2192 Import.</li> <li>Copy the entire content of the <code>grafana-dashboard.json</code> file and paste it into the \"Import via panel json\" text area.</li> <li>Click Load.</li> <li>On the next screen, select Prometheus from the dropdown menu.</li> <li>Click Import.</li> </ol> <p>You should now see a dashboard displaying your Go API metrics, populated by the running load tester.</p>"},{"location":"observibility/prometheus/#go-api-code-breakdown","title":"Go API Code Breakdown","text":"<p>This section explains the Go code in <code>main.go</code> to help you understand how the metrics are instrumented.</p> <pre><code>package main\n\nimport (\n    \"log\"\n    \"math/rand\"\n    \"net/http\"\n    \"time\"\n\n    \"github.com/prometheus/client_golang/prometheus\"\n    \"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\nvar (\n    httpRequestsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"go_api_http_requests_total\",\n            Help: \"Total number of HTTP requests.\",\n        },\n        []string{\"endpoint\"},\n    )\n\n    httpRequestDuration = prometheus.NewSummaryVec(\n        prometheus.SummaryOpts{\n            Name:       \"go_api_http_duration_seconds\",\n            Help:       \"HTTP request duration in seconds.\",\n            Objectives: map[float64]float64{0.5: 0.05, 0.9: 0.01, 0.99: 0.001},\n        },\n        []string{\"endpoint\"},\n    )\n\n    activeConnections = prometheus.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"go_api_active_connections\",\n            Help: \"Number of active HTTP connections.\",\n        },\n    )\n)\n\nfunc init() {\n    prometheus.MustRegister(httpRequestsTotal)\n    prometheus.MustRegister(httpRequestDuration)\n    prometheus.MustRegister(activeConnections)\n}\n\nfunc promMiddleware(next http.Handler, endpoint string) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        start := time.Now()\n        activeConnections.Inc()\n        httpRequestsTotal.WithLabelValues(endpoint).Inc()\n        log.Printf(\"Received request for endpoint %s\", endpoint)\n\n        next.ServeHTTP(w, r)\n\n        duration := time.Since(start).Seconds()\n        httpRequestDuration.WithLabelValues(endpoint).Observe(duration)\n        activeConnections.Dec()\n    })\n}\n\nfunc helloHandler(w http.ResponseWriter, r *http.Request) {\n    w.WriteHeader(http.StatusOK)\n    w.Write([]byte(\"Hello, world!\"))\n}\n\nfunc delayedHelloHandler(w http.ResponseWriter, r *http.Request) {\n    delay := time.Duration(rand.Intn(1500)+500) * time.Millisecond\n    time.Sleep(delay)\n\n    w.WriteHeader(http.StatusOK)\n    w.Write([]byte(\"Delayed hello!\"))\n}\n\nfunc main() {\n    http.Handle(\"/hello\", promMiddleware(http.HandlerFunc(helloHandler), \"hello\"))\n    http.Handle(\"/delayed_hello\", promMiddleware(http.HandlerFunc(delayedHelloHandler), \"delayed_hello\"))\n    http.Handle(\"/metrics\", promhttp.Handler())\n\n    log.Println(\"Go API server started on :8080\")\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        log.Fatalf(\"could not start server: %v\\n\", err)\n    }\n}\n</code></pre>"},{"location":"observibility/prometheus/#code-explanation","title":"Code Explanation","text":"<ul> <li><code>package main</code>: Declares the package as main, making it an executable program.</li> <li>Imports: Brings in packages for logging, random number generation, time, HTTP handling, and Prometheus client library.</li> <li>Metric Variables:  <ul> <li><code>httpRequestsTotal</code>: CounterVec for total HTTP requests, labeled by endpoint.</li> <li><code>httpRequestDuration</code>: SummaryVec for request durations, labeled by endpoint.</li> <li><code>activeConnections</code>: Gauge for current active HTTP connections.</li> </ul> </li> <li><code>init()</code>: Registers metrics with Prometheus.</li> <li><code>promMiddleware</code>: Middleware to instrument handlers with metrics.</li> <li>Handlers:  <ul> <li><code>helloHandler</code>: Returns a simple string.</li> <li><code>delayedHelloHandler</code>: Returns a string after a random delay (500ms-2000ms).</li> </ul> </li> <li><code>main()</code>: Registers handlers and starts the HTTP server on port 8080.</li> </ul> <p>You now have a full monitoring stack for a Go API, complete with metrics collection, visualization, and load testing.</p>"},{"location":"os/links/","title":"links","text":""},{"location":"os/links/#operating-system","title":"Operating System","text":""},{"location":"os/links/#youtube","title":"Youtube","text":"<ul> <li>Complete Operating System in one shot | Semester Exam | Hindi</li> <li>Operating Systems for Placements 2022</li> <li>Operating System (Complete Playlist)</li> <li>Complete Operating Systems in 1 Shot (With Notes) || For Placement Interviews</li> </ul>"},{"location":"os/links/#proxmox","title":"Proxmox","text":"<ul> <li>Proxmox Full Course</li> </ul>"},{"location":"terraform/concepts/","title":"Your first terraform script","text":""},{"location":"terraform/concepts/#your-first-terraform-script","title":"Your first terraform script","text":""},{"location":"terraform/concepts/#installation-guide","title":"Installation Guide","text":"<p>For detailed step-by-step instructions and platform-specific installation methods, please visit the official Terraform Installation Guide.</p>"},{"location":"terraform/concepts/#important-commands","title":"Important Commands","text":""},{"location":"terraform/concepts/#verify-installation","title":"Verify Installation","text":"<pre><code>terraform -v\n\nterraform -help\n\nterraform -help plan\n</code></pre>"},{"location":"terraform/concepts/#enable-tab-completion","title":"Enable tab completion","text":"<p>If you use either Bash or Zsh, you can enable tab completion for Terraform commands. To enable autocomplete, first ensure that a config file exists for your chosen shell. </p><pre><code>touch ~/.bashrc\ntouch ~/.zshrc\n</code></pre> <p>Then install the autocomplete package. </p><pre><code>terraform -install-autocomplete\n</code></pre>"},{"location":"terraform/concepts/#quick-start-tutorial","title":"Quick start tutorial","text":"<p>Now that you've installed Terraform, you can provision an NGINX server in less than a minute using Docker on Mac, Windows, or Linux. You can also follow the rest of this tutorial in your web browser.</p> <p>Create a directory named <code>learn-terraform-docker-container</code>.  This working directory houses the configuration files that you write to describe the infrastructure you want Terraform to create and manage. When you initialize and apply the configuration here, Terraform uses this directory to store required plugins, modules (pre-written configurations), and information about the real infrastructure it created.  In the working directory, create a file called <code>main.tf</code> and paste the following Terraform configuration into it.</p> <p>for MAC and linux </p><pre><code>terraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"~&gt; 3.0.1\"\n    }\n  }\n}\n\nprovider \"docker\" {}\n\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx\"\n  keep_locally = false\n}\n\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\n</code></pre> <p>for Windows </p><pre><code>terraform {\n  required_providers {\n    docker = {\n      source  = \"kreuzwerker/docker\"\n      version = \"~&gt; 3.0.1\"\n    }\n  }\n}\n\nprovider \"docker\" {\n  host    = \"npipe:////.//pipe//docker_engine\"\n}\n\nresource \"docker_image\" \"nginx\" {\n  name         = \"nginx\"\n  keep_locally = false\n}\n\nresource \"docker_container\" \"nginx\" {\n  image = docker_image.nginx.image_id\n  name  = \"tutorial\"\n\n  ports {\n    internal = 80\n    external = 8000\n  }\n}\n</code></pre> <p>Initialize the project, which downloads a plugin called a provider that lets Terraform interact with Docker. </p><pre><code>terraform init\n</code></pre> <p>Provision the <code>NGINX</code> server container with <code>apply</code>. When Terraform asks you to confirm type yes and press ENTER. </p><pre><code>terraform apply\n</code></pre> <p>Verify the existence of the NGINX container by visiting <code>localhost:8000</code> in your web browser or running <code>docker ps</code> to see the container.</p> <p>To stop the container, run <code>terraform destroy</code>.</p>"},{"location":"terraform/concepts/#working-with-aws","title":"Working with AWS","text":"<p>We will provision an <code>EC2</code> instance on <code>Amazon Web Services (AWS)</code>. <code>EC2</code> instances are virtual machines running on <code>AWS</code>, and a common component of many infrastructure projects.</p>"},{"location":"terraform/concepts/#prerequisites","title":"Prerequisites","text":"<p>To follow this tutorial you will need</p> <ul> <li>The Terraform CLI (1.2.0+) installed.</li> <li>The AWS CLI installed.</li> <li>AWS account and associated credentials that allow you to create resources.</li> </ul> <p>To use your IAM credentials to authenticate the Terraform AWS provider, set the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variable. </p><pre><code>export AWS_ACCESS_KEY_ID=\nexport AWS_SECRET_ACCESS_KEY=\n</code></pre> <p>Tip : If you don't have access to IAM user credentials, use another authentication method described in the AWS provider documentation.</p>"},{"location":"terraform/concepts/#build-infrastructure","title":"Build Infrastructure","text":""},{"location":"terraform/concepts/#write-configuration","title":"Write configuration","text":"<p>Create a directory for your configuration. Configuration files must be in this directory for Terraform to run them.</p> <pre><code>mkdir learn-terraform-aws-instance\ncd learn-terraform-aws-instance\n</code></pre> <p>Create a file named <code>main.tf</code> and paste the following configuration into it.</p> <pre><code>terraform {\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 4.16\"\n    }\n  }\n\n  required_version = \"&gt;= 1.2.0\"\n}\n\nprovider \"aws\" {\n  region  = \"ap-south-1\"\n}\n\nresource \"aws_instance\" \"app_server\" {\n  ami           = \"ami-830c94e3\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"ExampleAppServerInstance\"\n  }\n}\n</code></pre>"},{"location":"terraform/concepts/#terraform-block","title":"Terraform Block","text":"<p>The <code>terraform {}</code> block contains Terraform settings, including the required providers Terraform will use to provision your infrastructure. For each provider, the source attribute defines an optional hostname, a namespace, and the provider type. Terraform installs providers from the Terraform Registry by default. In this example configuration, the aws provider's source is defined as <code>hashicorp/aws</code>, which is shorthand for <code>registry.terraform.io/hashicorp/aws</code>.</p> <p>You can also set a version constraint for each provider defined in the required_providers block. The version attribute is optional, but we recommend using it to constrain the provider version so that Terraform does not install a version of the provider that does not work with your configuration. If you do not specify a provider version, Terraform will automatically download the most recent version during initialization.</p> <p>To learn more, reference the provider source documentation.</p>"},{"location":"terraform/concepts/#initialize-the-directory","title":"Initialize the directory","text":"<p>Initialize the configuration directory. This step downloads the necessary provider plugins for the configuration.</p> <pre><code>terraform init\n</code></pre> <p>checks whether your Terraform configuration files are syntactically valid and internally consistent. It does not check with the cloud provider or create any resources. It helps you catch errors or typos in your .tf files before running terraform plan or terraform apply. </p><pre><code>terraform validate\n</code></pre> <pre><code>terraform fmt\n</code></pre> <pre><code>terraform plan\n\n## We can additionally give variable value\nterraform plan -var-file=\"dev.tfvars\"\n</code></pre>"},{"location":"terraform/concepts/#apply-configuration","title":"Apply configuration","text":"<p>Apply the configuration to create your instance.</p> <p></p><pre><code>terraform apply\n\n## We can additionally give variable value\nterraform apply -var-file=\"dev.tfvars\"\n\nterraform apply --auto-approve -var-file=\"dev.tfvars\"\n</code></pre> When prompted to confirm, type <code>yes</code> and press ENTER."},{"location":"terraform/concepts/#verify-the-instance","title":"Verify the instance","text":"<p>Verify that your instance is running.</p> <pre><code>aws ec2 describe-instances --filters \"Name=instance-state-name,Values=running\"\n</code></pre>"},{"location":"terraform/concepts/#clean-up-your-infrastructure","title":"Clean up your infrastructure","text":"<p>Destroy the instance to avoid ongoing charges.</p> <pre><code>terraform destroy\n\n## We can additionally give variable value\nterraform destroy -var-file=\"dev.tfvars\"\n\n## \nterraform destroy --auto-approve -var-file=\"dev.tfvars\"\n</code></pre> <p>When prompted to confirm, type <code>yes</code> and press ENTER.</p> <pre><code>terraform show\n\n# terraform state show in json format\nterraform show -json\n</code></pre> <pre><code>terraform providers\n\nterraform providers mirror &lt;another-directory&gt;\n</code></pre> <pre><code>terraform output\n\nterraform output &lt;variable-name&gt;\n</code></pre> <pre><code>terraform apply --refresh-only\n</code></pre> <p>Normally, <code>terraform plan</code> checks the actual resources in your cloud provider to update its state file before showing what changes it will make. With <code>--refresh=false</code>, it skips this check and uses only the existing state file, which can make the plan faster but may not reflect the current real-world infrastructure. This is useful if you want to avoid unnecessary API calls or if you know the state hasn't changed outside of Terraform. </p><pre><code>terraform plan --refresh=false\n</code></pre> <pre><code># show in .svg format\nterraform graph\n\nterraform graph | dot -Tsvg &gt; graph.svg\n</code></pre>"},{"location":"terraform/concepts/#lifecycle-rules","title":"Lifecycle rules","text":"<p>create_before_destroy</p> <p>prevent_destroy</p> <p>ignore_changes</p>"},{"location":"terraform/harshicorp-vault/","title":"Harshicorp vault","text":""},{"location":"terraform/harshicorp-vault/#youtube","title":"Youtube","text":"<ul> <li>HashiCorp Vault Explained in 180 seconds</li> <li>HashiCorp Vault Tutorial for Beginners - What, Why and How</li> <li>HashiCorp Vault + Terraform: The Ultimate Secrets Management Guide</li> <li>HashiCorp Vault</li> <li>HashiCorp Vault Tutorial for Beginners | FULL COURSE in 1 Hour | HashiCorp Vault Fundamentals</li> </ul>"},{"location":"terraform/introduction/","title":"introduction","text":""},{"location":"terraform/introduction/#infrastructure-as-code-iac-with-terraform","title":"Infrastructure as Code (IaC) with Terraform","text":"<p>A simple crash course to start an EC2 instance, update it, and then destroy it.</p>"},{"location":"terraform/introduction/#introduction","title":"Introduction","text":"<ul> <li>Infrastructure as Code (IaC) tools manage infrastructure using configuration files instead of GUIs.</li> <li>Enable building, changing, and managing infrastructure in a safe, consistent, and repeatable manner.</li> <li>Allow defining resource configurations that can be versioned, reused, and shared.</li> </ul>"},{"location":"terraform/introduction/#why-terraform","title":"Why Terraform?","text":"<ul> <li>Multi-Platform Management: Manage infrastructure across multiple cloud platforms.</li> <li>Human-Readable Configuration Language: Quickly write infrastructure code.</li> <li>State Management: Track resource changes throughout deployments.</li> <li>Version Control Integration: Collaborate safely on infrastructure through version control systems.</li> </ul>"},{"location":"terraform/introduction/#terraform-providers","title":"Terraform Providers","text":"<ul> <li>Providers: Plugins that interact with cloud platforms and services via APIs.</li> <li>Over 1,000 providers are available for AWS, Azure, GCP, Kubernetes, GitHub, Splunk, DataDog, and more.</li> <li>Providers can be found in the Terraform Registry.</li> <li>If a specific provider is not available, custom providers can be created.</li> </ul>"},{"location":"terraform/introduction/#key-concepts","title":"Key Concepts","text":"<ul> <li>Resources: Individual units of infrastructure, like compute instances or networks.</li> <li>Modules: Reusable Terraform configurations composed of resources from different providers.</li> <li>Declarative Configuration Language: Describe the desired end-state of infrastructure.</li> <li>Automatic Dependency Management: Calculate dependencies to create/destroy resources in the correct order.</li> </ul>"},{"location":"terraform/introduction/#terraform-workflow","title":"Terraform Workflow","text":"<ol> <li>Scope: Identify the required infrastructure for your project.</li> <li>Author: Write the configuration for your infrastructure.</li> <li>Initialize: Install necessary plugins for infrastructure management.</li> <li>Plan: Preview changes Terraform will make.</li> <li>Apply: Execute changes to match the desired infrastructure state.</li> </ol>"},{"location":"terraform/introduction/#tracking-your-infrastructure","title":"Tracking Your Infrastructure","text":"<ul> <li>State File: Acts as the source of truth for your environment.</li> <li>Terraform uses the state file to determine changes needed to match your configuration.</li> </ul>"},{"location":"terraform/introduction/#collaboration-with-terraform","title":"Collaboration with Terraform","text":"<ul> <li>Remote State Backends: Securely share your state with teammates.</li> <li>Prevent race conditions when multiple people make changes simultaneously.</li> <li>Connect HCP Terraform to version control systems (VCSs) like GitHub, GitLab, etc.</li> <li>Automatically propose infrastructure changes when committing configuration changes to VCS.</li> <li>Manage infrastructure changes through version control, similar to application code.</li> </ul>"},{"location":"terraform/introduction/#example-code","title":"Example Code","text":"<pre><code># Configure the AWS Provider\nprovider \"aws\" {\n  region = \"us-west-2\"\n}\n\n# Create an EC2 Instance\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-0c55b159cbfafe1f0\"\n  instance_type = \"t2.micro\"\n\n  tags = {\n    Name = \"example-instance\"\n  }\n}\n</code></pre>"},{"location":"terraform/links/","title":"links","text":""},{"location":"terraform/links/#terraform","title":"Terraform","text":""},{"location":"terraform/links/#official-documentation","title":"Official documentation","text":"<ul> <li>Developer Terraform Tutorials</li> <li>Configuring OpenID Connect in Amazon Web Services</li> </ul>"},{"location":"terraform/links/#youtube","title":"Youtube","text":""},{"location":"terraform/links/#single-videos","title":"Single videos","text":"<ul> <li>Why You NEED To Learn Terraform | Practical Tutorial</li> </ul>"},{"location":"terraform/links/#playlists","title":"Playlists","text":""},{"location":"terraform/links/#personal","title":"Personal","text":"<ul> <li>terraform personal</li> </ul>"},{"location":"terraform/links/#anton-putra","title":"Anton Putra","text":"<ul> <li>Terraform Tutorials for Beginners</li> <li>antonputra/tutorials/tree/main/lessons/198</li> <li>Terraform Tutorial on AWS [Full Course]</li> <li>AWS Tutorials</li> <li>Terraform AWS VPC Tutorial - Public, Private, and Isolated Subnets</li> <li>AWS EKS Kubernetes Tutorial [Full Course]</li> <li>AWS EKS Kubernetes Tutorial</li> </ul>"},{"location":"terraform/links/#abhishek-veeramalla","title":"Abhishek Veeramalla","text":"<ul> <li>Terraform Zero to Hero</li> <li>Terraform Zero to Hero course</li> <li>iam-veeramalla/terraform-eks</li> <li>aws-devops-zero-to-hero/day-24</li> </ul>"},{"location":"terraform/links/#terraform-with-github-action-and-github-oidc-iam-role","title":"Terraform with github action and github oidc iam role","text":"<ul> <li>Securely deploy to AWS with GitHub Actions and OIDC</li> <li>GitHub Universe 2023 OIDC Demo</li> <li>Configuring OpenID Connect in Amazon Web Services</li> <li>How to Authenticate GitHub with AWS? | AWS IAM Role For GitHub | Terraform</li> <li>How to connect GitHub workflows and AWS with OICD (OpenID Connect)</li> <li>AWS Hands-On: Automate AWS Infra Deployment using Terraform and GitHub Actions</li> <li>Deploy AWS infrastructure using github actions and terraform</li> <li>gitmurali/github-actions-terraform</li> <li>Deploy to AWS with Terraform within a GitHub Action</li> <li>CloudSecurity.dev</li> <li>KasteM34/github-oidc-terraform</li> <li>KasteM34/github-oidc-terraform/blob/main/github-action.yml</li> <li>KasteM34/github-oidc-terraform/blob/main/policies/role-trusted-entity.json</li> <li>KasteM34/github-oidc-terraform/blob/main/policies/s3-bucket-state.json</li> <li>GitHub Actions Pipeline for AWS with Terraform | GitHub Actions API Trigger</li> </ul>"},{"location":"terraform/links/#blogs","title":"Blogs","text":"<ul> <li>How to Create API Gateway Using Terraform &amp; AWS Lambda</li> <li>sumeetninawe/tf-lambda-apig</li> </ul>"},{"location":"terraform/links/#medium","title":"Medium","text":"<ul> <li>AWS API Gateway &amp; Lambda with Terraform</li> <li>Securing Architectures with AWS Lambda, S3, and Terraform: Best Practices and Strategies</li> <li> <p>Automate Your API Gateway with Terraform</p> </li> <li> <p>Build a Secure Static Website on AWS S3 Using Terraform</p> </li> <li> <p>Deploying a 3-Tier Application on AWS Using Terraform</p> </li> </ul>"}]}